Hello my friends! Haha, I'm gonna slip this mic out. I'm Will Tracey, Vice President for Applied
Complexity up at the Santa Fe Institute. I am thrilled to be introducing tonight's lecture
to all of you. Before I do, I just want to touch on two items of importance very quickly. First,
many of you will know that our next community lecture is actually the Unalam lectures, which
is historically a two-part lecture. It looked at one point like we would only have one talk this
year, but I'm thrilled to announce that Venki has agreed to give a second lecture, and that will be
a second lecture on a different talk, not the same lecture twice, right? And so for those of you who
are unfamiliar, Venki Ramakrishnan has won some of the most prestigious honors in academia. He is
fractal faculty at the Santa Fe Institute. I'm being told that he also won some other award,
a dynamite prize or something like that. Oh no, sorry, a Nobel Prize. But more importantly, I don't
know if that matters or not, but he's at SFI. It's going to be a great set of lectures. The details are
on the back of your program. I encourage you to check it out. Before we get fully into the awesomeness
of Jillian's talk, and there is going to be some really good content to dig into tonight, I do just
want to pause for a moment and just say a word of thanks. So a lot of work goes into these lectures.
Caitlin McShay and her team up at SFI, the team here at the Lenzic, put a lot of time in. I should
also note that the Lenzic is one of our institutional partners, as is the Santa Fe Reporter, but I
especially want to pause and give an acknowledgement to the McKinnon Family Foundation, whose support is
crucial to making this happen. Those of us who are fortunate enough to call New Mexico our home,
I think, are aware of the great impact that the McKinnon Family has had here. So let's just
take a moment and a quick round of applause.
So now I do want to jump into it, and I'm really excited to be introducing Jillian Tett. Many of
you who know of Jillian might think of her as a journalist, and that's true. She spent over
30 years at the Financial Times, both as a leader and as a writer. Of that, she spent about a decade
as the U.S. managing editor of the Financial Times. Some of you also might instead think of her as an
academic leader, and she is now the boss at King's College in Cambridge. But when I think of Jillian,
I primarily, or at least initially, think of her as an anthropologist. In part, that's because of how we first
met. Back in 2016, we were convening a meeting to look at the intersection of political risk and the
world. And this was actually October of 2016. And so some of you may remember, there were big things
happening in U.S. politics on the horizon shortly after that meeting. And Jillian showed up, and without
any, you know, fear in her voice said, all of you are wrong. I've spent the last few months doing some fieldwork
in rural America. I've heard everyone in this room say nothing about except how this is going to be a
democratic blowout. But based from an anthropologist perspective of what I've seen in rural America, I
think all of your polling data is wrong. And at the time, everyone was like, who is this again? And no one
really paid much attention. But I would say about five weeks later, every editorial page of almost every
English-speaking paper was repeating to us the sort of the narrative that Jillian had shared with us
about five weeks in advance. And, you know, she later wrote a book called Anthrovision, which sort of
lays out the importance of bringing this anthropological vision to the different topics that you're
engaging with. None of these are the topics that she's talking about tonight. I'm just saying that to
give you a sense of sort of her ability to understand what animates these important issues. And so it is
with great excitement that I'm going to bring her out to talk today about AI, and in particular, how AI
impacts some of our social relationships and trust. So this is one, I think, of a stream of
conversations we've been having about AI up at the Institute, because it is such a complex issue the way
that AI impacts society. And as the next chapter of that, I think we're very grateful to have Jillian Tett.
And so with that, let me bring Jillian out. Jillian Tett!
Well, thank you very much indeed, Will. That was an amazing introduction. And I think after a write-up like
that, I can only go downhill. So I'll try not to do that too fast. But it's fantastic to be here.
Really amazing. I've been coming to Santa Fe for about 25 years. It's honestly one of my favourite places on the
planet. I deeply admire what the Santa Fe Institute is doing. And so it's a great honour to be here. It truly is.
Now, when I got the rubric about this talk, I was told that at the beginning I should say a few words briefly about my own
personal journey. So just to quickly flesh out what Will was saying, I've spent the last 30 years traipsing the halls of high finance and economics and politics with the Financial Times.
I'm still a columnist for them. But before that, as you just heard, I actually trained as a cultural anthropologist. And I did my field work initially in Western China and partly in Tibet.
But my main field work was done in a place called Tajikistan, just next to Afghanistan, where I lived up in a high mountain village for a year studying Tajik wedding rituals in the Soviet communist system.
Not an obvious training to become a financial journalist, let alone someone who talks about AI. But I'm just going to say very briefly why I think anthropology is one of the most underappreciated disciplines out there, where I truly think it's a superpower,
and why I think that more people should be paying attention to it, not just in relation to finance and economics, but also the tech landscape today.
And the issue is this. If you're not an anthropologist, before I ask, are there any other anthropologists in the audience?
Yay! Fantastic. I can tell you whenever I ask that question on Wall Street, I usually get one rather embarrassed hand go up.
So it's great to be amongst my peeps tonight. But when people who aren't anthropologists ask about anthropology, they tend to assume that basically it's like Indiana Jones for grownups.
In that you go off somewhere wacky and weird, have an amazing adventure, come back with great stories, and you're a fascinating person to have at a cocktail party, but otherwise pretty useless.
And that is certainly how a lot of the people I've spoken to as a Financial Times journalist have seen it over the years.
But in reality, anthropology, I believe, is a superpower, because it's essentially a three-part journey, intellectually, which anyone could and should learn from.
Part one, you deliberately immerse yourself in the lives and minds of people who are different from you.
That could be, in my case, in Tajikistan.
It could be down the end of your road, talking to people who, shock, support a different political party.
But you deliberately, with an open mind and humility, try and imagine yourself into their worldview.
And you do that not just to try and understand others, or as anthropologists say, try and make the strange familiar.
You also do it because the single best way to understand yourself is to jump out of your skin, go immerse yourself in the minds and lives of others, and then look back with fresh eyes.
The Chinese have a great proverb saying, a fish can't see water.
None of us can see the assumptions we inherit, which shape us, until we jump out of our fishbowl, go swim with other fish, and look back.
And when we do that, we can begin to see another crucial thing, which is social silences.
The context of our world.
All the assumptions that shape us that we never think about, but which matter enormously, if we want to not just understand other people, but also understand ourselves.
And I would argue that right now, that insider-outsider perspective, that determination to see context, is absolutely critical if you want to understand how artificial intelligence, AI, is shaping or reshaping our social connections, both for good and bad.
And the reason is this.
We live at a time when AI is obviously accelerating in a dramatic and, many people would say, scary way.
It's reshaping our world in a way that most of us have barely even begun to understand.
We also live at a time when many people are very nervous about AI.
Probably many of you in the room.
And polls show, just to give a couple of examples, that about two-thirds of Americans regard AI as deeply worrying.
Half of them think it's going to do harm.
And only about a quarter think it's actually going to bring about benefits for them personally.
But aside from the fact that the population is pretty nervous about AI, we also live at a time when the whole question of trust has become incredibly tangled and, in many ways, incredibly depressing in our society.
Or so it seems.
If you look at recent surveys about trust, and I recently worked with a group of people at Jigsaw, which is one of the incubators at Google, which is using anthropologists and ethnographers to study how humans are interacting with digital technology and platforms.
So I recently wrote a piece in foreign affairs about this with one of them.
And the data that we were looking at in terms of trust in general in society are, on the face of it, unbelievably depressing.
Essentially, if you look at surveys right now, only 70, sorry, if you go back six decades ago, 77% of Americans trusted Congress.
Today, I can hear the laughter already.
Anyone dare to guess what the number is today?
It's actually 2%.
Okay, just to make sure that this is equal opportunity embarrassment, back in 1999, 55% of Americans trusted the media.
Today, that's 32% and falling.
In fact, the Edelman Trust Barometer suggests that, although back in 2008 and 2009, it was bankers who were losing the trust of the public, today, trust in journalists and the media is even lower than bankers.
You can call that divine justice if you're a banker.
But almost everywhere you look, trust has been collapsing.
But what I'm going to argue is that if you want to understand how AI is playing into the dynamics of trust, or rather how trust is affecting how we do or don't approach AI,
one of the first things you have to do is to take an anthropological look and step back and ask, what are we not talking about in relation to trust?
What's the bigger context?
What's the social silence around this?
Because I believe that that impacts very profoundly how we are interacting with AI today.
And in particular, I believe that there are two big trends that are happening in the world around us today that we don't talk about very much, which matter very deeply in terms of how we are interacting with our digital technologies.
Now, the first revolves around the question of trust.
And in essence, anthropologists have been fascinated by trust for a very long time because trust is the glue that holds social groups together.
It also, with my Financial Times hat on, it's a glue that holds finance together because the roots of the word credit, as in credit markets, come from the Latin credere, meaning to believe, which is all about trust.
But when I was an anthropologist at Cambridge doing my PhD, we were taught that essentially trust can exist on two axes.
You can have horizontal trust, which is basically peer-to-peer trust, and that's kind of eyeball-to-eyeball trust that you have with people who you know well in your family, your class, your social group.
That's small, horizontal trust.
But it used to be assumed that that kind of trust was limited by geographical proximity.
You had to be close to people who you're going to eyeball.
And so the thinking was that when groups got big, you then started to rely on vertical trust, which is trust in leaders, trust in institutions, trust in authority figures, trust in rules and bureaucracy.
And those two types of trust don't necessarily contradict each other because in our everyday lives, people often oscillate between the two.
But the vision that used to exist was that you had peer-to-peer small group trust or vertical trust when groups got big.
In particular, any of you who read Robin Dunbar, that brilliant evolutionary anthropologist, points out that when you get above 150 people, your brain can't really cope with that many people, so you start to create an institution.
Now, what's fascinating about today and what matters deeply for AI is that the advent of this, our cell phones, excuse my case that's falling apart, has changed that pattern.
Because for the first time in history, it's now possible to create peer group trust on a massive scale amongst people who don't have the ability to eyeball each other.
There's a technical word for it, distributed trust.
And essentially what it means is that you can have large groups of peers who all interact and trust to some degree without actually being able to see each other physically or know each other that well.
Now, that might sound very weird, but if you stop and think about it for a moment, back in the old days, when someone like me came to Santa Fe and I wanted to find an amazing restaurant, I would turn to an expert authority figure, a leader, and ask them where to go.
I mean, I might have used, say, a travel agent. Remember those? An expert.
I might have used Fodor's or Zagat, some kind of restaurant guide.
Today, you're more likely to go online and look at a travel platform and look at the star rating from the crowds and take your advice from your peer group that's distributed.
Or if you think about another example, you know, 20 years ago, if someone said to you that you were going to get into a stranger's car or put your kid in a stranger's car, you would have said, you're nuts.
Today, courtesy of Uber and Lyft, many of us do it all the time.
And if you ask yourself, why? What do I trust about this? Well, maybe you trust Uber.
Maybe you trust the idea that the government's looking at Uber. I kind of doubt it.
You probably actually, if you think about it, trust the fact that there is a peer rating system whereby if you get into a car with a driver whose rating is one, you're going to go, yikes, and look at what other people have posted about them.
And I'm not saying that kind of system for trust safety nets is foolproof because it's absolutely not.
But the point is this. We now have three types of trust. We have eyeball to eyeball trust, we have vertical trust, and we have distributed trust.
And what I would argue very strongly is that the surveys that show that our trust is collapsing in the world today are all about vertical trust, or almost all.
For the most part, they ask people, do you trust in the government? And everyone goes, heck no.
They don't say, do you trust in your online crowd when you're actually trying to decide where to go on holiday?
And what you have in essence is a pattern where trust is migrating rather than collapsing.
Because insofar as there are surveys of our peer group trust, those indicate they've actually held up very, very well.
I mean, Edelman, to go back to the PR company, which runs a huge survey on trust each year, has found consistently that a metric known as a person like me, trust in that has held up and in some surveys actually risen to a point where people trust a person like me more than a CEO.
And in fact, Edelman is now advising companies that if they want to send trusted messages, get used to the fact that your highly paid CEO may not be automatically trusted when they open their mouth.
And in fact, people might actually trust employees more for the truth than the CEO.
So that's the first big shift, and I'll come back in a moment as to how that's changing AI.
But the second big shift, which I think is also reshaping us, is the rise of what I sometimes call Gen P, Generation Pick and Mix, or Generation C, Gen Customization.
And by that I mean something quite fundamental has gone on in recent decades about how we imagine the individual relative to society.
And the issue is this.
Back when I was doing my anthropology PhD, what we were taught classically was that sometime after the Enlightenment era in Western Europe, there was a Copernican shift in terms of how we imagine the individual humans interacting with a social group.
And to be very, very crude about it, in many cultures in the past, and in parts of the world today, an individual is seen as a derivative of the social group.
Meaning, as the evolutionary anthropologist Joseph Henrich says, you know, I as an individual, essentially, I'm just a cog in a machine of a social group.
And my role and my identity is assigned at birth, pretty much.
Or it's assigned according to my family position or where I'm born.
And so, if someone says to you, who are you?
In many non-Western cultures, you'll say, I'm the son of so-and-so, or I'm the wife of so-and-so.
But, starting with the Enlightenment and accelerating in the last 200 years, essentially, the idea took hold that, in fact, society was a derivative of individuals.
I think, therefore, I am.
I exist independently of society.
And when you look at the 20th century, that idea took hold deeper and deeper.
You know, Margaret Thatcher famously said, for any of you who are British or have been in Britain, there is no such thing as society.
It's just individuals.
You had the rise of the me generation.
The idea the world revolved around me.
I had human rights.
I had the right to do whatever I want to basically shape the world myself.
That has taken on more and more importance in the late 20th century.
But, today, I would argue that this, again, our cell phones, has added a whole new twist to it.
And, essentially, what our cell phones have taught us to do is not just assume that I am the center of my world,
but to also think that I have the right to fashion and customize the world according to my personal desires.
In a way that, yes, it existed before in history, but used to only belong to kings or leaders or the ultra-rich.
And to understand what I mean by that, think for a moment about music.
If you wanted to listen to amazing music 150 years ago, your only option was to come somewhere like here
and listen to a concert set on someone else's schedule playing what they wanted.
And then we had radio, which you could switch on and listen as you wanted,
but someone else chose what was going to play.
Then you had vinyl records. Remember those?
I mean, you look like you're old enough, many of you.
And you could play that whenever you wanted, but somebody else assembled it.
Today, no Gen Z person, and I can see one or two of you, can imagine a world without a playlist.
Nobody can imagine a world where you couldn't put headphones on and listen to what you want, where you want,
how you want, whenever you want, on tap.
It's the rise of Gen P, Gen Playlist.
And that is extrapolated to almost every area of our life today.
I mean, thinking about consumer culture.
You know, the old days when you went to a coffee shop and you said black or white.
I mean, any of you who have got teenagers and gone to Starbucks, good luck.
It takes half an hour to order a coffee.
I mean, think about food.
There was a time when schools served one school meal for everybody.
Today, almost no school does that.
Everyone has buffets.
I know because in my role at Cambridge, whenever I have the students over and I say to them,
what do you want to eat?
I come back with a page this long setting out all their food requirements.
I mean, I usually end up saying, okay, I'll get Metze.
Just pick a mix.
Do whatever you want.
We have the same approach towards jobs.
I mean, again, any of you who are employers of Gen Z know that the idea of telling them to turn up nine to five, five days a week,
and this is your career ladder, is increasingly unpopular.
And it's not just because of COVID.
It's extrapolated onto politics.
I mean, political parties are like vinyl records.
It's a preassembled package that people today say, hang on a sec, I want to pick and mix my issues and my brands.
I like this issue and that issue and that person.
And our identities are the same.
You know, the old days of being told who you were by virtue of where you were born have gone.
You can go online and you can be anybody you want in any way.
And if you want to change your gender, if you want to change your name, if you want to change your presentation, you can do it.
If you want to define your family as having a dog in it, you can do that too.
I bet many of you have got dogs in your family.
That's a very common idea today.
A hundred years ago, it was totally weird because dogs were in the yard.
You couldn't redefine them as part of a family like a quasi-human being.
But we are living in this pick and mix era.
And in some ways, when you look at those two trends, the shift in pattern from vertical to horizontal,
and the idea that we can all pick and mix everything, it's unbelievably empowering and wonderful and amazing.
And I don't think anybody in Gen Z, or probably older, would want to go back to a world where they had to simply suck it up, one size fit all, listen to authority figures, and do what they're told.
But there are also big dangers too, obviously.
I mean, one problem is that if you only trust your peer group for advice, say in the media, and you're also essentially pick and mixing your tribe online,
the tendency always is to resort to more tribalism, not less, and to essentially create these echo chambers that are polarized and which don't actually interact.
We've seen that over and over again.
Because one of the bitter, bitter ironies of this is that as we have a pick and mix customized culture,
and as people have the ability to define their identity, they're not becoming less tribal.
On the contrary, many of us are choosing online to cling to people we recognize and feel comfortable with and cling to the symbols of that and become more tribal, not less.
So a world of pick and mix combined with a world of shifting trust is also a recipe for polarization and fragmentation and echo chambers and cyber flash mobs and all the stuff we're living with right now in our political ecosystem.
So, like every single innovation, there's a good side and a bad side.
But all of this is a backdrop, the context that we're not talking about in terms of how AI is entering our lives.
Because we are interacting with AI amid these changes, and that's shaping how we're both creating AI tools and reacting to them in very profound ways.
And to explain what I mean, I'll give you three data points.
Jigsaw, which I mentioned earlier, recently did a big ethnographic study of Gen Z in both India and America.
And they looked at what they trust for medical information and, to a certain degree, financial information too, and discovered that a majority of Gen Z prefer to deal with AI bots for medical information rather than human doctors.
If any of you are doctors in the room, that might sound scary.
But a majority of Gen Z prefer to deal with AI bots than human doctors for medical information.
Second data point, another ethnographic study by Jigsaw shows that when Gen Z consume content, i.e. read stories online, what they typically do is read the headline first, then read the comments under the article, and only then actually read the article if they feel that the comments justify reading it.
And when I tell people who, like me, are Gen Xers or Boomers this, they actually don't believe me.
Go find a Gen Z person and ask them about it.
Or watch them reading things online.
Because the reality is, this is very widespread now.
And what both of those things show is that essentially what you're seeing is trust patterns shifting.
As people move away, sorry, the media one, as people move away from trust in authority figures to trust in their online peer group crowd.
One other data point.
Oliver Wyman recently did a survey of workers and discovered that amongst Gen Z, about 39% in America prefer to have an AI bot as a manager rather than a human manager.
In fact, 39%.
Again, very similar finding to the issue about medical bots.
And that sounds really scary.
Except, if you start thinking about the two big trends I sketched out just now.
The shift in trust and the move towards pick and mix.
In some ways, it's not surprising.
Because when people think about AI and imagine how humans might interact with AI or not AI, we often talk about our interactions with AI as if it's a kind of single, undifferentiated mass.
You know, we're dealing with a robot, we're dealing with a human.
But in reality, if you stop and think about it, there's actually at least four ways that we can interact with AI.
And they're not the same, and they matter enormously around trust.
One image of AI is that, essentially, we can have AI introduced into our lives through that vertical trust axis as a kind of almost overlord or authority figure.
That's a vision of AI that most of us have probably got in our heads.
Because in Western media culture, we grew up with the idea of AI robots taking over the world and dominating us.
You know, Space Odyssey 2001, how in the spaceship, and AI as a bossy thing telling us what to do.
That's the vision of AI as a master.
But you can also imagine AI as a mate, somebody who's actually on our horizontal plane, almost like a friendly member of our tribe, our peer group.
That's almost more like R2G2 in Star Wars.
Or you can have AI as a mirror to ourselves.
And you can also have AI as a moderator in terms of conversations between humans.
And I'll come on to that one in a moment.
But much of American culture has tended to assume, I would argue, that insofar as AI was going to come into our world, it was going to come in as a master.
It's one reason we're scared of it.
It's going to come in and take us over.
Now, I should stress that that vision of AI is not actually universal.
I mean, yes, sitting in America, we tend to assume that Silicon Valley dominates the way we imagine AI culturally.
And we assume that everyone else must have the same attitude as well.
Newsflash, they don't.
If you go somewhere like Japan, where I lived for a number of years, attitudes towards AI and robots are actually quite different.
They are much more positive.
People in Japan are not nearly as scared of AI as Americans are.
I mean, again, to go back to the survey, when I said that only...
Where are we?
Going down.
Yes.
So, two thirds of adults in the United States and the United Kingdom and Canada say that AI makes them nervous.
In Japan, it's only 29%.
And although only a third of people in America say they're excited about AI, in Japan, it's about half.
Very, very different attitude towards AI and robots.
In fact, Japan is the only country in the world I know where even the unions like the idea of automation.
That's partly because of demographics.
They're running out of workers.
It's also because their media history is quite different.
In the sense that when Americans were being terrified of robots by, you know, HAL in Space 2001 Odyssey, or in Star Trek or Star Wars, or if you're British, with Daleks and Doctor Who,
Japanese were reading things like Astro Boy, which presents AI and robots in a very positive, cuddly way.
And the other reason, I also suspect, is because of Shintoism.
That Japanese culture, or rather the Shintoist religion, doesn't clearly distinguish between animate and inanimate objects.
It's a spectrum.
And if you believe that a rock can have a soul, then it's actually not that hard to believe that your iPhone can have a soul as well.
In America, in Judeo-American tradition, in fact, the dividing line is much, much clearer.
And that means that people tend to get very scared by the idea that a machine might actually have a brain or a soul.
So attitudes towards AI are not universal.
But in America, the idea has been that AI would come into our lives through a vertical axis.
In reality, though, it's actually coming into our lives, I would argue, in a horizontal axis of trust.
Because the way that most of us are encountering AI and digital technologies is through this.
It's literally at the tip of our fingers.
It's a very intimate relationship.
And I would argue that we're putting it into our lives on a horizontal platform of trust.
And it's almost becoming part of our peer group.
And that's critically important.
Because when you think about why Gen Z in India and America might say that they prefer AI bots to human doctors,
one way to make sense of that is, well, you can argue, if you're a human doctor, that they're just stupid.
Or you can say, well, actually, if you're a Gen Z person, getting hold of a human doctor is actually quite hard.
It's time consuming.
It's inconvenient.
Your phone's at the end of your fingers all the time.
And doctors, like many human authority figures, are perceived by Gen Z as having an irritating habit of lecturing them,
telling them what to do, being bossy, being an authority figure.
So, whereas an AI bot is often neutral.
I mean, Gen Z, according to these studies, suggests that they find human doctors more threatening for their privacy,
because they're authority figures, than AI bots in their phones.
Again, for most of you, this might sound mind-boggling.
But it makes sense when you think about the two bigger shifts.
Same thing for financial advice.
Same thing for trusting AI bots as managers, instead of human managers.
They're less bossy.
They're less intrusive.
And in many ways, they also echo the fact that we live in Gen P, Generation Playlist.
Because what AI essentially does is reinforce our love for customization.
We have an AI bot that knows all about me.
It gives me what I want.
I'm the center of my world.
I can customize it courtesy of that bot.
It's addictive.
And of course, the very fact we love customization is part of what's driving the rise of AI as it hoovers up all of our data and what makes us personally tick.
So customization and shifting trust patterns, in some ways, make it entirely rational that you have these results where Gen Z says they prefer AI bots to human doctors.
Is this bad or good?
Well, in many ways, it's terrifying.
Because when I said before that you've got the ability to have AI as a master, as a mate, as a mirror, or as a moderator, there's a fifth M, which is as a manipulator.
And AI bots in the wrong hands can be manipulative and dangerous in ways that we're only really beginning to get our heads around.
Some of you may have read that tragic, shocking story about the teenager who committed suicide because of interacting with a bot via character AI.
And essentially, and I should stress, this is all subject to lawsuits.
So obviously, there's still a lot to come out.
But according to the reporting of that story, it seems that a 14-year-old fell in love with an AI bot that came from the Game of Thrones, that interacted with it for a long time, and essentially persuaded him to eventually not only kill himself, but even taught him how to unlock the gun that his father had in the house.
That's a dark side of trusting in an AI bot as part of your peer group in a very, very intimate way, the most intimate way possible.
And I fully expect that there's going to be endless amounts of stories coming out like that over the next year or two.
But, but, but, but, like all innovations, every innovation from the creation of fire onwards, there's a dark side and there's a light side.
And this shift in trust patterns and our interaction with AI can also do amazingly positive things.
I mean, let's start with the issue of Gen Z trusting AI bots and medical advice more than human doctors.
Yes, that is horrifying to a human doctor.
But we also live in a world where access to healthcare is really difficult and expensive.
Wouldn't it be great if everybody could suddenly get expertise literally at the tip of their fingers here?
Or think about education.
Think about the propensity for using AI bots as a kind of mirror, if you like, almost for therapy.
That's happening today, and it's happening in some cases in a very positive way.
Or think about the use of AI platforms and bots in quite a different context, which is the fourth M I mentioned as a moderator between human-to-human conversations.
And this is truly fascinating.
I said earlier that we live in a society which is becoming increasingly polarized and fragmented because we have this shift in trust patterns
and the growing pick-and-mix generation choosing to be in echo chambers who ignore everyone else.
And as we all know from, if you're older, reading the newspapers, some people still do that,
or from talking around the dinner table or looking at communities,
one consequence of this is a deeply fragmented and polarized landscape today in America.
It's very hard for people from different political tribes to even talk to each other today effectively.
But here's something very interesting, which is that just as the brilliant techies have created bots and tools which can translate languages from one language to another,
they can also be used to translate political dialogue and conversations and viewpoints.
They can actually act as moderators for difficult conversations between large groups of people.
And if that sounds very far-fetched, big experiments have been going on in Taiwan recently under an amazingly innovative digital minister called Audrey Tang
to do exactly that with thousands of people communicating with each other through AI platforms
and trying to find common ground on issues in a way that you couldn't have done with human moderators.
There's an experiment going on, or just been going on recently, over in Kentucky in a town called Bowling Green,
which has, or is in the process of doubling in size, and the urban civic leaders are trying to plan the future for the town.
They wanted to get everyone engaged.
They knew that trying to do old-fashioned voting would take far too long, and polling is very crude.
So they organized a massive conversation over a number of weeks using AI platforms to try and work out what the community cared about.
And the results were quite amazing.
And the most encouraging thing of all is that in Kentucky, where passions can run very high around politics sometimes,
what they discovered was that on the vast majority of issues there was a huge amount of consensus that no one had even realized
because they were so busy shouting at each other about what they all thought about Donald Trump.
And humans would have found it very hard to find that common ground of consensus because humans can't scan 10,000 conversations.
There's actually another experiment like that going on right now in New Jersey,
and there's another one that's been done by DeepMind in Oxford with a very similar kind of pattern.
AI bots can actually be more effective in moderating human-to-human conversations sometimes than humans can.
Or I'll give you another example.
There's a lot of research now showing that if you want to talk to a conspiracy theorist,
the best way to actually deal with them and debunk conspiracy theories is not with a bossy, do-gooding human
who tells them why they're wrong.
It's actually with an AI bot.
And if you think I'm joking, a group of researchers recently at MIT have done a massive study using an AI bot for that very purpose
and shown that actually belief in conspiracy theories declines dramatically with an AI bot compared to a human.
And the reason is very similar to Gen Z with doctors.
The bots are seen as being neutral, less of an authority figure, they're not patronizing and bossy,
they're patient, they don't lose their temper, they don't storm out,
you don't have a family row over the dinner table with a bot.
They actually listen.
And they can act like mirrors and moderators.
So what I'm really trying to say is that as we look at how we interact with AI,
we're in very, very early days in terms of how this evolves.
There are enormous dangers.
There are enormous potential benefits.
And what happens next really comes down to a key word that we should all be thinking about a lot,
which is agency.
There is a way of managing our interaction with bots, which gives us, or AI tools, agency.
Emotional agency, economic agency, political and social agency.
There's a way of seeing bots and AI as tools or companions that we can deploy when we want.
And they can be designed in that way to be effective.
Or there's a way to turn around and say, these things are taking over our world,
we're going to surrender our agency, turn off our brains,
and essentially let big tech design them without any guardrails,
without any knowledge of what they're doing to us, and simply sit back.
To a large degree, that choice now is with us as a society about how we actually choose to interact with AI.
What path we go on will partly depend on how we also interact with other human beings,
and the level of confidence we have in our own political economy.
One of the other interesting details of the anthropologists working with Jigsaw,
who are looking around the world at how different cultures are interacting with AI,
and I can't stress strongly enough, the American vision of AI is not the only one at all.
But one of the very interesting things they discovered is that in countries which have a high level of social stability
and trust in government overall, like Singapore, AI bots and tools are seen really just as tools.
They're something you use to augment your life, but they're simply an addition.
Whereas in countries like America, where there's a high level of anxiety about the future and instability,
you increasingly see the flourishing of AI tools as, say, quasi-therapists.
You don't see that in Singapore.
So there's all kinds of interesting cross-cultural patterns happening.
And there's also a lot of cultural shift.
And this is really the last point I want to leave you with, which is this.
As an anthropologist, you know, I'm often told or I tell people that what I study is culture.
And people think, well, culture exists a bit like a Tupperware box in that you have something called, say, maybe English culture,
which is sealed, static, fixed, and you can stack up different cultures like Tupperware boxes on top of each other in a hierarchy of value.
So if you're English, you assume that English culture, the Tupperware box called English culture, sits at the top, followed by maybe French culture, Italian culture, whatever else.
But that vision is actually completely wrong.
Cultures never exist as static boxes, which have boundaries and sealed edges.
They're more like slow-moving rivers.
They constantly change.
And new streams come in, and the banks are very muddy.
We've seen that with technology.
I mean, one of the most interesting studies I've seen in recent years of how humans are interacting with digital technology came from a group at Intel who went out about a decade ago to study facial recognition technologies.
And they did on the ground research in China and in America.
And what they wanted to study was why Chinese consumers back then appeared to be totally embracing of facial recognition technologies and were really excited about them in a way that they assumed was very alien to American culture or American cultures.
Because in America, there was this fear of facial recognition technologies and AI and things like that.
So the whole study was predicated on the idea that Chinese cultures, consumers, generally liked AI and liked facial recognition technologies for convenience, and whereas American ones didn't.
Within a month of that study being published, we suddenly had the introduction of facial recognition technologies on our iPhones.
And guess what?
We all, or almost all of us, adopted it instantaneously.
And suddenly, frankly, we've all started to look more Chinese than American in terms of our attitudes towards facial recognition technologies.
Ideas and attitudes cross borders.
It's a very, very fluid time in how we interact with each other and technology.
So my last point is this.
If you want to understand how we are trusting or not trusting AI, don't just look at the noise.
technologies, don't just look at AI and technology.
Look at the silence around that, which is how are we trusting other human beings or not?
And how does AI play into that or not?
Dana Boyd, a very brilliant anthropologist, went off about two decades ago or 15 years ago to study how teenagers reacted to social media and their cell phones.
And made the obvious point that we tend to ignore, which is that one reason why teenagers are addicted to cell phones is because it's the only place that they can actually roam without parents snooping in a world where we all have stranger danger fear, where teenagers are massively over-scheduled.
They can't travel as they used to do 100 years ago.
And so iPhones are the only place, cyberspace, where they can actually roam freely and congregate and experiment without parents watching them.
So you can't understand how teenagers use iPhones without looking at how they're behaving in the real world without iPhones and how parents have changed their environments at the same time.
We can't understand how we're interacting with AI until we start looking at how we're interacting with each other and how patterns of trust are shifting.
So a world of artificial intelligence needs a world of anthropology intelligence too.
One AI needs the other AI to create what I think we should be aiming for today, which is the third type of AI, which is augmented intelligence.
Where we have the agency to set where we're going, not just with the bots, but with other humans too.
So thank you all for listening.
And I welcome questions about that or anything else.
And for all of you who are not a Gen Z-er, I hope this understands your own weird Gen Z people in your life a little bit better.
Thank you.
Okay, so if you have a question, please raise your hand.
My friend Joan and I will be coming around with the microphone.
We'll try to address everyone as we can.
We also want to hear from unfamiliar voices, maybe representatives from all generations.
So I'll come over here to start.
I should say, by the way, while we're getting a question, one of the other places where the idea of using an AI as a moderator between human to humans to decode what different tribes are thinking, and I'm not joking, is inside companies where baffled CEOs and the C-suite are trying to understand their Gen Z workers.
And they're literally doing experiments in some companies now of using AI tools as moderators between these different generational tribes.
Hi, thank you so much.
My question is how you think about the direction of AI in light of the falling hierarchical trust, so trust in institutions.
So who directs the version of AI that we all agree societally to move towards sort of aligned AI, right, values aligned AI, versus the sort of more malignant AI?
That's a really good question, and the answer is we don't know at the moment.
There are ways to have more distributed development and agency.
You know, that involves things like giving consumers a lot of choice about what platforms and what companies they use for different AI tools.
Giving them transparency about the different choices, giving them the ability to essentially use agency in choosing what kind of data sets are being used to train different AI bots and models, which is absolutely critical.
And to also decide how to actually deploy any AI advice or not.
And there are experiments going on around how to do that right now.
And the idea that you might be able to give different institutions or different people institutions much more agency and control over how you actually develop and train the tools that you are or are not using.
You know, in an ideal world, coming back to the point about retaining a human agency around it, you know, one way to try and visualize it is a bit like a Fitbit or an Aura ring, whereby you can use amazing digital tech to tell you all kinds of things you wouldn't have known otherwise about yourself and your fitness and health.
And you can use that ring or anything else to go and prod you to go and live a healthier lifestyle and do all kinds of things.
And you can even set it up to lecture you if you want or just be like a friend or you can use peer group pressure and you can put your competitive training scores up on some common platform with your friends and you have peer to peer pressure that way.
But you at the end of the day retain control over whether you actually decide to go on that run or not.
So there's still an element of agency there.
And ideally, that would be the kind of model of what we'd use in terms of developing our use of AI bots and tools in our everyday life to have that sense of agency and control in the last moment.
But whether we can do that also depends obviously in the macro picture and the question of whether at the moment these two very hierarchical vertical institutions and hierarchies of government and big tech will actually create the ability for us to do that.
And that comes back down to common pressure.
You know, a world where only 2% of people trust Congress is probably not a world where people trust Congress to do the right thing around AI.
And therein lies the problem.
Actually, this is a good piggyback question.
If you were in front of all of Congress and the Senate right now, so I think they could cuddle up in here right and fit.
What case would you make for the legislation we need to pass to protect the future that you would best imagine, whether it's data privacy or transparency?
What legislation do we need?
I think, personally speaking, I think there are several key things.
One is to have maximum consumer choice around platforms and to make sure we don't just end up with an oligarchy of platform choices.
To have maximum choice and transparency around the data sets that are being used to train the platforms.
To have some element, I would argue very strongly, for intellectual IP protection for the people actually creating content.
I'm obviously totally biased in that respect because I spent most of my career as a journalist.
And media content is being scraped all the time.
And, you know, in a way that's essentially taking away the economic value of journalism and giving to tech companies.
I'm in the group of people who think that eventually tech companies are going to have to pay some kind of tax or some kind of way of paying back into society for all the material they're extracting right now.
You know, we can't have taxes on robots right now, but we probably should do.
So, and I think also, at the end of the day, we need to look very seriously at the legal code around the culpability or not of bots and AI.
Because at the moment, it's a gray zone.
And also, try and put in guardrails against some of the worst potential outcomes, like, you know, a 14-year-old committing suicide because of their interaction with the bot.
Now, again, I can't stress strongly enough.
When people say AI is dangerous because it does horrible things to humans and we can't trust it because it can abuse us and prompt us to do horrible things, that is totally true.
But, newsflash, humans do horrible things to other humans all the time on a massively bigger scale right now.
So, the amount of, I hate to say this, amount of deaths occurring because of an AI bot going mad is nothing compared to the amount of human-on-human abuse that's occurring.
And everything we talk about has to be seen in that context.
But that doesn't mean that just as we have rules for trying to prevent horrible things that humans might do to other humans from not occurring, we need to apply that to bots as well, in my view.
Hello. Thank you for your talk.
So, one thing that I was curious about is the factor of social isolation in this.
And you've referenced that young man who committed suicide a few times.
When I read that story, I actually thought that that was very resonant as a symptom of social isolation as a consequence of this dependency on the AI bot.
And I don't know if you remember reading the actual transcript that he had shared with that bot.
Yeah.
It hadn't actually encouraged him to commit suicide.
He had posed that idea and it gave very bland, generic responses.
It was a mirror.
Yeah.
Yeah.
And related to what you were talking about with online communities contributing to extremism, one thing that I kind of think about is how online communities can be a substitute for real life community.
And how AI can kind of create communities of one where you will never need to interact with another human being if you decide that you'd rather substitute that to a bot.
So, I'm curious about the social isolation aspect.
I couldn't agree more.
And like everything, it cuts both positive and very negative.
You know, on the one hand, if we just stick with the technology aspect of it, when I was a teenager growing up, I was an obsessive writer.
And I wrote novels in my bedroom by myself all through my teenage years.
And I didn't know anybody else who had this weird obsession like me.
And I never showed my writing to anybody else.
And I felt completely on my own in that respect, you know, in 1970s suburban Britain.
My youngest kid has somehow inherited this obsession too.
And her experience could not be more different because, like me, they started scribbling away, age 12, 13, passionately, obsessively.
But then they immediately went online and discovered something called Wattpad, which to my mind is an absolutely brilliant innovation.
And it's basically a club for other weirdo teenagers who also write obsessively.
And it's amazing because you can go online, you can post your writing, you can talk to people all over the world, and you can get your stuff read.
And you actually make better literature very fast by creating a community almost immediately, which you would never have been able to do, in my case, 50 years ago or 40 years ago.
That's the upside, that you can suddenly find a community where you never had it before.
And, of course, the other upside is you have the validation of thinking, you know what, I'm not weird.
And if I want to define myself as a writer, and my parents don't think I should be writing, they think I should be a jock playing lacrosse every weekend, you can literally go online and you can be empowered to be who you want to be.
That is amazing.
And I would not for a millisecond want to lose that, because in so many ways this digital technology has brought about incredible opportunities.
The dark side, of course, is that the rise of digitization, the fact we had COVID, those two things together, as people like Jonathan Haidt have described so brilliantly, have done terrible things, it seems, to teenagers' mental health, and cultivated this sense of isolation.
And for some kids, cause them to withdraw into their bedrooms and from the world.
And that's horrifying.
You know, it's also created very dark communities.
You know, you talk about incels and extremists.
I mean, the one that I know a lot about, because of my own family, wider family network, is eating disorders and suicidal ideation.
And I've seen up close and personal what can happen when kids get sucked into that.
But, again, talking about the light and dark, you know, a couple of kids I know who have almost not made it to the age of 20, because they've been dragged down into that dark world by virtue of social media, have then recovered also by virtue of social media.
Because out there on the internet, and if any of you have ever had the horror of dealing with a teenager who's been in this situation, you might know about this, there's now a whole community of recovery apps and recovery tools and recovery communities.
You know, Recovery Row, you know, Linda Sun, all these people are amazing.
And if you're a recovering teen today from an eating disorder, and by the way, if anyone is in that situation of having kids in your life in that situation, I can't stress strongly enough, there's actually a way to use social media for good.
That has been a crucial part of their recovery journey.
So, as I say, it's good and bad.
And the question for us now is how do we harness the good and minimize the bad?
That's a critical question.
Three observations and a question.
At one point in time, television was an innovative technology, and it turned into a wasteland where violence is entertainment.
People are now developing different identities.
They have an internet identity and their real life identity.
Some people think this makes people schizophrenic.
Also, the difference in Europe and America, or Europe, they trust government more and corporations less, and the US is the opposite.
They trust corporations more and the government less.
So, in this, where does hacking fit into all this?
You mean hacking in the sense of?
Your personal hacking, your Instagram, everybody else gets hacked, they want to get money, and on the larger case with governments, information, corporations, they get very compromised.
It's terrifying.
It's terrifying.
There's no two ways about it.
It's terrifying.
I mean, you know, we're dealing with potential both hacking in the sense of massive manipulation.
You know, the reason why the data, you know, it is absolutely critically important what the AI tools are being trained on in terms of data sets.
And, you know, there are already signs that people are trying to pollute and revert the data sets by hacking into the data sets or basically putting all kinds of horrible material into that stream.
There's hacking in terms of simple robbery from people.
There's hacking in terms of identity theft.
You know, I mean, it's an absolute Wild West right now.
And there is an urgent need for both more resources, more appreciation, and more coordination in efforts to try and fight back.
And, frankly, a lot more education of anyone who's going online at the same time.
So, it's very, very serious.
And I'm not, for a millisecond, downplaying that whatsoever.
It's very serious.
Of course, I mean, I would just also point out that, you know, if you go back, you know, 150, 200 years, where we're standing right now could also be very dangerous because of things like highway robberies and all other kinds of, you know, violence as well, which, thankfully, is less common now.
So, forms of, you know, ways for humans to do horrible things to other humans have been, you know, prevalent for a very, very long time.
But, I mean, I'm not trying to minimize cyber hacking, but that is a new threat today.
Thank you.
Given what you've described as almost chaotic cultural changes, do you think that there is a fundamental concept of trust?
Do you think the definition of trust has changed?
It seems to me that when you talk about distributed trust, that the notion of trust is somehow more restricted.
Maybe that's my own impression.
Trust is restricted.
More restricted.
We're almost turning a blind eye to some areas where we might have trusted or not trusted before.
And perhaps I'm not quite understanding this concept of silence.
And my second question, more of a question, sorry, is if AI systems are going to become as agentic or perhaps more agentic than us, will we need some kind of formalism for what trust is?
I think that's a very good question.
I mean, first of all, to make the obvious point, trust exists in a spectrum.
So it's all the way from complete lack of trust to total blind trust.
And as we all know, in terms of our human relationships or anything else, interactions in the world, trust can shift, trust can be shattered, it can change.
You know, the old thing of trust and, you know, trust and then verify, I think is, again, a mantra that should be taught to everyone going online right now.
So, you know, I'm not saying it's black or white at all, but I'm trying to indicate that there is more than one way to create trust than the traditional time of pattern.
And, you know, the other thing I should have said is that, you know, because we've gone into this world of, you know, trust migrating from a vertical to a horizontal and growing pick and mix, it doesn't mean that necessarily we'll always keep going in that direction forever.
I mean, you know, these things can swing back and forth.
I sometimes think, you know, is there anything that could cause our modern generations to suddenly say, I don't want customization all the time.
I'm happy to have a one size fits all.
Is there anything which could basically cause us to suddenly say, yes, I'm going to fall in line and obey authority figures unquestioningly, no matter what?
Frankly, I think probably the only thing that could do that today is war.
That, you know, if you're in the army, the definition of being in the army is you have to basically suck it up as a one size fits all and deal with authority figures.
We've seen periods of time when that was a norm in the past.
You know, if, heavens forbid, we end up in a military conflict, maybe this trust pattern will shift again.
But certainly at the moment, it's very hard to see anything that would actually put the genie back in the bottle where suddenly society collectively says we all are going to trust government and trust authority figures in the way that people did 80 years ago.
Okay, so I have one over here.
Hi, thank you for your very human perspective on things.
When I think of trust, I think of the word faith.
And I think about how one of the longest standing institutions of trust besides trusting the physical world and physics, we trust, I think, religion and faith.
There's a lot of talk about consciousness.
I really love that you brought up Shintoism.
And so my question for you, as somebody who studies culture, is I see us going towards a new faith in something very tangible.
And I think artificial superintelligence is inevitable.
I think it's going to happen within the next five to 10 years.
So what happens culturally when we have a very tangible digital messiah?
Well, then we will have a whole new iteration of potential trust, blind trust, misplaced trust, trust that can be manipulated.
We don't know.
But on the issue of religion, again, religion is another area where we are starting to see a sense of pick and mix culture coming in.
You know, there's a lot of dispute right now about whether, you know, younger Americans are less religious, more religious.
And a lot of that debate centers around the definition of what is religion.
Is believing in wellness and wellness apps and meditation and having a finch on your phone, which is one of these little apps that tells you, you know, sort of ethical messages every day.
Is that religion or is religion in a church or a mosque or a synagogue?
You know, again, we're dealing with a world of, you know, is the idea of going to church like having a vinyl record?
You're given a preset menu on a platter in a world where people like to pick and mix what they want all the time.
And so that's the main way that I thought about, you know, how religious patterns and behavior is shifting.
But, you know, if you're right and we're going to end up with, you know, AGI, you know, an artificial super intelligence soon that we all end up blinding blindly trusting, will that be a deity or new deity?
I think for many people in the room, they would say absolutely not.
And look at that with horror.
But, you know, if you're looking at that within the whole spectrum of how culture is shifting, then one might potentially frame it in those ways.
Remember, I come, my day job is at King's College in Cambridge, which has a massive great chapel.
And I spent a huge amount of time in a 600-year-old chapel, you know, looking up at the ceiling and thinking about how extraordinary it is that this chapel, created 600 years ago under Henry VIII,
remained so dominant in that college and that city and culture in a way that certainly I would not have expected necessarily.
Would that be replaced by an AGI?
Hard to believe.
But we are going into some really fascinating, terrifying, but also exciting times in all kinds of areas in terms of what AI could end up doing.
So we have time for three more questions.
And I've acknowledged three individuals.
So there's this man in the center with a microphone.
And then there's a young woman in the front who had a question.
And then Chris will end with you.
Thank you for a wonderful presentation.
I have a question.
Historically, journalists and people doing research get other opinions before they publish.
They double check.
I saw a presentation recently where an individual involved in AI went to one AI source, one chat, then went to another and asked it what they thought of the first.
And in fact, he used four.
Do you see a role for this in the future?
Well, again, there's a good and bad side.
The bad side is that, as you would have read, increasingly training data to train the models is essentially relying on AI-generated data.
So you're ending up with a kind of hall of mirrors and self-referential hall of mirrors that ends up polluting and corrupting much of the source material.
And so the idea of AI using other AI tools to kind of verify, et cetera, et cetera, is potentially very dangerous.
And it's one of the reasons why you can end up with crazy results, hallucinations, or things like the recent grok results around Hitler that some of you may have seen.
So that's the dark side of creating a self-referential, self-feeding echo chamber feedback loop, et cetera.
The other side, and this is a positive way to try and look at AI, is that it can actually expand your team of fact-checkers and second opinions in a very creative way that might actually create a much richer sense of knowledge and dialogue.
And as a journalist, increasingly people are using AI tools to kind of, as checks and balances, to crowdsource ideas, to crowdsource facts.
It doesn't mean the AI is necessarily writing the pieces, but you actually get a much wider range of ideas and prompts by potentially using them to be more creative.
And one of the many bitter ironies of my own sort of career in life is that about, when my last book came out called Anthrovision, I went around about five years ago saying with great misplaced confidence that there was one thing that AI would never do, which is to tell a really good joke.
So I used to say, well, the great thing is, if you're a comedian, you have job security.
And the reason is that humor, in many ways, is the ultimate cultural artifact.
Because the reason why anyone laughs at anything else rests in a sense of tribalism.
You know, you have to be in the in-group to get a joke.
If you're not in the in-group, you don't get the joke.
So jokes define tribal groups and vice versa.
But also because humor tends to work through contradiction and ambiguity in our cultural patterns.
And it also tends to tap into social silences, the stuff we don't actually want to acknowledge.
And it used to be assumed that all of that was very, very hard for AI platforms and tools to actually read or replicate.
And then it turned out that I was dead wrong.
And the reason is as follows that the old AI systems, pre-transformers, which were basically about trying to plot a logical path of thought, did indeed find it very hard to tell jokes.
Because jokes aren't logical, generally, unless they are the very crude, knock, knock, who's there, Christmas cracker type jokes.
If you're using transformers and you're basically working on a probabilistic prediction system, you can just observe loads of jokes and work out the probability of what will be funny next time around.
And what's happened is that joke writing apps have cropped up.
And they're only funny about 40% of the time.
But newsflash, human comedians are only funny 40% of the time too.
And I say that because, I'm actually deadly serious, because if you look at who actually writes late night television comedy skits, insofar as we still have late night television comedy skits on TV.
You know, you might think it's one brilliant person, Stephen Colbert, sitting in a room, developing his jokes, all by himself, boom, magic, he tells them.
And that's not true.
Any of you who've worked in the television world know.
What actually happens is you have a team of comedy writers sitting together, coming up with jokes, only of which 40% are funny.
And they throw out the other 60% and they refine them.
So now what's actually happening is that you're having, in some cases, an AI bot added into the team and creating another set of 40% good jokes.
And simply adding into the mix and making a richer dynamic in terms of checks and balances at the moment.
Now, in the future, maybe that AI bot will end up being the only joke teller, or you'll have a whole team of AI jokes, each of which are producing jokes which are 40% funny.
And I'm deadly serious, if anyone's interested.
I mean, there's actually quite, there's some very interesting research done on this whole thing.
But what I'm trying to say is when it comes to journalism and fact checking, you know, in some ways, I imagine the best, most positive outcome and future will be using AI, not as artificial intelligence, but augmented intelligence, or additional intelligence, or accelerated intelligence, to actually create a richer dynamic in a team.
That, to me, is the best way looking forward.
Thank you.
So, you've given advice to Congress, and we've heard you talk a lot about the good and the bad.
And another group that's really struggling to navigate that is parents.
So, what advice would you give to parents who are struggling, one, with the social isolation that you've already talked about,
and two, trying to preserve space for that wandering and discovery that you mentioned is more rare of a commodity these days?
Yes.
Well, I feel very unqualified to give advice, because I'm a parent, and I screwed up endlessly myself.
So, there you go.
So, I guess what I'd say is, firstly, take what Dana Boyd says to heart, that you cannot begin to understand how teenagers are using cell phones and computers and digital devices just by looking at the noise, which is a cell phone.
You have to look at how they're interacting in the physical world as well.
And someone like Jonathan Haidt says, and I strongly agree, that if you want to create kids who are less prone to being addicted and seduced by digital tech, let them roam in the real world in a way that's gone out of fashion today.
So, that's the first point.
Secondly, I would agree with a lot of what Jonathan says about trying to make sure that you have some sense of oversight insofar as you can.
Be aware of your own behavior.
You know, if someone's addicted to their cell phone as an adult, it's very unfair to expect kids not to behave the same way.
Ask your kids to teach you about how they're using social media and apps and try and learn from them as well.
And try and recognize that there's both a very good and very bad side to all this.
And, you know, the bad side is not going to disappear.
It really isn't.
There are so many horrifying dangers today.
But trying to plug into the good side as well is really important.
And at the end of the day, you know, what will shape or what will give us the best chance of having healthy human-AI interactions for teens as well as anyone else is having healthy human-to-human interactions.
And that really is the most important thing of all.
But, again, I think keeping this idea in your mind of, you know, AI or bots as masters, mates, mirrors, or moderators is quite a good way to frame it.
And recognize that there are different ways that teens are using these today.
Mostly it's around mates or mirrors.
And trying to see where the both good and bad sides are.
Thank you, Dr. Tett, very much.
A long-time fan of FT.
And it's wonderful to hear that you have a relationship with Santa Fe.
So welcome back.
I couldn't help but...
I really chuckled at your characterization of, like, the US thinking of AI as like HAL 9000 and Singapore thinking of it more as a R2-D2 type of interaction.
But I'm just curious to hear your kind of sober prognostication of, like, what this will turn out to be in the US given kind of our political directions.
While at the same time thinking of Singapore as a place where, you know, what LKY is spinning in his grave because he didn't have AI to help, you know, shape the Singaporean society with the PAP.
And then the corollary to that is I really like the vertical versus horizontal trust framework.
But I feel like the MAGA movement is kind of a strange creature in the sense that they coalesced in a horizontal way but now are finding themselves in a vertical way.
So I don't know how you see that kind of playing out in the bigger picture.
Yeah, I mean, I can't stress strongly enough. It's not always an either or.
And so what's happened with something, you know, what typically happens with something like the MAGA movement is that they are operating as a peer group, you know, as a horizontal platform.
But coalescing around a shiny brand, if you like, you know, they're not adopting the entire political party called Republicanism.
You know, that's like vinyl records. They're choosing to pick a mix bits of what's on offer and that right now the brand they're coalescing around is Donald Trump and it's a personality cult.
It's a personality cult. And so it's a personality cult that they hang on to very strongly because they chose it. They feel they chose it.
If you choose something and choose a symbol, then you tend to hang on to it even stronger than it is simply assigned to you at birth or an entire package is just given to you.
You know, in terms of what that means for AI, where it's going, I mean, you know, I wouldn't dare to make a prediction as to how those two things are going to interact in the next 10 years.
But clearly, AI is accelerating. Clearly, at the moment, one of the things I find very scary is the concentration of power and knowledge around AI into a small group of people.
And there's a real danger of regulatory capture or political capture that you see in any industry.
You know, I was covering finance on Wall Street for many years and saw that there. And there's a real danger of that occurring.
And what's particularly alarming in the US context is the lack of digital literacy on a mass scale and also the lack of any sort of proactive training or, dare I say, industrial policy around this in a way that will actually engage a wide range of people.
You know, I am not remotely starry eyed about Singapore. You know, it's got some very bad things about it as well as good things.
However, Singapore is striking because there is a high level of digital literacy and there's a very deliberate, very, very deliberate effort to create maximum digital literacy and engagement and agency in schools.
There's a very clear cut industrial policy which is trying to, you know, essentially underpin that.
And as I said earlier, there is a sense of some level of social cohesion and social trust, which means that people are turning to AI tools as a supplement, not a crutch.
And that's quite different. But, you know, once again, I can't stress strongly enough the way that cultures and things can change. And one other example I'll cite is Ukraine.
In that I've been going to Ukraine for many years and gone many, many times since the war started.
And one of the things that most people in the West don't know about Ukraine is that even before the war started, because Ukraine had a massive population of mostly back office coders and techies, which were serving Western companies in the country.
And because they were very much respected, you really had a much more digitally savvy, savvy culture and government.
And before the war started, they created something called Dia, which is an app on your phone, which basically allows you to do most government functions, most civic functions on your phone.
They have it in Estonia and places like Singapore as well. And when the war started, Dia almost overnight went from being a sort of nice to have to an absolute bedrock of how Ukrainian society exists today.
So today you can have your passport and your driving license and file your tax forms and your bank accounts.
And you can report bombing. You can send pictures in from, you know, bash buildings.
You can take photographs of existing still still unbombed buildings to save them for future generations.
You can do just about everything on your app. And if you want to know how Ukrainian societies kept functioning, although half the population has fled through Dia.
And, you know, that's really changed culture and it's changed how people interact with each other and created a lot of, you know, horizontal interactions in ways that were unimaginable 10, 20 years ago.
So the point I'm trying to make is that, and by the way, there's also all kinds of air raid alarm systems and stuff like that as well on, you know, on people's phones.
Cultures can change more rapidly than we realize when technology comes in, particularly when there's a crisis.
And conversely, if people are mindful, you can also use your cultural base to shape how technology develops as well.
And that, in essence, is what I'm trying to argue.
AI needs a second AI, anthropology intelligence, to create augmented intelligence.
And augmented intelligence with agency is above all else what we should be aiming for today.
Thank you very much.
Thank you.
Thank you.
Thank you.
.
