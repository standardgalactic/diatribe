Probability and Computing 
Randomized Algorithms and Probabilistic Analysis 
'. 
'. 
• • . .. \ 
Michael Mitzenmacher 
Eli Upfal 

Probability and Computing 
Randomization and probabilistic techniques play an important role in modern com-
puter science, with applications ranging from combinatorial optimization and machine 
learning to communication networks and secure protocols. 
This textbook is designed to accompany a one- or two-semester course for advanced 
undergraduates or beginning graduate students in computer science and applied mathe-
matics. It gives an excellent introduction to the probabilistic techniques and paradigms 
used in the development of probabilistic algorithms and analyses. It assumes only an 
elementary background in discrete mathematics and gives a rigorous yet accessible 
treatment of the material, with numerous examples and applications. 
The first half of the book covers core material, including random sampling, expec-
tations, Markov's inequality, Chebyshev's inequality, ChernotT bounds, balls-and-bins 
models, the probabilistic method, and Markov chains. In the second half, the authors 
delve into more advanced topics such as continuous probability, applications of limited 
independence, entropy, Markov chain Monte Carlo methods. coupling, martingales, 
and balanced allocations. With its comprehensive selection of topics, along with many 
examples and exercises, this book is an indispensable teaching tool. 
Michael Mitzenmacher is John L. Loeb Associate Professor in Computer Science at 
Harvard University. He received his Ph.D. from the University of California. Berke-
ley, in 1996. Prior to joining Harvard in 1999, he was a research staff member at Digital 
Systems Research Laboratory in Palo Alto. He has received an NSF CAREER Award 
and an Alfred P. Sloan Research Fellowship. In 2002, he shared the IEEE Information 
Theory Society "Best Paper" Award for his work on error-correcting codes. 
Eli Upfal is Professor and Chair of Computer Science at Brown University. He received 
his Ph.D. from the Hebrew University, Jerusalem, Israel. Prior to joining Brown in 
1997, he was a research staff member at the IBM research division and a professor at 
the Weizmann Institute of Science in Israel. His main research interests are randomized 
computation and probabilistic analysis of algorithms, with applications to optimization 
algorithms, communication networks, parallel and distributed computing. and compu-
tational biology. 

Probability and Computing 
Randomized Algorithms and 
Probabilistic Analysis 
Michael Mitzenmacher 
Eli Upfal 
Harl'ard Unil'crsity 
Bn!\\'Il Unil'ersit\' 
CAMBRIDGE 
UNIVERSITY PRESS 

PUBLISHED BY THE PRESS SYNDICATE OF THE UNIVERSITY OF CAMBRIDGE 
The Pitt Building, Trumpington Street. Cambridge. United Kingdom 
CAMBRIDGE UNIVERSITY PRESS 
The Edinburgh Building. Cambridge CB2 2RU. UK 
40 West 20th Street. New York. NY 10011-4211. USA 
477 Williamstown Road. Port Melbourne. VIC 3207. Australia 
Ruiz de Alarc6n 13.28014 \ladrid. Spain 
Dock House. The Waterfront. Cape Town 8001. South Africa 
http://www.cambridge.org 
© Michael Mitzenmacher and Eli l'pfal 2005 
This book is in copyright. Subject to statutory exception and 
to the provisions of relevant collective licensing agreements. 
no reproduction of any part may take place \\ithllut 
the written permission of Cambridge University Press. 
First published 2005 
Printed in the United States of America 
Type/ace Times 10.5/13 pt. 
System AMS-TEX 
[FH] 
A catalog record for this book is available from the British Library. 
Library of Congress Cataloging in Publication data 
Mitzenmacher, Michael. 1969-
Probability and computing: randomized algorithms and probabilistic 
analysis / Michael Mitzenmacher. Eli Upfal. 
p. 
cm. 
Includes index. 
ISBN 0-521-83540-2 (alk. paper) 
I. Algorithms. 2. Probahilities. 3. Stochastic analysis. I. Upfal. Eli. 1954-. II. Title. 
QA274.M574 
2005 
51W.1 - dc22 
ISBN 0521 835402 hardback 
2004054540 

Contents 
Preface 
Events and Probability 
1.1 
Application: Verifying Polynomial Identities 
1.2 
Axioms of Probability 
1.3 
Application: Verifying Matrix Multiplication 
1.4 
Application: A Randomized Min-Cut Algorithm 
1.5 
Exercises 
2 Discrete Random Variables and Expectation 
2.1 
Random Variables and Expectation 
2.1.1 
Linearity of Expectations 
2.1.2 
Jensen's InequaJi ty 
2.2 
The Bernoulli and Binomial Random Variables 
2.3 
Conditional Expectation 
2.4 
The Geometric Distribution 
2.4.1 
Example: Coupon Collector's Problem 
2.5 
Application: The Expected Run-Time of Quicksort 
2.6 
Exercises 
3 Moments and Deviations 
3.1 
Markov's Inequality 
3.2 
Variance and Moments of a Random Variable 
3.2.1 
Example: Variance of a Binomial Random Variable 
3.3 
Chebyshev's Inequality 
3.3.1 
Example: Coupon Collector's Problem 
3.4 
Application: A Randomized Algorithm for Computing the Median 
3.4.1 
The Algorithm 
3.4.2 
Analysis of the Algorithm 
3.5 
Exercises 
vii 
page Xlll 
1 
3 
8 
12 
14 
20 
20 
22 
23 
25 
26 
30 
32 
34 
38 
44 
44 
45 
48 
48 
50 
52 
53 
54 
57 

CONTENTS 
4 
Chernoff Bounds 
4.1 
Moment Generating Functions 
4.2 
Deriving and Applying Chernoff Bounds 
4.2.1 
Chernoff Bounds for the Sum of Poisson Trials 
4.2.2 
Example: Coin Flips 
4.2.3 
Application: Estimating a Parameter 
4.3 
Better Bounds for Some Special Cases 
4.4 
Application: Set Balancing 
4.5* Application: Packet Routing in Sparse Networks 
4.5.1 
Permutation Routing on the Hypercube 
4.5.2 
Permutation Routing on the Butterfly 
4.6 
Exercises 
5 
Balls, Bins, and Random Graphs 
5.1 
Example: The Birthday Paradox 
5.2 
Balls into Bins 
5.2.1 
The Balls-and-Bins Model 
5.2.2 
Application: Bucket Sort 
5.3 
The Poisson Distribution 
5.3.1 
Limit of the Binomial Distribution 
5.4 
The Poisson Approximation 
5.4.1 * Example: Coupon Collector's Problem, Revisited 
5.5 
Application: Hashing 
5.5.1 
Chain Hashing 
5.5.2 
Hashing: Bit Strings 
5.5.3 
Bloom Filters 
5.5.4 
Breaking Symmetry 
5.6 
Random Graphs 
5.6.1 
Random Graph Models 
5.6.2 
Application: Hamiltonian Cycles in Random Graphs 
5.7 
Exercises 
5.8 
An Exploratory Assignment 
6 The Probabilistic Method 
6.1 
The Basic Counting Argument 
6.2 
The Expectation Argument 
6.2.1 
Application: Finding a Large Cut 
6.2.2 
Application: Maximum Satisfiability 
6.3 
Derandomization Using Conditional Expectations 
6.4 
Sample and Modify 
6.4.1 
Application: Independent Sets 
6.4.2 
Application: Graphs with Large Girth 
6.5 
The Second Moment Method 
6.5.1 
Application: Threshold Behavior in Random Graphs 
viii 
61 
61 
63 
63 
67 
67 
69 
71 
72 
73 
78 
83 
90 
90 
92 
92 
93 
94 
98 
99 
104 
106 
106 
108 
109 
112 
112 
112 
113 
118 
124 
126 
126 
128 
129 
130 
131 
133 
133 
134 
134 
135 

CONTENTS 
6.6 
The Conditional Expectation Inequality 
6.7 
The Lovasz Local Lemma 
6.7.1 
Application: Edge-Disjoint Paths 
6.7.2 
Application: Satistiability 
6.8* 
Explicit Constructions Using the Local Lemma 
6.8.1 
Application: A Satisfiability Algorithm 
6.9 
Lovasz Local Lemma: The General Case 
6.10 
Exercises 
7 
~larkov Chains and Random Walks 
7.1 
7.2 
Markov Chains: Definitions and Representations 
7.1.1 
Application: A Randomized Algorithm for 2-Satisfiability 
7.1.2 
Application: A Randomized Algorithm for 3-Satisfiability 
Classification of States 
7.2.1 
Example: The Gambler's Ruin 
7.3 
Stationary Distributions 
7.3.1 
Example: A Simple Queue 
7.4 
Random Walks on Undirected Graphs 
7.4.1 
Application: An s-t Connectivity Algorithm 
7.5 
Parrondo's Paradox 
7.6 
Exercises 
X Continuous Distributions and the Poisson Process 
8.1 
8.2 
8.3 
8.4 
Continuous Random Variables 
8.1.1 
Probability Distributions in lR 
8.1.2 
Joint Distributions and Conditional Probability 
The Uniform Distribution 
8.2.1 
Additional Properties of the Uniform Distribution 
The Exponential Distribution 
8.3.1 
Additional Properties of the Exponential Distribution 
8.3.2* Example: Balls and Bins with Feedback 
The Poisson Process 
8.4.1 
Interarrival Distribution 
8.4.2 
Combining and Splitting Poisson Processes 
8.4.3 
Conditional Arrival Time Distribution 
8.5 
Continuous Time Markov Processes 
8.6 
Example: Markovian Queues 
8.6.1 
Mj Mj I Queue in Equilibrium 
8.6.2 
MjMjljK Queue in Equilibrium 
8.6.3 
The N umber of Customers in an M j M j x Queue 
8.7 
Exercises 
'I 
Entropy, Randomness, and Information 
9.1 
The Entropy Function 
9.2 
Entropy and Binomial Coefficients 
ix 
136 
138 
141 
142 
142 
143 
146 
148 
153 
153 
156 
159 
163 
166 
167 
173 
174 
176 
177 
182 
188 
188 
188 
191 
193 
194 
196 
197 
199 
201 
204 
205 
207 
210 
212 
213 
216 
216 
219 
225 
225 
228 

CONTENTS 
9.3 
Entropy: A Measure of Randomness 
9.4 
Compression 
9.5* Coding: Shannon's Theorem 
9.6 
Exercises 
10 
The Monte Carlo Method 
230 
234 
237 
245 
252 
10.1 
The Monte Carlo Method 
252 
10.2 
Application: The DNF Counting Problem 
255 
10.2.1 
The Na"ive Approach 
255 
10.2.2 
A Fully Polynomial Randomized Scheme for ONF Counting 
257 
10.3 
From Approximate Sampling to Approximate Counting 
259 
10.4 
The Markov Chain Monte Carlo Method 
263 
10.4.1 
The Metropolis Algorithm 
265 
10.5 
Exercises 
267 
10.6 
An Exploratory Assignment on Minimum Spanning Trees 
270 
11 * Coupling of Markov Chains 
11.1 
Variation Distance and Mixing Time 
11.2 
Coupling 
11.2.1 
Example: Shuffling Cards 
11.2.2 
Example: Random Walks on the Hypercube 
11.2.3 
Example: Independent Sets of Fixed Size 
11.3 
Application: Variation Distance Is Nonincreasing 
11.4 
Geometric Convergence 
11.5 
Application: Approximately Sampling Proper Colorings 
11.6 
Path Coupling 
11.7 
Exercises 
12 
Martingales 
12.1 
Martingales 
12.2 
Stopping Times 
12.2.1 
Example: A Ballot Theorem 
12.3 
Wald's Equation 
12.4 
Tail Inequalities for Martingales 
12.5 
Applications of the Azuma-Hoeffding Inequality 
12.5.1 
General Formalization 
12.5.2 
Application: Pattern Matching 
12.5.3 
Application: Balls and Bins 
12.5.4 
Application: Chromatic Number 
12.6 
Exercises 
13 
Pairwise Independence and Universal Hash Functions 
271 
271 
274 
275 
276 
277 
278 
281 
282 
286 
289 
295 
295 
297 
299 
300 
303 
305 
305 
307 
308 
308 
309 
314 
13.1 
Pairwise Independence 
314 
13.1.1 
Example: A Construction of Pairwise Independent Bits 
315 
13.1.2 
Application: Oerandomizing an Algorithm for Large Cuts 
316 
x 

CONTENTS 
13.1.3 
Example: Constructing Pairwise Independent Values Modulo 
a Prime 
13.2 
Chebyshev's Inequality for Pairwise Independent Variables 
13.2.1 
Application: Sampling Using Fewer Random Bits 
13.3 
Families of Universal Hash Functions 
317 
318 
319 
321 
13.3.1 
Example: A 2-Universal Family of Hash Functions 
323 
13.3.2 
Example: A Strongly 2-Universal Family of Hash Functions 
324 
13.3.3 
Application: Perfect Hashing 
13.4 
Application: Finding Heavy Hitters in Data Streams 
13.5 
Exercises 
14 * Balanced Allocations 
14.1 
The Power of Two Choices 
14.1.1 
The Upper Bound 
14.2 
Two Choices: The Lower Bound 
14.3 
Applications of the Power of Two Choices 
14.3.1 
Hashing 
14.3.2 
Dynamic Resource Allocation 
14.4 
Exercises 
Further Reading 
Index 
\"ote: Asterisks indicate advanced materiaL 
xi 
326 
328 
333 
336 
336 
336 
341 
344 
344 
345 
345 
349 
350 

Preface 
\\-hy Randomness? 
\\-hy should computer scientists study and use randomness? Computers appear to 
~have far too unpredictably as it is! Adding randomness would seemingly be a dis-
.1J\antage, adding further complications to the already challenging task of efficiently 
utilizing computers. 
Science has learned in the last century to accept randomness as an essential com-
r~.)nent in modeling and analyzing nature. In physics, for example, Newton's laws led 
~uple to believe that the universe was a deterministic place; given a big enough calcu-
;.1tur and the appropriate initial conditions, one could determine the location of planets 
~ cars from now. The development of quantum theory suggests a rather different view; 
the universe still behaves according to laws, but the backbone of these laws is proba-
~llistic. "God does not play dice with the universe" was Einstein's anecdotal objection 
:,) modern quantum mechanics. Nevertheless, the prevailing theory today for subpar-
:h:k physics is based on random behavior and statistical laws, and randomness plays a 
'l~niticant role in almost every other field of science ranging from genetics and evolu-
:11 111 in biology to modeling price fluctuations in a free-market economy. 
Computer science is no exception. From the highly theoretical notion of proba-
~J!i"tic theorem proving to the very practical design of PC Ethernet cards, randomness 
.1nJ probabilistic methods playa key role in modern computer science. The last two 
Je~:ades have witnessed a tremendous growth in the use of probability theory in com-
puting. Increasingly more advanced and sophisticated probabilistic techniques have 
xen developed for use within broader and more challenging computer science appli-
.: .1tions. In this book, we study the fundamental ways in which randomness comes 
tl1 hear on computer science: randomized algorithms and the probabilistic analysis of 
.J.l~orithms. 
Randomized algorithms: Randomized algorithms are algorithms that make random 
.:hnices during their execution. In practice, a randomized program would use values 
~enerated by a random number generator to decide the next step at several branches 
xiii 

PREFACE 
of its execution. For example, the protocol implemented in an Ethernet card uses ran-
dom numbers to decide when it next tries to access the shared Ethernet communication 
medium. The randomness is useful for breaking symmetry, preventing different cards 
from repeatedly accessing the medium at the same time. Other commonly used ap-
plications of randomized algorithms include Monte Carlo simulations and primality 
testing in cryptography. In these and many other important applications, randomized 
algorithms are significantly more efficient than the best known deterministic solutions. 
Furthermore, in most cases the randomized algorithms are also simpler and easier to 
program. 
These gains come at a price; the answer may have some probability of being incor-
rect, or the efficiency is guaranteed only with some probability. Although it may seem 
unusual to design an algorithm that may be incorrect, if the probability of error is suf-
ficiently small then the improvement in speed or memory requirements may well be 
worthwhile. 
Probabilistic analysis (~f algorithms: Complexity theory tries to classify computa-
tion problems according to their computational complexity, in particular distinguish-
ing between easy and hard problems. For example, complexity theory shows that the 
Traveling Salesmen problem is NP-hard. It is therefore very unlikely that there is an 
algorithm that can solve any instance of the Traveling Salesmen problem in time that 
is subexponential in the number of cities. An embarrassing phenomenon for the clas-
sical worst-case complexity theory is that the problems it classifies as hard to compute 
are often easy to solve in practice. Probabilistic analysis gives a theoretical explanation 
for this phenomenon. Although these problems may be hard to solve on some set of 
pathological inputs, on most inputs (in particular, those that occur in real-life applica-
tions) the problem is actually easy to solve. More precisely, if we think of the input as 
being randomly selected according to some probability distribution on the collection of 
all possible inputs, we are very likely to obtain a problem instance that is easy to solve, 
and instances that are hard to solve appear with relatively small probability. Probabilis-
tic analysis of algorithms is the method of studying how algorithms perform when the 
input is taken from a well-defined probabilistic space. As we will see, even NP-hard 
problems might have algorithms that are extremely efficient on almost all inputs. 
The Book 
This textbook is designed to accompany one- or two-semester courses for advanced 
undergraduate or beginning graduate students in computer science and applied math-
ematics. The study of randomized and probabilistic techniques in most leading uni-
versities has moved from being the subject of an advanced graduate seminar meant 
for theoreticians to being a regular course geared generally to advanced undergraduate 
and beginning graduate students. There are a number of excellent advanced, research-
oriented books on this subject, but there is a clear need for an introductory textbook. 
\\"e hope that our book satisfies this need. 
The textbook has developed from courses on probabilistic methods in computer sci-
~nc~ taught at Brown (CS 155) and Harvard (CS 223) in recent years. The emphasis 
xiv 

PREFACE 
1n these courses and in this textbook is on the probabilistic techniques and paradigms, 
nnt on particular applications. Each chapter of the book is devoted to one such method 
\)r technique. Techniques are clarified though examples based on analyzing random-
1/~d algorithms or developing probabilistic analysis of algorithms on random inputs. 
\ 1any of these examples are derived from problems in networking, reflecting a promi-
n~nt trend in the networking field (and the taste of the authors). 
The book contains fourteen chapters. We may view the book as being divided into 
:\\ 0 parts, where the first part (Chapters 1-7) comprises what we believe is core mate-
nal. The book assumes only a basic familiarity with probability theory, equivalent to 
\\ hat is covered in a standard course on discrete mathematics for computer scientists. 
Chapters 1-3 review this elementary probability theory while introducing some inter-
c .... ting applications. Topics covered include random sampling, expectation, Markov's 
1I1~quality, variance, and Chebyshev's inequality. If th~ class has sufficient background 
111 probability, then these chapters can be taught quickly. We do not suggest skipping 
th~m, however, because they introduce the concepts of randomized algorithms and 
rrobabilistic analysis of algorithms and also contain several examples that are used 
throughout the text. 
Chapters 4-7 cover more advanced topics, including Chernoff bounds, balls-and-
t"lins models, the probabilistic method, and Markov chains. The material in these chap-
:cr .... is more challenging than in the initial chapters. Sections that are particularly chal-
:cnging (and hence that the instructor may want to consider skipping) are marked with 
.In asterisk. The core material in the first seven chapters may constitute the bulk of a 
'-!uarter- or semester-long course, depending on the pace. 
The second part of the book (Chapters 8-14) covers additional advanced material 
that can be used either to fill out the basic course as necessary or for a more advanced 
'l'cond course. These chapters are jargely self-contained, so the instructor can choose 
th~ topics best suited to the class. The chapters on continuous probability and en-
trnpy are perhaps the most appropriate for incorporating into the basic course. Our 
Introduction to continuous probability (Chapter 8) focuses on uniform and exponential 
JI .... tributions, including examples from queueing theory. Our examination of entropy 
,Chapter 9) shows how randomness can be measured and how entropy arises naturally 
1n the context of randomness extraction, compression, and coding. 
Chapters 10 and 11 cover the Monte Carlo method and coupling, respectively; these 
.:hapters are closely related and are best taught together. Chapter 12, on martingales, 
.:\ )\~rs important issues on dealing with dependent random variables, a theme that con-
tinues in a different vein in Chapter 13's development of pairwise independence and 
Jl'randomization. Finally, the chapter on balanced allocations (Chapter 14) covers a 
t,)pic close to the authors' hearts and ties in nicely with Chapter 5's analysis of balls-
.1nJ-bins problems. 
The order of the subjects, especially in the first part of the book, corresponds to 
~hl'ir relative importance in the algorithmic literature. Thus, for example, the study 
\)f Chernoff bounds precedes more fundamental probability concepts such as Markov 
.:hains. However, instructors may choose to teach the chapters in a different order. A 
.:,)urse with more emphasis on general stochastic processes, for example, may teach 
\1arkov chains (Chapter 7) immediately after Chapters 1-3, following with the chapter 
xv 

PREFACE 
on balls, bins, and random graphs (Chapter 5, omitting the Hamiltonian cycle exam-
ple). Chapter 6 on the probabilistic method could then be skipped, following instead 
with continuous probability and the Poisson process (Chapter 8). The material from 
Chapter 4 on Chernoff bounds. however, is needed for most of the remaining material. 
Most ofthe exercises in the book are theoreticaL but we have included some program-
ming exercises - including two more extensive exploratory assignments that require 
some programming. We have found that occasional programming exercises are often 
helpful in reinforcing the book's ideas and in adding some variety to the course. 
We have decided to restrict the material in this book to methods and techniques based 
on rigorous mathematical analysis; with few exceptions. all claims in this book are fol-
lowed by full proofs. Obviously, many extremely useful probabilistic methods do not 
fall within this strict category. For example, in the important area of Monte Carlo meth-
ods, most practical solutions are heuristics that have been demonstrated to be effective 
and efficient by experimental evaluation rather than by rigorous mathematical analy-
sis. We have taken the view that, in order to best apply and understand the strengths 
and weaknesses of heuristic methods, a firm grasp of underlying probability theory and 
rigorous techniques - as we present in this book - is necessary. We hope that students 
will appreciate this point of view by the end of the course. 
Acknowledgments 
Our first thanks go to the many probabilists and computer scientists who developed the 
beautiful material covered in this book. \\le chose not to overload the textbook with 
numerous references to the original papers. Instead. we provide a reference list that 
includes a number of excellent books giving background material as well as more ad-
vanced discussion of the topics cO\"ered here. 
The book owes a great deal to the comments and feedback of students and teaching 
assistants who took the courses CS 155 at Brown and CS 223 at Harvard. In particu-
lar we wish to thank Aris Anagnostopoulos. Eden Hochbaum, Rob Hunter, and Adam 
Kirsch. all of whom read and commented on early drafts of the book. 
Special thanks to Dick Karp. who used a draft of the book in teaching CS 174 at 
Berkeley during fall 2003. His early comments and corrections were most valuable in 
improving the manuscript. Peter Bartlett taught CS 174 at Berkeley in spring 2004, 
also providing many corrections and useful comments. 
We thank our colleagues who carefully read parts of the manuscript, pointed out 
many errors, and suggested important improvements in content and presentation: Artur 
Czumaj, Alan Frieze, Claire Kenyon. Joe Marks, Salil Vadhan, Eric Vigoda, and the 
anonymous reviewers who read the manuscript for the publisher. 
We also thank Rajeev Matwani and Prabhakar Raghavan for allowing us to use some 
of the exercises in their excellent book Randomized Algorithms. 
We are grateful to Lauren Cowles of Cambridge University Press for her editorial 
help and advice in preparing and organizing the manuscript. 
Writing of this book was supported in part by NSF ITR Grant no. CCR-0l21154. 
xvi 

CHAPTER ONE 
Events and Probability 
This chapter introduces the notion of randomized algorithms and reviews some basic 
(oncepts of probability theory in the context of analyzing the performance of simple 
randomized algorithms for verifying algebraic identities and finding a minimum cut-set 
in a graph. 
1.1. Application: Verifying Polynomial Identities 
Computers can sometimes makes mistakes, due for example to incorrect programming 
or hardware failure. It would be useful to have simple ways to double-check the results 
of computations. For some problems, we can use randomness to efficiently verify the 
(orrectness of an output. 
Suppose we have a program that multiplies together monomials. Consider the prob-
km of verifying the following identity, which might be output by our program: 
(x + l)(x - 2)(x + 3)(x - 4)(x + 5)(x - 6) ;1 x 6 -
7x 3 + 25. 
There is an easy way to verify whether the identity is correct: multiply together the 
terms on the left-hand side and see if the resulting polynomial matches the right-hand 
.... ide. In this example, when we multiply all the constant terms on the left, the result 
Joes not match the constant term on the right, so the identity cannot be valid. More 
generally, given two polynomials F(x) and G(x), we can verify the identity 
') 
F(x) ::i:: G(x) 
by converting the two polynomials to their canonical forms (L~i=O c;x;); two polyno-
mials are equivalent if and only if all the coefficients in their canonical forms are equal. 
From this point on let us assume that, as in our example, F(x) is given as a product 
F(x) = n~l=I(X - a;) and G(x) is given in its canonical form. Transforming F(x) to 
its canonical form by consecutively multiplying the ith monomial with the product of 
the first i-I monomials requires 8(d 2) multiplications of coefficients. We assume in 
1 

EVENTS AND PROBABILITY 
what follows that each multiplication can be performed in constant time, although if 
the products of the coefficients grow large then it could conceivably require more than 
constant time to add and multiply numbers together. 
So far, we have not said anything particularly interesting. To check whether the 
computer program has multiplied monomials together correctly, we have suggested 
mUltiplying the monomials together again to check the result. Our approach for check-
ing the program is to write another program that does essentially the same thing we 
expect the first program to do. This is certainly one way to double-check a program: 
write a second program that does the same thing. and make sure they agree. There 
are at least two problems with this approach, both stemming from the idea that there 
should be a difference between checking a given answer and recomputing it. First, if 
there is a bug in the program that multiplies monomials. the same bug may occur in the 
checking program. (Suppose that the checking program was written by the same per-
son who wrote the original program!) Second, it stands to reason that we would like 
to check the answer in less time than it takes to try to solve the original problem all 
over again. 
Let us instead utilize randomness to obtain a faster method to verify the identity. We 
informally explain the algorithm and then set up the formal mathematical framework 
for analyzing the algorithm. 
Assume that the maximum degree, or the largest exponent oLe in F(x) and G(x) is 
d. The algorithm chooses an integer r uniformly at random in the range {I, ... , lOOd}, 
where by "uniformly at random" we mean that all integers are equally likely to be 
chosen. The algorithm then computes the values F(r) and G(r). If F(r) i- G(r) the 
algorithm decides that the two polynomials are not equivalent, and if F(r) = G(r) the 
algorithm decides that the two polynomials are equivalent. 
Suppose that in one computation step the algorithm can generate an integer cho-
sen uniformly at random in the range {L ... . lOOd}. Computing the values of F(r) and 
G(r) can be done in O(d) time. which is faster than computing the canonical form of 
F(r). The randomized algorithm. however. may give a wrong answer. 
How can the algorithm give the wrong answer? 
If F(:r) == G(x). then the algorithm gives the correct answer, since it will find that 
F(r) = G(r) for any value of r. 
If F(x) ¢. G(x) and F(r) i- G(r). then the algorithm gives the correct answer since 
it has found a case where FCr) and G(x) disagree. Thus, when the algorithm decides 
that the two polynomials are not the same. the answer is always correct. 
If F(x) ¢. G(x) and F(r) = G(r), the algorithm gives the wrong answer. In 
other words, it is possible that the algorithm decides that the two polynomials are the 
same when they are not. For this error to occur, r must be a root of the equation 
F(x) -
G(x) = O. The degree of the polynomial F(x) -
G(x) is no larger than d 
and, by the fundamental theorem of algebra, a polynomial of degree up to d has no 
more than d roots. Thus, if F (x) ¢. G (x), then there are no more than d val ues in the 
range {l, ... , lOOd} for which F(r) = G(r). Since there are 100d values in the range 
{l, ... , lOOd}, the chance that the algorithm chooses such a value and returns a wrong 
answer is no more than 1/100. 
2 

1.2 AXIOMS OF PROBABILITY 
1.2. Axioms of Probability 
We turn now to a formal mathematical setting for analyzing the randomized algorithm. 
Any probabilistic statement must refer to the underlying probability space. 
Definition 1.1: A probability space has three components: 
1. a sample space Q, which is the set of all possible outcomes of the random process 
modeled by the probability space; 
2. afamily of sets F representing the allowable events, where each set in F is a subset 
of the sample space Q; and 
3. a probability function Pr: F -+ R satisfying Definitiol1 1.2 . 
. -\n element of Q is called a simple or elementary event. 
In the randomized algorithm for verifying polynomial identities, the sample space 
i-; the set of integers {1, ... , lOOd}. Each choice of an integer r in this range is a simple 
e\ent. 
Definition 1.2: A probability function is any function Pr: F -+ R that sati,~fies the 
t()llowing conditions: 
1. lor any event E, O.:s Pr(E) .:s 1; 
2. Pr(Q) = 1: and 
3. lor any finite or countably infinite sequence of pairwise mutually disjoint events 
E" E2, E3,· .. , 
pr( U E;) = LPr(E;) 
;:::' 
;:::' 
In most of this book we will use discrete probability spaces. In a discrete probability 
.... pace the sample space Q is finite or countably infinite, and the family F of allow-
..ihle events consists of all subsets of Q. In a discrete probability space, the probability 
function is uniquely defined by the probabilities of the simple events. 
Again, in the randomized algorithm for verifying polynomial identities, each choice 
\)( an integer r is a simple event. Since the algorithm chooses the integer uniformly at 
random, all simple events have equal probability. The sample space has lOOd simple 
\?\~nts, and the sum of the probabilities of all simple events must be 1. Therefore each 
,imple event has probability 1/ lOOd. 
Because events are sets, we use standard set theory notation to express combinations 
,)f ~vents. We write E, n E2 for the occurrence of both E, and E2 and write E, U E2 
fllr the occurrence of either E, or E2 (or both). For example, suppose we roll two dice. 
If E, is the event that the first die is a 1 and E2 is the event that the second die is a I, 
th~n E, n E2 denotes the event that both dice are 1 while E, U E2 denotes the event that 
..it l~ast one of the two dice lands on 1. Similarly, we write E, - E2 for the occurrence 
,)( an event that is in E, but not in E 2 . With the same dice example, E, - E2 consists 
,)l' the event where the first die is a 1 and the second die is not. We use the notation E 
3 

EVENTS AND PROBABILITY 
as shorthand for Q -
E; for example, if E is the event that we obtain an even number 
when rolling a die, then E is the event that we obtain an odd number. 
Definition 1.2 yields the following obvious lemma. 
Lemma 1.1: For any two events EI and E2, 
Proof' From the definition, 
Pr (E I) = Pr (E 1 -
(E 1 n E 2 )) + Pr ( E 1 n E 2 ), 
Pr (E 2) = Pr ( E 2 -
(E 1 n E 2 )) + Pr ( E 1 n E 2 ), 
Pr (E 1 U E 2) = Pr (E 1 -
(E 1 n E 2)) + Pr (E 2 -
(E 1 n E 2 )) + Pr ( E 1 n E 2 ) . 
The lemma easily follows. 
• 
A consequence of Definition 2 is known as the union bOllnd. Although it is very sim-
ple, it is tremendously useful. 
Lemma 1.2: For any finite or countably infinite sequence of events E 1, E 2 , ••• , 
pr( U E) .::0 LPr(E;) 
i 2' 1 
; 2' 1 
Notice that Lemma 1.2 differs from the third part of Definition 1.2 in that Definition 1.2 
is an equality and requires the events to be pairwise mutually disjoint. 
Lemma 1.1 can be generalized to the following equality, often referred to as the 
ill(·llisioll-e.tclilsioll prillciple. 
Lemma 1.3: Let E 1 • ••• , Ell be allY n events. Then 
pr( U 
E) = tPr(E;) - LPr(E; n Ej ) 
1=1 
1=1 
;<j 
+ L Pr (E; n Ej n E k) 
;<)<1:. 
The proof of the inclusion-exclusion principle is left as Exercise 1.7. 
We showed before that the only case in which the algorithm may fail to give the cor-
rect answer is when the two input polynomials F(x) and G(x) are not equivalent; the 
algorithm then gives an incorrect answer if the random number it chooses is a root of 
the polynomial F(x) -
G (x). Let E represent the event that the algorithm failed to 
give the correct answer. The elements of the set corresponding to E are the roots of the 
4 

1.2 AXIOMS OF PROBABILITY 
polynomial F(x) -
G(x) that are in the set of integers {l, ... , 100d}. Since the poly-
nomial has no more than d roots it follows that the event E includes no more than d 
..;imple events, and therefore 
d 
I 
Pr(algorithm fails) = Pr(E) < -- = -. 
-
100d 
100 
It may seem unusual to have an algorithm that can return the wrong answer. It may 
help to think of the correctness of an algorithm as a goal that we seek to optimize in 
conjunction with other goals. In designing an algorithm, we generally seek to mini-
mize the number of computational steps and the memory required. Sometimes there is 
J trade-off: there may be a faster algorithm that uses more memory or a slower algo-
rithm that uses less memory. The randomized algorithm we have presented gives a 
trade-off between correctness and speed. Allowing algorithms that may give an incor-
rect answer (but in a systematic way) expands the trade-off space available in designing 
.. dgorithms. Rest assured, however, that not all randomized algorithms give incorrect 
.. il1swers, as we shall see. 
For the algorithm just described, the algorithm gives the correct answer 99% of the 
time even when the polynomials are not equivalent. Can we improve this probability? 
One way is to choose the random number r from a larger range of integers. If our sam-
rk space is the set of integers {I, ... , lOOOd}, then the probability of a wrong answer 
1 .... at most 1/1000. At some point, however. the range of \'alues we can use is limited 
~y the precision available on the machine on which we run the algorithm. 
Another approach is to repeat the algorithm multiple times. using ditTerent random 
\ ~dues to test the identity. The property we use here is that the algorithm has a olle-sided 
t rmr. The algorithm may be wrong only when it outputs that the two polynomials are 
~LlLlivalent. If any run yields a number r such that F(r) i- G(r), then the polynomi-
.1J.... are not equivalent. Thus, if we repeat the algorithm a number of times and find 
Fir) i- G(r) in at least one round of the algorithm, we know that F(x) and G(x) are 
rhlt equivalent. The algorithm outputs that the two polynomials are equivalent only if 
there is equality for all runs. 
In repeating the algorithm we repeatedly choose a random number in the range 
J ...• , 100d}. Repeatedly choosing random numbers according to a given distribution 
> generally referred to as sampling. In this case, we can repeatedly choose random 
:lumbers in the range {I ..... IOOd} in two ways: we can sample either ~vith replacemellf 
\ 'r \\"itllOlit replacement. Sampling with replacement means that we do not remember 
',\ hich numbers we have already tested; each time we run the algorithm, we choose 
.! number uniformly at random from the range {I, ... , 100d} regardless of previous 
~'h~lices, so there is some chance we will choose an r that we have chosen on a previ-
,'Lb run. Sampling without replacement means that, once we have chosen a number r, 
'.\ c do not allow the number to be chosen on subsequent runs; the number chosen at a 
;1\ en iteration is uniform over all previously unselected numbers. 
Let us first consider the case where sampling is done with replacement. Assume 
:hJt we repeat the algorithm k times, and that the input polynomials are not equiva-
,~nt. What is the probability that in all k iterations our random sampling from the set 
1." .. 100d} yields roots of the polynomial F(x) -
G(x), resulting in a wrong output 
5 

EVENTS AND PROBABILITY 
by the algorithm? If k = 1, we know that this probability is at most d/ 100d = 1/100. 
If k = 2, it seems that the probability that the first iteration finds a root is 1/100 and 
the probability that the second iteration finds a root is 1/100, so the probability that 
both iterations find a root is at most O/lOO):? Generalizing, for any k, the probability 
of choosing roots for k iterations would be at most (1/ 100)k. 
To formalize this. we introduce the notion of independence. 
Definition 1.3: Two events E and F are independent ~f and only if 
Pr (E n F) = Pr ( E) . Pr (F ) . 
More generally. events E" E 2, ... , Ek are mutually independent if and only if, for any 
subsetl ~ [Lk], 
pr( n Ei) = n Pr(E,) 
;E I 
;EI 
If our algorithm samples with replacement then in each iteration the algorithm chooses 
a random number uniformly at random from the set {I, ... , 100d}, and thus the choice in 
one iteration is independent of the choices in previous iterations. For the case where the 
polynomials are not equivalent, let E; be the event that, on the ith run of the algorithm, 
we choose a root ri such that F(r;) -
G(r;) = O. The probability that the algorithm 
returns the wrong answer is given by 
Since Pre E;) is at most d/lOOd and since the events E" E2 , ... , Ek are independent, 
the probability that the algorithm gives the wrong answer after k iterations is 
The probability of making an error is therefore at most exponentially small in the num-
ber of trials. 
Now let us consider the case where sampling is done without replacement. In this 
case the probability of choosing a given number is conditioned on the events of the 
previous iterations. 
Definition 1.4: The conditional probability that event E occurs given that event F 
occurs is 
Pr (E IF) = Pr (E n F) . 
Pr(F) 
The conditional probabili(v is well-defined only (f Pre F) > O. 
Intuitively, we are looking for the probability of E n F within the set of events defined 
by F. Because F defines our restricted sample space, we normalize the probabilities 
by dividing by Pr(F), so that the sum of the probabilities of all events is 1. When 
Pre F) > 0, the definition can also be written in the useful form 
Pr (E IF) Pr ( F) = Pr (E n F). 
6 

1.2 AXIOMS OF PROBABILITY 
~otice that, when E and F are independent and Pr(F) =I- 0, we have 
Pr (E n F) 
Pr ( E ) Pr ( F ) 
Pr(E I F) = 
= 
= Pr(E). 
Pr(F) 
Pr(F) 
This is a property that conditional probability should have; intuitively, if two events are 
Independent, then information about one event should not affect the probability of the 
'econd event. 
Again assume that we repeat the algorithm k times and that the input polynomials 
.ire not equivalent. What is the probability that in all the k iterations our random sam-
pling from the set {l, ... , lOOd} yields roots of the polynomial F(x) - G(x), resulting 
In a wrong output by the algorithm? 
As in the analysis with replacement, we let Ei be the event that the random num-
~r ri chosen in the ith iteration of the algorithm is a root of F(x) - G(x); again, the 
probability that the algorithm returns the wrong answer is given by 
Pr (E I n E 2 n ... n E,,). 
-\pplying the definition of conditional probability, we obtain 
.md repeating this argument gives 
Pr (E I n E 2 n ... n E,,) 
= Pr (E I) . Pr ( E 2 lEd . Pr ( E 3 I E I n E 2) ... Pr (E" I E I n E 2 n ... n E" -I) . 
Can we bound Pr(Ej I EI n E2 n ... n E)-I)? Recall that there are at most d values 
r for which F(r) - G(r) = 0; if trials 1 through j - I < d have found j - 1 of them, 
rhen when sampling without replacement there are only d - (j -
1) values out of the 
IOOd - (j -
1) remaining choices for which F(r) -
G(r) = O. Hence 
Pr(E· I EI n E2 n ... n E'_I) < 
d - (j -
1) 
} 
} 
-
100d - (j -
1) , 
.md the probability that the algorithm gives the wrong answer after k :S d iterations is 
tlounded by 
k 
d _ (j _ 1) 
(I)" 
Pr(EI n E2 n ... n EI.) < n 
< 
-
. 
K 
-, 100d - (j - I) -
100 
J=I 
Because (d-(j-l))/OOOd-(j-I)) < d/100d when j > I,ourboundson theprob-
.lbility of making an error are actually slightly better without replacement. You may 
.llso notice that, if we take d + 1 samples without replacement and the two polynomi-
.lIs are not equivalent, then we are guaranteed to find an r such that F(r) - G(r) =I- O. 
Thus, in d + 1 iterations we are guaranteed to output the correct answer. However, com-
puting the value of the polynomial at d + I points takes 0) (d 2 ) time using the standard 
.lpproach, which is no faster than finding the canonical form deterministically. 
Since sampling without replacement appears to give better bounds on the probabil-
ity of error, why would we ever want to consider sampling with replacement? In some 
7 

EVENTS AND PROBABILITY 
cases, sampling with replacement is significantly easier to analyze, so it may be worth 
considering for theoretical reasons. In practice, sampling with replacement is often 
simpler to code and the etlect on the probability of making an error is almost negligi-
ble, making it a desirable alternative. 
1.3. Application: Verifying Matrix Multiplication 
\Ve now consider another example where randomness can be used to verify an equal-
ity more quickly than the known deterministic algorithms. Suppose we are given three 
II x II matrices A, B, and C. For convenience, assume we are working over the integers 
modulo 2. We want to verify whether 
AB=C. 
One way to accomplish this is to multiply A and B and compare the result to C. The 
simple matrix multiplication algorithm takes (>-<)(1/3) operations. There exist more so-
phisticated algorithms that are known to take roughly (0 (n 2.37 ) operations. 
Once again, we use a randomized algorithm that allows for faster verification - at the 
expense of possibly returning a wrong answer with small probability. The algorithm is 
similar in spirit to our randomized algorithm for checking polynomial identities. The 
algorithm chooses a random vector r = (rl, r2,.'" r ll ) E {a, l}1l. It then computes ABr 
by first computing Br and then A( Br), and it also computes Cr. If A(Bi) =I=- Cr, then 
AB =I=- C. Otherwise, it returns that AB = C. 
The algorithm requires three matrix-vector multiplications, which can be done in 
time 8 (n 2 ) in the obvious way. The probability that the algorithm returns that AB = 
C when they are actually not equal is bounded by the following theorem. 
Theorem 1.4: ff AB =I=- C and (fr is chosen un(forml.v at random from {a, l}1l, then 
I 
Pr(ABr = Cr) < -. 
-
2 
Proof: Before beginning, we point out that the sample space for the vector r is the set 
{a, I}II and that the event under consideration is ABr = Cr. We also make note of the 
following simple but useful lemma. 
Lemma 1.5: Choosing r = (rl, r2, ... , r17 ) E {O, 1}1I un(fonnly at random is equivalent 
to choosing each ri independently and uniforml.vfrom {a, I}. 
Proof' If each ri is chosen independently and uniformly at random, then each of the 
211 possible vectors r is chosen with probability 2-
17
, giving the lemma. 
D 
Let D = AB - C =I=- 0. Then ABr = Cr implies that Dr = 0. Since D =I=-° 
it must 
have some nonzero entry; without loss of generality, let that entry be d ll . 
8 

1.3 APPLICATION: VERIFYING MATRIX MULTIPLICATION 
For Dr = 0, it must be the case that 
or, equivalently, 
/1 
Ldljrj = ° 
j=1 
rl =- 2:.)=2 dljrj 
d ll 
(1.1 ) 
Now we introduce a helpful idea. Instead of reasoning about the vector r, suppose 
that we choose the rk independently and uniformly at random from {O, I} in order, from 
/'/1 down to r I. Lemma 1.5 says that choosing the rk in this way is equivalent to choos-
ing a vector r uniformly at random. Now consider the situation just before r 1 is chosen. 
At this point, the right-hand side of Eqn. (1.1) is determined, and there is at most one 
choice for r 1 that will make that equality hold. Since there are two choices for r I, the 
equality holds with probability at most 1/2, and hence the probability that ABr = Cr 
is at most 1/2. By considering all variables besides rl as having been set, we have re-
duced the sample space to the set of two values {a, I} for rl and have changed the event 
being considered to whether Eqn. (1.1) holds. 
This idea is called the principle of deferred decisions. When there are several ran-
dom variables, such as the ri of the vector r, it often helps to think of some of them 
as being set at one point in the algorithm with the rest of them being left random - or 
deferred - until some further point in the analysis. Formally. this corresponds to con-
ditioning on the revealed values; when some of the random variables are revealed, we 
must condition on the revealed values for the rest of the analysis. We will see further 
examples of the principle of deferred decisions later in the book. 
To formalize this argument, we first introduce a simple fact, known as the law of 
total probability. 
Theorem 1.6 [Law of Total Probability]: Let E 1, E2 , ••• , E/1 be mutually disjoint 
(Tents in the sample space Q, and let U ;1= 1 Ei = Q. Then 
11 
11 
i=1 
i=1 
Proof: Since the events B n E; (i = 1, ... , n) are disjoint and cover the entire sample 
space Q, it follows that 
Further, 
11 
11 
Pr(B) = L 
Pr(B n E;). 
;=1 
11 
LPr(B n E;) = LPr(B I Ei)Pr(E;) 
;=1 
;=1 
by the definition of conditional probability. 
9 
D 

EVENTS AND PROBABILITY 
Now, using this law and summing over all collections of values (X2, X3, X4, ... , Xn) E 
{O, l}11-1 yields 
Pr(ABi = Cr) 
L 
Pr((ABi = Ci) n ((r2, ... ,rn) = (X:2, .. "X/1))) 
\\:.. 
\,. I"" (O.ll"-l 
< 
< 
1 
L 
"2 Pr ( (r2 , ... , rll ) = (X:2, ... , X II )) 
(x=, ..... Xn)E{O, l}"-l 
2 
Here we have used the independence of r I and (r2, ... , rll ) in the fourth line. 
• 
To improve on the error probability of Theorem 1.4, we can again use the fact that the 
algorithm has a one-sided error and run the algorithm multiple times. If we ever find 
an i such that ABi =I- Ci, then the algorithm will correctly return that AB =I- C. If we 
always find ABr = Cr, then the algorithm returns that AB = C and there is some 
probability of a mistake. Choosing r with replacement from {O, 1}11 for each trial, we 
obtain that, after k trials, the probability of error is at most 2 -k. Repeated trials increase 
the running time to B(kn:2). 
Suppose we attempt this verification 100 times. The running time of the random-
ized checking algorithm is still 8(n2), which is faster than the known deterministic 
algorithms for matrix multiplication for sufficiently large n. The probability that an in-
correct algorithm passes the verification test 100 times is 2- 100, an astronomically small 
number. In practice, the computer is much more likely to crash during the execution 
of the algorithm than to return a wrong answer. 
An interesting related problem is to evaluate the gradual change in our confidence in 
the correctness of the matrix multiplication as we repeat the randomized test. Toward 
that end we introduce Bayes' law. 
Theorem 1.7 [Bayes' Law]: Assume that E I , E 2 , ... , Ell are mutually disjoint sets 
such that Ul'~1 Ei = E. Then 
Pr(E. I B) = Pr(Ej n B) 
} 
Pr(B) 
Pr(B I Ej ) Pr(Ej ) 
As a simple application of Bayes' law, consider the following problem. We are given 
three coins and are told that two of the coins are fair and the third coin is biased, land-
ing heads with probability 2/3. We are not told which of the three coins is biased. We 
10 

1.3 APPLICATION: VERIFYING MATRIX MULTIPLICATION 
permute the coins randomly, and then flip each of the coins. The first and second coins 
come up heads, and the third comes up tails. What is the probability that the first coin 
is the biased one? 
The coins are in a random order and so, before our observing the outcomes of the 
coin flips, each of the three coins is equally likely to be the biased one. Let Ei be the 
event that the ith coin flipped is the biased one, and let B be the event that the three 
coin flips came up heads, heads, and tails. 
Before we flip the coins we have Pr(E[) = 1/3 for all i. We can also compute the 
probability of the event B conditioned on Ei : 
2 
1 
I 
1 
Pr (B IE,) = Pr (B I E 2) = :3 . 2 . 2 = (;, 
and 
1 1 1 
I 
Pr (B I E,\) = - . - . - = -. 
-
2 
2 
3 
12 
Applying Bayes' law, we have 
Pr(B I E,) Pr(E,) 
2 
Pr(E, I B) = -3-------
Li=' Pr(B I E[) Pr(E[) 
5 
Thus, the outcome of the three coin flips increases the likelihood that the first coin is 
the biased one from 1/3 to 2/5. 
Returning now to our randomized matrix multiplication test, we want to evaluate 
the increase in confidence in the matrix identity obtained through repeated tests. In 
the Bayesian approach one starts with a prior model, giving some initial value to the 
model parameters. This model is then modified, by incorporating new observations, to 
obtain a posterior model that captures the new information. 
In the matrix multiplication case, if we have no information about the process that 
generated the identity then a reasonable prior assumption is that the identity is correct 
\vith probability 1/2. If we run the randomized test once and it returns that the matrix 
identity is correct, how does this change our confidence in the identity? 
Let E be the event that the identity is correct, and let B be the event that the test re-
turns that the identity is correct. We start with Pr(E) = Pr(E) = 1/2, and since the 
test has a one-sided error bounded by 1/2, we have Pr(B I E) = 1 and Pr(B I E) :s 
1/2. Applying Bayes' law yields 
Pr(B I E) Pr(E) 
1/2 
Pr(E I B) = 
> -----
Pr (B IE) Pr (E) + Pr (B IE) Pr (E ) 
1/2 + 1/2 . 1/2 
2 
3 
Assume now that we run the randomized test again and it again returns that the iden-
tity is correct. After the first test, I may naturally have revised my prior model, so that 
I believe Pr(E) ~ 2/3 and Pr(E) :s 1/3. Now let B be the event that the new test 
returns that the identity is correct; since the tests are independent, as before we have 
Pr(B I E) = 1 and Pr(B I E) :s 1/2. Applying Bayes' law then yields 
2/3 
4 
Pr (E I B) ~ 2/3 + 1/3 . 1/2 = 5' 
11 

EVENTS AND PROBABILITY 
In general: If our prior model (before running the test) is that Pr(E) ~ 2i/(2 i + I) 
and if the test returns that the identity is correct (event B), then 
2i + I 
Pr(E I B) ~ ------
2i 
1 
I 
--+---
2i + I 
2 2i + I 
2i + 1 
I 
---= 1--.-. 
2i+1 + 1 
21 + I 
Thus, if all 100 calls to the matrix identity test return that the identity is correct, our 
confidence in the correctness of this identity is at least 1- 1/(2 100 + I). 
1.4. Application: A Randomized Min-Cut Algorithm 
A cut-set in a graph is a set of edges whose removal breaks the graph into two or 
more connected components. Given a graph G = (V, E) with 11 vertices, the minimum 
cut - or min-cut - problem is to find a minimum cardinality cut-set in G. Minimum 
cut problems arise in many contexts, including the study of network reliability. In the 
case where nodes correspond to machines in the network and edges correspond to con-
nections between machines, the min-cut is the smallest number of edges that can fail 
before some pair of machines cannot communicate. Minimum cuts also arise in clus-
tering problems. For example, if nodes represent Web pages (or any documents in a 
hypertext-based system) and two nodes have an edge between them if the correspond-
ing nodes have a hyperlink between them, then small cuts divide the graph into clusters 
of documents with few links between clusters. Documents in different clusters are 
likely to be unrelated. 
We shall proceed by making use of the definitions and techniques presented so far in 
order to analyze a simple randomized algorithm for the min-cut problem. The main op-
eration in the algorithm is edge contraction. In contracting an edge {u, v} we merge the 
two vertices u and v into one vertex, eliminate all edges connecting u and v, and retain 
all other edges in the graph. The new graph may have parallel edges but no self-loops. 
Examples appear in Figure 1.1, where in each step the dark edge is being contracted. 
The algorithm consists of 11 -
2 iterations. In each iteration, the algorithm picks an 
edge from the existing edges in the graph and contracts that edge. There are many pos-
sible ways one could choose the edge at each step. Our randomized algorithm chooses 
the edge uniformly at random from the remaining edges. 
Each iteration reduces the number of vertices in the graph by one. After 11 -
2 it-
erations, the graph consists of two vertices. The algorithm outputs the set of edges 
connecting the two remaining vertices. 
It is easy to verify that any cut-set of a graph in an intermediate iteration of the algo-
rithm is also a cut-set of the original graph. On the other hand, not every cut-set of the 
original graph is a cut-set of a graph in an intermediate iteration, since some edges of 
the cut-set may have been contracted in previous iterations. As a result, the output of 
the algorithm is always a cut-set of the original graph but not necessarily the minimum 
cardinality cut-set (see Figure 1.1). 
12 

1.4 APPLICATION: A RANDOMIZED MIN-CUT ALGORITHM 
~5 ~5gi?5 <J5 
1,2,3,4 
242 
2 
(a) A successful run of min-cut. 
1 
3 
1 
1 
~ 
~5 ~5 
~45 
~ 
~ 
~ 2,3,4,5 
242 
2 
(b) An unsucces"ful run of min-cut. 
Figure 1.1: An example of two executions of min-cut in a graph with minimum cut-set of size 2. 
We now establish a lower bound on the probability that the algorithm returns a cor-
rect output. 
Theorem 1.8: The algorithm outputs a mill-cut set with prohability at least 2/11(11 -
I). 
Proof: Let k be the size of the min-cut set of G. The graph may have several cut-sets 
of minimum size. We compute the probability of finding one specific sLlch set C. 
Since C is a cut-set in the graph, removal of the set C partitions the set of vertices 
into two sets, S and V - S, such that there are no edges connecting \'ertices in S to 
\ertices in V - S. Assume that, throughout an execution of the algorithm, we contract 
only edges that connect two vertices in S or two vertices in V - 5, but not edges in C. 
In that case, all the edges eliminated throughout the execution will be edges connect-
ing vertices in S or vertices in V - S, and after 11 - 2 iterations the algorithm returns a 
graph with two vertices connected by the edges in C. We may therefore conclude that, 
if the algorithm never chooses an edge of C in its n - 2 iterations, then the algorithm 
returns C as the minimum cut-set. 
This argument gives some intuition for why we choose the edge at each iteration 
uniformly at random from the remaining existing edges. If the size of the cut C is small 
and if the algorithm chooses the edge uniformly at each step, then the probability that 
the algorithm chooses an edge of C is small - at least when the number of edges re-
maining is large compared to C. 
Let Ei be the event that the edge contracted in iteration i is not in C, and let Fi = 
n~=, Ej be the event that no edge of C was contracted in the first i iterations. We need 
to compute Pr(F,1-2). 
We start by computing Pr(E,) = Pr(F,). Since the minimum cut-set has kedges, 
all vertices in the graph must have degree k or larger. If each vertex is adjacent to at 
least k edges, then the graph must have at least 11k /2 edges. The first contracted edge 
is chosen uniformly at random from the set of all edges. Since there are at least nk/2 
edges in the graph and since C has k edges, the probability that we do not choose an 
edge of C in the first iteration is given by 
13 

EVENTS AND PROBABILITY 
2k 
2 
Pr (E I) = Pr (F I ) ~ 1 -
-
= 1 - -. 
nk 
n 
Let us suppose that the first contraction did not eliminate an edge of C. In other 
words, we condition on the event Fl. Then, after the first iteration, we are left with an 
(n - I)-node graph with minimum cut-set of size k. Again, the degree of each vertex in 
the graph must be at least k, and the graph must have at least k(n - 1)/2 edges. Thus, 
k 
2 
Pr (E J 
I F I ) > I -
= I - --. 
~ 
-
k(n - 1)/2 
n - I 
Similarly, 
k 
2 
Pr(E· I F I) > 1 -
= I - ---
I 
1-
-
k(n - i + 1)/2 
n - i + I' 
To compute Pr(Fn- 2 ), we use 
Pr ( Fn - 2) = Pr ( E n - 2 n FI/ - 3) = Pr ( Ell - 2 I Fn - 3) . Pr ( F, 1- 3 ) 
= Pr(EIl - 2 I F,1-3) . Pr(En- 3 I F,1-4)'" Pr(E2 I F1) • Pr(FI) 
1l-2( 
2) n-2( 
. 1) 
~n I-n-i+I =n :=~~I 
1=1 
1=1 
= c 
~ 2)(: = 
~)C = 
~) ... G)G)(DG) 
2 
• 
n(n -
I) 
Since the algorithm has a one-sided error, we can reduce the error probability by repeat-
ing the algorithm. Assume that we run the randomized min-cut algorithm 11 (n - 1) In n 
times and output the minimum size cut-set found in all the iterations. The probability 
that the output is not a min-cut set is bounded by 
(1-
2 )n(n-I)lnn 
JI 
I 
< e-- nil = _ 
l1(n -
I) 
-
n2' 
In the first inequality we have used the fact that I - x .:s e-x . 
1.5. Exercises 
Exercise 1.1: We flip a fair coin ten times. Find the probability of the following events. 
(a) The number of heads and the number of tails are equal. 
(b) There are more heads than tails. 
(c) The ith flip and the (11 - i )th flip are the same for i = I, ... , 5. 
(d) We flip at least four consecutive heads. 
14 

1.5 EXERCISES 
Exercise 1.2: We roll two standard six-sided dice. Find the probability of the follow-
ing events, assuming that the outcomes of the rolls are independent. 
(a) The two dice show the same number. 
( b) The number that appears on the first die is larger than the number on the second. 
(e) The sum of the dice is even. 
(d) The product of the dice is a perfect square. 
Exercise 1.3: We shuffle a standard deck of cards, obtaining a permutation that is uni-
form over all 52! possible permutations. Find the probability of the following events. 
(a) The first two cards include at least one ace. 
(b) The first five cards include at least one ace. 
(e) The first two cards are a pair of the same rank. 
(d) The first five cards are all diamonds. 
(e) The first five cards form a full house (three of one rank and two of another rank). 
Exercise 1.4: We are playing a tournament in which we stop as soon as one of us wins 
1/ games. We are evenly matched, so each of us wins any game with probability 1/2, 
independently of other games. What is the probability that the loser has won k games 
\\hen the match is over? 
Exercise 1.5: After lunch one day, Alice suggests to Bob the following method to de-
termine who pays. Alice pulls three six-sided dice from her pocket. These dice are not 
the standard dice, but have the following numbers on their faces: 
• die A - L L 6, 6, 8, 8: 
• die B-2, 2,4,4, 9, 9; 
• dieC-3,3,5,5,7,7. 
The dice are fair, so each side comes up with equal probability. Alice explains that 
.-\lice and Bob will each pick up one of the dice. They will each roll their die, and the 
nne who rolls the lowest number loses and will buy lunch. So as to take no advantage, 
Alice offers Bob the first choice of the dice. 
(a) Suppose that Bob chooses die A and Alice chooses die B. Write out all of the pos-
sible events and their probabilities, and show that the probability that Alice wins 
is greater than 1/2. 
(b) Suppose that Bob chooses die B and Alice chooses die C. Write out all of the pos-
sible events and their probabilities, and show that the probability that Alice wins 
is greater than 1/2. 
(e) Since die A and die B lead to situations in Alice's favor. it would seem that Bob 
should choose die C. Suppose that Bob does choose die C and Alice chooses die A. 
Write out all of the possible events and their probabilities, and show that the prob-
ability that Alice wins is still greater than 1/2. 
15 

EVENTS AND PROBABILITY 
Exercise 1.6: Consider the following balls-and-bin game. We start with one black ball 
and one white ball in a bin. We repeatedly do the following: choose one ball from the 
bin uniformly at random, and then put the ball back in the bin with another ball of the 
same color. We repeat until there are n balls in the bin. Show that the number of white 
balls is equally likely to be any number between 1 and!1 -
1. 
Exercise 1.7: (a) Prove Lemma 3, the inclusion-exclusion principle. 
( b) Prove that. when e is odd, 
pr(UE).::o tPr(E;)- LPr(E;nEj ) 
i=1 
i=1 
i<j 
+ L Pr (Ei n E j n E k) 
i<j<k 
- ... +(_1)£+1 
il<12<"'<i l 
(c) Prove that, when e is even, 
+ L Pr(Ei n Ej n E k ) 
i<j<k 
_ ... + (_1/+ 1 
Exercise 1.8: I choose a number uniformly at random from the range [1,1,000,000]. 
Using the inclusion-exclusion principle, determine the probability that the number cho-
sen is divisible by one or more of 4, 6, and 9. 
Exercise 1.9: Suppose that a fair coin is flipped n times. For k > 0, find an upper 
bound on the probability that there is a sequence of log2 n + k consecutive heads. 
Exercise 1.10: I have a fair coin and a two-headed coin. I choose one of the two coins 
randomly with equal probability and flip it. Given that the flip was heads, what is the 
probability that I flipped the two-headed coin? 
Exercise 1.11: I am trying to send you a single bit, either a 0 or a 1. When I transmit 
the bit, it goes through a series of n relays before it arrives to you. Each relay flips the 
bit independently with probability p. 
(a) Argue that the probability you receive the correct bit is 
16 

1.S EXERCISES 
(b) We consider an alternative way to calculate this probability. Let us say the relay 
has bias q if the probability it flips the bit is (1 - q)j2. The bias q is therefore a 
real number in the range [-1, 1]. Prove that sending a bit through two relays with 
bias q, and q2 is equivalent to sending a bit through a single relay with bias q, q2. 
(c) Prove that the probability you receive the correct bit when it passes through n re-
lays as described before (a) is 
1 + (2p -
1)1/ 
2 
Exercise 1.12: The following problem is known as the Monty Hall problem, after the 
host of the game show "Let's Make a Deal". There are three curtains. Behind one 
curtain is a new car, and behind the other two are goats. The game is played as fol-
lows. The contestant chooses the curtain that she thinks the car is behind. Monty then 
opens one of the other curtains to show a goat. (Monty may have more than one goat 
to choose from; in this case, assume he chooses which goat to show uniformly at ran-
dom.) The contestant can then stay with the curtain she originally chose or switch to 
the other unopened curtain. After that, the location of the car is revealed, and the con-
testant wins the car or the remaining goat. Should the contestant switch curtains or not, 
m does it make no difference? 
Exercise 1.13: A medical company touts its new test for a certain genetic disorder. 
The false negative rate is small: if you have the disorder, the probability that the test 
returns a positive result is 0.999. The false positive rate is also small: if you do not 
have the disorder, the probability that the test returns a positive result is only 0.005. 
Assume that 2% of the population has the disorder. If a person chosen uniformly from 
the population is tested and the result comes back positive, what is the probability that 
the person has the disorder? 
Exercise 1.14: I am playing in a racquetball tournament, and I am up against a player 
I have watched but never played before. I consider three possibilities for my prior 
model: we are equally talented, and each of us is equally likely to win each game; I 
am slightly better, and therefore I win each game independently with probability 0.6; 
m he is slightly better, and thus he wins each game independently with probability 0.6. 
Before we play, I think that each of these three possibilities is equally likely. 
In our match we play until one player wins three games. I win the second game, but 
he wins the first, third, and fourth. After this match, in my posterior modeL with what 
probability should I believe that my opponent is slightly better than I am'? 
Exercise 1.15: Suppose that we roll ten standard six-sided dice. What is the probabil-
ity that their sum will be divisible by 6, assuming that the rolls are independent? (Hint: 
l'se the principle of deferred decisions, and consider the situation after rolling all but 
one of the dice.) 
Exercise 1.16: Consider the following game, played with three standard six-sided dice. 
If the player ends with all three dice showing the same number, she wins. The player 
17 

EVENTS AND PROBABILITY 
starts by rolling all three dice. After this first rolL the player can select anyone, two, 
or all of the three dice and re-roll them. After this second roll, the player can again se-
lect any of the three dice and re-roll them one final time. For questions (a)-(d), assume 
that the player uses the following optimal strategy: if all three dice match, the player 
stops and wins: if two dice match, the player re-rolls the die that does not match; and 
if no dice match. the player re-rolls them all. 
(a) Find the probability that all three dice show the same number on the first roll. 
(b) Find the probability that exactly two of the three dice show the same number on 
the first roll. 
(c) Find the probability that the player wins, conditioned on exactly two of the three 
dice showing the same number on the first roll. 
(d) By considering all possible sequences of rolls, find the probability that the player 
wins the game. 
Exercise 1.17: In our matrix multiplication algorithm, we worked over the integers 
modulo 2. Explain how the analysis would change if we worked over the integers mod-
ulo k for k > 2. 
Exercise 1.18: We have a function F: {O, ... , n - I} ~ {O, ... , m - I}. We know that, 
for ° :s x, y :s n -
1, F((x + y) modn) = (F(x) + F(y)) modm. The only way we 
have for evaluating F is to use a lookup table that stores the values of F. Unfortunately, 
an Evil Adversary has changed the value of 1/5 of the table entries when we were not 
looking. 
Describe a simple randomized algorithm that. given an input z, outputs a value that 
equals F(z) with probability at least 1/2. Your algorithm should work for every value 
of z, regardless of what values the Adversary changed. Your algorithm should use as 
few lookups and as little computation as possible. 
Suppose I allow you to repeat your initial algorithm three times. What should you 
do in this case, and what is the probability that your enhanced algorithm returns the 
correct answer? 
Exercise 1.19: Give examples of events where Pr(A I B) < Pr(A), Pr(A I B) 
Pr(A), and Pr(A I B) > Pr(A). 
Exercise 1.20: Show that, if E" E 2 , ••• , Ell are mutually independent, then so are 
-
-
-
E"E2,···,E II • 
Exercise 1.21: Give an example of three random events X, y, Z for which any pair are 
independent but all three are not mutually independent. 
Exercise 1.22: (a) Consider the set {l, ... , n}. We generate a subset X of this set as fol-
lows: a fair coin is flipped independently for each element of the set: if the coin lands 
heads then the element is added to X, and otherwise it is not. Argue that the resulting 
set X is equally likely to be anyone of the 2" possible subsets. 
18 

I.S EXERCISES 
(b) Suppose that two sets X and Yare chosen independently and uniformly at ran-
dom from all the 211 subsets of {l, ... ,n}. Determine Pr(X ~ Y) and Pr(X U Y = 
11 .... , n D. (Hint: Use the part (a) of this problem.) 
Exercise 1.23: There may be several different min-cut sets in a graph. Using the analy-
sis of the randomized min-cut algorithm, argue that there can be at most n(n -
1)/2 
distinct min-cut sets. 
Exercise 1.24: Generalizing on the notion of a cut-set, we define an r-way cut-set in a 
graph as a set of edges whose removal breaks the graph into r or more connected com-
ponents. Explain how the randomized min-cut algorithm can be used to find minimum 
r-way cut-sets, and bound the probability that it succeeds in one iteration. 
Exercise 1.25: To improve the probability of success of the randomized min-cut algo-
rithm, it can be run multiple times. 
(a) Consider running the algorithm twice. Determine the number of edge contractions 
and bound the probability of finding a min-cut. 
(b) Consider the following variation. Starting with a graph with n vertices. first con-
tract the graph down to k vertices using the randomized min-cut algorithm. Make 
copies of the graph with k vertices, and now run the randomized algorithm on this 
reduced graph e times, independently. Determine the number of edge contractions 
and bound the probability of finding a minimum cut. 
(e) Find optimal (or at least near-optimal) values of k and t for the \ariation in (b) that 
maximize the probability of finding a minimum cut while using the same number 
of edge contractions as running the original algorithm twice. 
Exercise 1.26: Tic-tac-toe always ends up in a tie if players play optimally. Instead, 
we may consider random variations of tic-tac-toe. 
(a) First variation: Each of the nine squares is labeled either X or 0 according to an 
independent and uniform coin flip. If only one of the players has one (or more) 
winning tic-tac-toe combinations, that player wins. Otherwise. the game is a tie. 
Determine the probability that X wins. (You may want to use a computer program 
to help run through the configurations.) 
(b) Second variation: X and 0 take turns, with the X player going first. On the X 
player's turn, an X is placed on a square chosen independently and uniformly at 
random from the squares that are still vacant; 0 plays similarly. The first player 
to have a winning tic-tac-toe combination wins the game. and a tie occurs if nei-
ther player achieves a winning combination. Find the probability that each player 
wins. (Again, you may want to write a program to help you.) 
19 

CHAPTER TWO 
Discrete Random Variables 
and Expectation 
In this chapter, we introduce the concepts of discrete random variables and expectation 
and then develop basic techniques for analyzing the expected performance of algo-
rithms. We apply these techniques to computing the expected running time of the well-
known Quicksort algorithm. In analyzing two versions of Quicksort. we demonstrate 
the distinction between the analysis of randomized algorithms, where the probability 
space is defined by the random choices made by the algorithm. and the probabilistic 
analysis of deterministic algorithms, where the probability space is defined by some 
probability distribution on the inputs. 
Along the way we define the Bernoulli, binomial. and geometric random variables, 
study the expected size of a simple branching process, and analyze the expectation of 
the coupon collector's problem - a probabilistic paradigm that reappears throughout 
the book. 
2.1. Random Variables and Expectation 
When studying a random event. we are often interested in some value associated with 
the random event rather than in the event itself. For example, in tossing two dice we 
are often interested in the sum of the two dice rather than the separate value of each die. 
The sample space in tossing t\\"O dice consists of 36 events of equal probability, given 
by the ordered pairs of numbers {(I,l), (1,2), ... , (6,5), (6, 6)}. If the quantity we are 
interested in is the sum of the two dice. then we are interested in 11 events (of unequal 
probability): the 11 possible outcomes of the sum. Any such function from the sample 
space to the real numbers is called a random variable. 
Definition 2.1: A random variable X on a sample space Q is a real-valued function 
on Q; that is, X: Q ---+ ITt. A discrete random variable is a random variable that takes 
on only a finite or countably infinite number of values. 
Since random variables are functions, they are usually denoted by a capital letter such 
as X or Y, while real numbers are usually denoted by lowercase letters. 
20 

2.1 RANDOM VARIABLES AND EXPECTATION 
For a discrete random variable X and a real value a, the event "X = a" includes all 
the basic events of the sample space in which the random variable X assumes the value 
d. That is, "X =a"representstheset{sEQ I X(s) =a}. We denote the probability 
of that event by 
Pr(X = a) = 
L 
Pr(s). 
\'EQ:X(s)=a 
I f X is the random variable representing the sum of the two dice, then the event X = 4 
corresponds to the set of basic events {(l, 3), (2.2), (3, l)}. Hence 
3 
I 
Pr( X = 4) = -
= -. 
36 
12 
The definition of independence that we developed for events extends to random 
\ariables. 
Definition 2.2: Two random variables X and Yare independent ~f and only ~f 
Pr((X = x) n (Y = y)) = Pr(X = x) . Pr(Y = y) 
f(JI' all values x and y. Similarly, random variables Xl. X 2 •...• X" are mutually inde-
fJendent if and only ~t; for any subset / ~ [1. k 1 and any wilues Xi, i E /, 
pr( n X; = x;) = n Pr( X, = X;). 
iE I 
I E I 
:\ basic characteristic of a random variable is its expectation. The expectation of a 
random variable is a weighted average of the values it assumes, where each value is 
weighted by the probability that the variable assumes that value. 
Definition 2.3: The expectation of a discrete random variable X, denoted by E[Xl, is 
~i\'en by 
E[Xl = L i Pr(X = i), 
lI'here the summation is over all values in the range of X. The expectation is finite (I' 
\"' i Ii I Pr(X = i) converges; otherwise, the expectation is unbounded. 
For example, the expectation of the random variable X representing the slim of two 
Jice is 
1 
2 
3 
1 
E[Xl = -
. 2 + -
. 3 + -
·4+ ... + -
. 12 = 7. 
36 
36 
36 
36 
You may try using symmetry to give simpler argument for why E[X] = 7. 
As an example of where the expectation of a discrete random variable is unbounded, 
-:onsider a random variable X that takes on the value 2i with probability 1/21 for i = 
I. :2, .... The expected value of X is 
21 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
E[XJ = f: ;,2' = f: 1 = 00. 
i=l 
i=l 
Here we use the somewhat informal notation E[X] = 00 to express that E[X] IS 
unbounded. 
2.1.1. Linearity of Expectations 
A key property of expectation that significantly simplifies its computation is the linear-
ity o(expectations. By this property, the expectation of the sum of random variables is 
equal to the sum of their expectations. Formally, we have the following theorem. 
Theorem 2.1 [Linearity of Expectations]: For any finite collection of discrete ran-
dom \'ariables Xl, X 2 , ... , XII with finite expectations, 
Proof: We prove the statement for two random variables X and Y: the general case 
follows by induction. The summations that follow are understood to be over the ranges 
of the corresponding random variables: 
E [X + Y J = L L (i + j) Pr ( (X = i) n (Y = j)) 
j 
= L 
L 
i Pr (( X = i) n (Y = j)) + L 
L 
j Pr (( X = i) n (Y = j)) 
j 
= Li LPr((X = i) n (Y = j)) + Lj LPr((X = i) n (Y = j)) 
j 
.i 
= L i Pr (X = i) + L j Pr (Y = j) 
= E[X] + E[Y]. 
The first equality follows from Definition l.2. In the penultimate equation we have used 
Theorem 1.6, the law of total probability. 
• 
We now use this property to compute the expected sum of two standard dice. Let X = 
Xl + X 2, where Xi represents the outcome of die i for i = 1,2. Then 
1 
6 
7 
E[XiJ = 6 L j = 2' 
j=l 
Applying the linearity of expectations, we have 
22 

2.1 RANDOM VARIABLES AND EXPECTATION 
It is worth emphasizing that linearity of expectations holds for any collection of 
random variables, even if they are not independent! For example, consider again the 
previous example and let the random variable Y = Xl + Xf. We have 
t:\en though Xl and XF are clearly dependent. As an exercise, you may verify this 
identity by considering the six possible outcomes for Xl. 
Linearity of expectations also holds for countably infinite summations in certain 
('ases. Specifically, it can be shown that 
E[ f:Xi] = I:E[Xil 
i=l 
i=l 
\\henever L~l E[IXi I] converges. The issue of dealing with the linearity of expecta-
tions with countably infinite summations is further considered in Exercise 2.29. 
This chapter contains several examples in which the linearity of expectations signif-
i('antly simplifies the computation of expectations. One result related to the linearity 
l)f expectations is the following simple lemma. 
Lemma 2.2: For any constant c and discrete random \'ariable X. 
E[cX] = cE[X]. 
Proof: The lemma is obvious for c = O. For c "# 0, 
E[cX] = L j Pr(cX = j) 
2.1.2. Jensen's Inequality 
= c L (j/c) Pr(X = j/e) 
= eLk Pr( X = k) 
k 
= cE[XJ. 
• 
Suppose that we choose the length X of a side of a square uniformly at random 
from the range [1,99]. What is the expected value of the area'? We can write this 
.1" E[X2]. It is tempting to think of this as being equal to E[X12, but a simple calcula-
tion shows that this is not correct. In fact, E[X]2 = 2500 whereas E[X2] = 9950/3 > 
2500. 
More generally, we can prove that E[X2] ::::: (E[X])2. Consider Y = (X - E[X])2. 
The random variable Y is nonnegative and hence its expectation must also be nonneg-
ative. Therefore, 
23 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
O:s Ery] = E[(X - E[X])2] 
= E[X 2 - 2XE[Xl + (E[X])2] 
= E[X2] - 2E[XE[X JJ + (E[X])2 
= ErX2] - (E[Xl)2. 
To obtain the penultimate line, we used the linearity of expectations. To obtain the last 
line we used Lemma 2.2 to simplify E[XE[X]] = E[X] . E[X]. 
The fact that E[X2] :::: (E[X])2 is an example of a more general theorem known 
as Jensen's inequality. Jensen's inequality shows that. for any convex function f, we 
haveE[f(X)]:::: f(E[X]). 
Definition 2.4: A function f: R ~ R is said to be convex ~f, for any Xl, X2 and 0 :s 
A:Sl. 
Visually. a convex function f has the property that, if you connect two points on the 
graph of the function by a straight line, this line lies on or above the graph of the func-
tion. The following fact. which we state without proof, is often a useful alternative to 
Definition 2.4. 
Lemma 2.3: rtf is ([ nrice d~fferentiable function, then f is convex ~f and only (f 
f"(x) :::: O. 
Theorem 2.4 [Jensen's Inequality]: rtf is a convex function, then 
E [f( X)] 2: f(E[Xl). 
Proof: We prove the theorem assuming that f has a Tay lor expansion. Let fl = E [X]. 
By Taylor's theorem there is a value c such that 
t· 
j' 
t'! 
f"(c)(x -
fl)2 
. (x) = 
(Ii) +. (fl)(X -
fl) + 
2 
:::: .n 11) + f I ( p. ) (x -
fl), 
since f"(c) > 0 by convexity. Taking expectations of both sides and applying linearity 
of expectations and Lemma 2.2 yields the result: 
E[f(X)1 :::: E[f(fl) + f'(fl)(X -
fl)1 
= E[f(fl)l + f'(fl)(E[X] -
p.) 
= f(fl) = f(E[Xl). 
• 
An alternative proof of Jensen's inequality, which holds for any random variable X that 
takes on only finitely many values. is presented in Exercise 2.10. 
24 

2.2 THE BERNOULLI AND BINOMIAL RANDOM VARIABLES 
2.2. The Bernoulli and Binomial Random Variables 
Suppose that we run an experiment that succeeds with probability p and fails with 
probability 1 - p. 
Let Y be a random variable such that 
if the experiment succeeds, 
otherwise. 
The variable Y is called a Bernoulli or an indicator random variable. Note that, for a 
Bernoulli random variable, 
E [Y] = p . 1 + (1 - p) . 0 = p = Pr (Y = 1). 
For example, if we flip a fair coin and consider the outcome "heads" a success, then 
the expected value of the corresponding indicator random variable is 1/2. 
Consider now a sequence of n independent coin flips. \\'hat is the distribution of the 
number of heads in the entire sequence? More generally. consider a sequence of 11 inde-
pendent experiments, each of which succeeds with probability p. If we let X represent 
the number of successes in the n experiments, then X has a hinolllial distrihution. 
Definition 2.S: A binomial random variable X }vith parameters nand p. denoted hy 
B (11, p), is defined by thefollmving probability distribution on j = O. 1. :2 ....• n: 
Pr (X = j) = C) p J (I - p)"-1 
That is, the binomial random variable X equals j when there are exactl) j successes 
and n - j failures in n independent experiments, each of which is successful with prob-
ability p. 
As an exercise, you should show that Definition 2.5 ensures that L~I=() Pr( X = .i) = 
1. This is necessary for the binomial random variable to be a valid probability function. 
according to Definition 1.2. 
The binomial random variable arises in many contexts, especially in sampling. As a 
practical example, suppose that we want to gather data about the packets going through 
a router by postprocessing them. We might want to know the approximate fraction of 
packets from a certain source or of a certain data type. We do not have the memory 
J\ailable to store all of the packets, so we choose to store a random subset - or sam-
[11(' - of the packets for later analysis. If each packet is stored with probability p and 
If n packets go through the router each day, then the number of sampled packets each 
Jay is a binomial random variable X with parameters nand p. If we want to know how 
much memory is necessary for such a sample, a natural starting point is to determine 
the expectation of the random variable X. 
Sampling in this manner arises in other contexts as well. For example, by sampling 
the program counter while a program runs, one can determine what parts of a program 
..ire taking the most time. This knowledge can be used to aid dynamic program opti-
mization techniques such as binary rewriting, where the executable binary form of a 
25 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
program is modified while the program executes. Since rewriting the executable as the 
program runs is expensive, sampling helps the optimizer to determine when it will be 
worthwhile. 
What is the expectation of a binomial random variable X? We can compute it di-
rectly from the definition as 
E[X] = tj(~)pj(l- p)l1- j 
j=O 
} 
11 
(n - 1)' 
. 
. 
= np L 
. 
pJ-I(l - p)(Il-I)-U-I) 
. 
(j-l)!((n-l)-(j-l))! 
J=I 
11-1 
( 
1)' 
= n ~ 
n -. 
k(l _ 
J)(I1-I)-k 
PLk!((n-l)-k)~P 
1 
k=O 
= np I:( n ; l)pkO_ p),,,-li-k 
k=O 
= np, 
where the last equation uses the binomial identity 
(x + r)" = tG )xkynk 
k=O 
The linearity of expectations allows for a significantly simpler argument. If X is a 
binomial random variable with parameters nand p, then X is the number of successes 
in n trials, where each trial is successful with probability p. Define a set of n indicator 
random variables XI, ... , X I1 , where Xi = 1 if the ith trial is successful and 0 otherwise. 
Clearly, E [XtJ = p and X = L :.1= I Xi and so, by the lineari ty of expectations, 
[ 
11 
] 
11 
E[X]=E LXi =LE[XtJ=np. 
i=1 
i=1 
The linearity of expectations makes this approach of representing a random variable by a 
'ium of simpler random variables, such as indicator random variables, extremely useful. 
2.3. Conditional Expectation 
Ju .... r <l'i we have defined conditional probability, it is useful to define the conditional 
npecfufioll of a random variable. The following definition is quite natural. 
26 

2.3 CONDITIONAL EXPECTATION 
Definition 2.6: 
E [Y I Z = z] = L y Pr (Y = y I Z = z), 
It'here the summation is over all y in the range of Y. 
The definition states that the conditional expectation of a random variable is, like the 
expectation, a weighted sum of the values it assumes. The difference is that now each 
value is weighted by the conditional probability that the variable assumes that value. 
For example, suppose that we independently roll two standard six-sided dice. Let 
XI be the number that shows on the first die, X2 the number on the second die, and X 
the sum of the numbers on the two dice. Then 
~ 
1 
11 
E[X I XI = 2] = LX Pr(X = x I XI = 2) = LX' (; = 2' 
x 
r=3 
As another example, consider E[XI I X = 5]: 
4 
E [XI I X = 5] = LX Pr ( X I = x I X = 5) 
x=1 
4 
= LX Pr (X I = X n X = 5) 
Pr(X = 5) 
\-=1 
= i>~j~~ =~. 
x=1 
The following natural identity follows from Definition 2.6. 
Lemma 2.5: For any random variables X and Y, 
E[X] = L Prey = y)E[X I Y = y], 
y 
ll'here the sum is over all values in the range of Y and all of the expectations exist. 
Proof: 
L Prey = y) E [X I Y = y] = L Pre Y = y) LX Pre X = x I Y = y) 
x 
= L LX Pre X = x I Y = y) Pr( Y = y) 
= L LX Pr(X = x n Y = y) 
= LX Pr(X = x) 
x 
= E[X]. 
27 
• 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
The linearity of expectations also extends to conditional expectations. This is clarified 
in Lemma 2.6, whose proof is left as Exercise 2.l1. 
Lemma 2.6: For anyfinite collection of discrete random variables Xl, X 2 , ... , Xn with 
finite expectations andfor any random variable Y, 
[ 
11 
] 
/I 
E LXi I Y = Y = LE[Xi I Y = y]. 
i=l 
i=l 
Perhaps somewhat confusingly, the conditional expectation is also used to refer to the 
following random variable. 
Definition 2.7: The expression E[Y I Zl is a random \'oriahle feZ) that takes on the 
value E[Y I Z = z] when Z = z. 
We emphasize that E [Y 
I Z] is not a real value; it is actually a function of the ran-
dom variable Z. Hence E[Y I Z] is itself a function from the sample space to the real 
numbers and can therefore be thought of as a random variable. 
In the previous example of rolling two dice, 
Xl+ 6 
1 
7 
E[X I Xl] = LX Pr(X = x I Xl) = LX. (; = Xl + 2· 
x 
X=Xl+l 
We see that E [X I Xl] is a random variable whose value depends on Xl. 
If E[Y I Z] is a random variable. then it makes sense to consider its expectation 
E[E[Y I Z]]. In our example. we found that E[X I Xl] = Xl + 7/2. Thus 
E[E[X I Xl]] = E Xl + -
= - + - = 7 = E[X]. 
[ 
7] 7 7 
:2 
2 
2 
More generally. we have the following theorem. 
Theorem 2.7: 
E[Y] =E[E[Y I Z]]. 
Proof' From Definition 2.7 we have E [Y I Z] = feZ), where feZ) takes on the value 
E[Y I Z = z] when Z = .=. Hence 
E[E[Y I Z]] = L E[Y I Z = z] Pr(Z = z). 
The right-hand side equals ElY] by Lemma 2.5. 
• 
We now demonstrate an interesting application of conditional expectations. Consider 
a program that includes one call to a process S. Assume that each call to process S re-
cursively spawns new copies of the process S, where the number of new copies is a 
28 

2.3 CONDITIONAL EXPECTATION 
binomial random variable with parameters nand p. We assume that these random vari-
ables are independent for each call to S. What is the expected number of copies of the 
process S generated by the program? 
To analyze this recursive spawning process, we introduce the idea of generations. 
The initial process S is in generation O. Otherwise, we say that a process S is in gen-
eration i if it was spawned by another process S in generation i-I. Let Yi denote the 
number of S processes in generation i. Since we know that Yo = 1, the number of pro-
cesses in generation 1 has a binomial distribution. Thus, 
Similarly, suppose we knew that the number of processes in generation i-I was 
-"i-I, so Yz'-I = Yi-I· Let Zk be the number of copies spawned by the kth process 
spawned in the (i - l)th generation for 1 :s k :s Yi-I. Each Zk is a binomial random 
variable with parameters nand p. Then 
[
\';-1 
] 
E[li I }i-I = Yi-I] = E L Zk I Yi- I = Yi-I 
k=1 
(
\';-1 
) 
= L j Pr L Z k = j 
I li -I = .vi-I 
j~O 
k=1 
= Lj pr(I: z, = j) 
j~O 
k=1 
\';-1 
= LE[Zd 
k=1 
= Yi-Inp. 
In the third line we have used that the Z k are all independent binomial random vari-
ables; in particular, the value of each Zk is independent of Yi - I , allowing us to remove 
the conditioning. In the fifth line, we have applied the linearity of expectations. 
Applying Theorem 2.7, we can compute the expected size of the ith generation in-
ductively. We have 
E[li] = E[E[Yi lli-I]] = E[li-Inp] = npE[li-ll. 
8 y induction on i, and using the fact that Yo = 1, we then obtain 
E[li] = (np/ 
The expected total number of copies of process S generated by the program is 
given by 
29 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
E[ L 
Y;] = LE[Y;] = L(l1p)£ 
i?:'O 
i?:'O 
i?:'O 
If np :::: 1 then the expectation is unbounded: if np < 1, the expectation is I/O - np). 
Thus, the expected number of processes generated by the program is bounded if and 
only if the expected number of processes spawned by each process is less than 1. 
The process analyzed here is a simple example of a branching process, a probabilis-
tic paradigm extensively studied in probability theory. 
2.4. The Geometric Distribution 
Suppose that we flip a coin until it lands on heads. What is the distribution of the 
number of flips? This is an example of a geometric distribution, which arises in the 
following situation: we perform a sequence of independent trials until the first success, 
where each trial succeeds with probability p. 
Definition 2.8: A geometric random variable X ,"vith parameter p is given by the fol-
lowing probability distribution on n = 1,2, ... : 
Pr (X = n) = (1 _ p) /1- I p . 
That is, for the geometric random variable X to equal n, there must be 11 -
1 failures, 
followed by a success. 
As an exercise, you should show that the geometric random variable satisfies 
L Pr (X = n) = 1. 
II"' I 
Again, this is necessary for the geometric random variable to be a valid probability 
function. according to Definition 1.2. 
In the context of our example from Section 2.2 of sampling packets on a router, if 
packets are sampled with probability p. then the number of packets transmitted after 
the last sampled packet until and including the next sampled packet is given by a geo-
metric random variable with parameter p. 
Geometric random variables are said to be memoryless because the probability that 
you will reach your first success n trials from now is independent of the number of fail-
ures you have experienced. Informally, one can ignore past failures because they do 
not change the distribution of the number of future trials until first success. Formally, 
we have the following statement. 
Lemma 2.8: For a geometric random variable X with parameter p andfor 11 > 0, 
Pr (X = 11 + k I X > k) = Pr (X = n). 
30 

Proof: 
2.4 THE GEOMETRIC DISTRIBUTION 
Pr ( (X = n + k) n (X > k)) 
Pr(X = n +k I X > k) = ----------
Pre X > k) 
Pr(X = 11 + k) 
Pre X > k) 
(1 -
p)lI+k-lp 
~'X 
i 
Li=k (l -
p) p 
(1- p)II+k-lp 
(l -
p)k 
= (1 -
p)II-lp 
= Pr(X = n). 
The fourth equality uses the fact that, for 0 < x < L L~k Xi = xk/(l - x). 
• 
We now turn to computing the expectation of a geometric random variable. When a 
random variable takes values in the set of natural numbers _\- = {O.!' 2. 3 .... }. there 
is an alternative formula for calculating its expectation. 
Lemma 2.9: Let X be a discrete random wzriable that takes un un/y l1onnegati\'e inte-
~er values. Then 
E[X] = L Pr(X ~ i). 
i=l 
Proof: 
L Pre X ~ i) = L L Pr (X = j) 
i=l 
i=l j=i 
:x; 
j 
=LLPr(X=j) 
j=l i=l 
= LjPr(X = j) 
j=l 
= E[X]. 
The interchange of (possibly) infinite summations is justified. since the terms being 
,ummed are all nonnegative. 
• 
For a geometric random variable X with parameter p, 
x 
Pr (X ~ i) = L (1 -
p) /1- I p = (1 _ p) i-I. 
lI=i 
Hence 
31 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
oc 
E[X] = L Pr(X ~ i) 
i=l 
oc 
= L(l - p)i-l 
i=l 
1 -
(1 -
p) 
p 
Thus, for a fair coin where p = 1/2, on average it takes two flips to see the first heads. 
There is another approach to finding the expectation of a geometric random vari-
able X with parameter p - one that uses conditional expectations and the memory-
less property of geometric random variables. Recall that X corresponds to the num-
ber of flips until the first heads given that each flip is heads with probability p. Let 
Y = ° 
if the first flip is tails and Y = 1 if the first flip is heads. By the identity from 
Lemma 2.5, 
E[X] = Prey = O)E[X I Y = 0] + Prey = I)E[X I Y = 11 
= (1- p)E[X I Y =01+ pE[X I Y = 1]. 
If Y = 1 then X = 1, so E[X I Y = 1] = 1. If Y = 0, then X > 1. In this case, let 
the number of remaining flips (after the first flip until the first heads) be Z. Then, by 
the linearity of expectations, 
E[X] = (1 - p)E[Z + 1] + p . 1 = (1 - p)E[Z] + 1. 
By the memoryless property of geometric random variables, Z is also a geometric ran-
dom variable with parameter p. Hence E[Z] = E[X], since they both have the same 
distribution. We therefore have 
E[X] = (1 - p)E[Z] + 1 = (1- p)E[X] + 1, 
which yields E[X] = l/p. 
This method of using conditional expectations to compute an expectation is often 
useful, especially in conjunction with the memoryless property of a geometric random 
variable. 
2.4.1. Example: Coupon Collector's Problem 
The coupon collector's problem arises from the following scenario. Suppose that each 
box of cereal contains one of n different coupons. Once you obtain one of every type 
of coupon, you can send in for a prize. Assuming that the coupon in each box is chosen 
independently and uniformly at random from the n possibilities and that you do not 
32 

2.4 THE GEOMETRIC DISTRIBUTION 
collaborate with others to collect coupons, how many boxes of cereal must you buy 
before you obtain at least one of every type of coupon? This simple problem arises in 
many different scenarios and will reappear in several places in the book. 
Let X be the number of boxes bought until at least one of every type of coupon is 
obtained. We now determine E[X]. If Xi is the number of boxes bought while you had 
exactly i-I different coupons, then clearly X = L~I=1 Xi. 
The advantage of breaking the random variable X into a sum of n random variables 
Xi. i = 1, ... , n, is that each Xi is a geometric random variable. When exactly i-I 
coupons have been found, the probability of obtaining a new coupon is 
i-I 
Pi = 1- --. 
n 
Hence, Xi is a geometric random variable with parameter Pi. and 
1 
11 
E[XiJ = -
= 
. 
. 
Pi 
n -[ + 1 
C sing the linearity of expectations, we have that 
E[Xl = E[ t x;] 
1=1 
11 
n 
1 
=nL--:-' 
[ 
i=1 
The summation L~l=1 Iii is known as the harmonic number H(n). and as we show 
next, H(n) = Inn + 8(1). Thus, for the coupon collector's problem. the expected 
number of random coupons required to obtain all n coupons is n In 11 + H (n ). 
Lemma 2.10: The harmonic number H(n) = L;·I=ll/i satisfies H(n) = Inn + 0)(1). 
Proof: Since l/x is monotonically decreasing, we can write 
1
n 1 
n 
1 
In n = 
- dx .:s L -
x=1 X 
k=1 k 
,md 
11 
1 
111 1 
L k ::s 
- dx = In n. 
k=2 
x=1 X 
33 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
E2
1 
j (X) =-
X 
'I 
1 2 
1r(~~1 
,t 
" j 1-----'>. 
1 (11-1 
1 (11-1 
I (11~ {II :j:,-:::":j:::::::c·::::f:::::::::·::~:::::::::::::::::::':':::j:::::::::~;;;;;=1-
I (11~ {I =1=: 
::j::::::::::::t::::::::::::1:::::::::::::::::::::::::t:-===":J=--,-
• •• 
11-1 
11+1 
. .• 11-1 
(a) Approximating l/x from above. 
(bl Approximating l/x from below. 
Figure 2.1: Approximating the area below f(x) = 1/.-1:. 
This is clarified in Figure 2.1, where the area below the curve fer) = llx corre-
sponds to the integral and the areas of the shaded regions correspond to the summations 
L:~=l 11k and L:Z=2 11k. 
Hence Inn::::: H(n) ::::: Inn + 1, proving the claim. 
• 
As a simple application of the coupon collector's problem, suppose that packets are 
sent in a stream from a source host to a destination host along a fixed path of routers. 
The host at the destination would like to know which routers the stream of packets has 
passed through, in case it finds later that some router damaged packets that it processed. 
If there is enough room in the packet header. each router can append its identification 
number to the header, giving the path. Unfortunately, there may not be that much room 
available in the packet header. 
Suppose instead that each packet header has space for exactly one router identi-
fication number, and this space is used to store the identification of a router chosen 
uniformly at random from all of the routers on the path. This can actually be accom-
plished easily; we consider how in Exercise 2.18. Then, from the point of view of the 
destination host, determining all the routers on the path is like a coupon collector's 
problem. If there are 11 routers along the path, then the expected number of packets in 
the stream that must arrive before the destination host knows all of the routers on the 
path is n H (n) = n In 11 + (0 ( n ) . 
2.5. Application: The Expected Run-Time of Quicksort 
Quicksort is a simple - and, in practice, very efficient - sorting algorithm. The input 
is a list of n numbers X I, X2, ... , XII' For convenience, we will assume that the num-
bers are distinct. A call to the Quicksort function begins by choosing a pivot element 
from the set. Let us assume the pivot is x. The algorithm proceeds by comparing every 
34 

2.5 APPLICATION: THE EXPECTED RUN-TIME OF QUICKSORT 
Quicksort Algorithm: 
Input: A list S = {XI, ... , X/1} of n distinct elements over a totally ordered 
universe. 
Output: The elements of S in sorted order. 
1. If S has one or zero elements, return S. Otherwise continue. 
2. Choose an element of S as a pivot; call it x. 
3. Compare every other element of S to X in order to divide the other elements 
into two sublists: 
(a) SI has all the elements of S that are less than x: 
(b) S2 has all those that are greater than .~r. 
4. Use Quicksort to sort S I and S2. 
5. Return the list SI,X, S2. 
Algorithm 2.1: Quicksort. 
other element to x, dividing the list of elements into two sublists: those that are less 
than x and those that are greater than x. Notice that if the comparisons are performed 
in the natural order, from left to right, then the order of the elements in each sublist is 
the same as in the initial list. Quicksort then recursively sorts these sublists. 
In the worst case, Quicksort requires Q (n 2 ) comparison operations. For example. 
,uppose our input has the form XI = n, X2 = n -
L ... , X/I_I = 2. X/I = 1. Suppose 
also that we adopt the rule that the pivot should be the first element of the list. The 
tirst pivot chosen is then n, so Quicksort performs 11 - 1 comparisons. The division has 
yielded one sublist of size 0 (which requires no additional work) and another of size 
11 -
1, with the order 11 -
1, n - 2, ... ,2, 1. The next pivot chosen is 11 -
1. so Quick-
.... ort performs 11 -
2 comparisons and is left with one group of size 11 -
2 in the order 
11 -
2, n - 3, ... ,2, 1. Continuing in this fashion, Quicksort performs 
11(11-1) 
(11 -
1) + (11 -
2) + ... + 2 + 1 = 
comparisons. 
2 
This is not the only bad case that leads to Q (11 2 ) comparisons: similarly poor perfor-
mance occurs if the pivot element is chosen from among the smallest few or the largest 
few elements each time. 
We clearly made a bad choice of pivots for the given input. A reasonable choice of 
pivots would require many fewer comparisons. For example, if our pivot always split 
the list into two sublists of size at most 111/2l, then the number of comparisons C(I1) 
\\ould obey the following recurrence relation: 
C(I1):'S 2C(II1/2l) + (0(n). 
The solution to this equation yields C(n) = O(1110gn), which is the best possible re-
,LIlt for comparison-based sorting. In fact, any sequence of pivot elements that always 
35 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
split the input list into two sublists each of size at least cn for some constant c would 
yield an O(n log n) running time. 
This discussion provides some intuition for how we would like pivots to be chosen. 
In each iteration of the algorithm there is a good set of pivot elements that split the 
input list into two almost equal sublists: it suffices if the sizes of the two sublists are 
within a constant factor of each other. There is a also a bad set of pivot elements that do 
not split up the list significantly. If good pivots are chosen sufficiently often, Quicksort 
will terminate quickly. How can we guarantee that the algorithm chooses good pivot 
elements sufficiently often? We can resolve this problem in one of two ways. 
First, we can change the algorithm to choose the pivots randomly. This makes Quick-
sort a randomized algorithm: the randomization makes it extremely unlikely that we 
repeatedly choose the wrong pivots. We demonstrate shortly that the expected number 
of comparisons made by a simple randomized Quicksort is 2n In n + O(n), matching 
(up to constant factors) the Q (n log n) bound for comparison-based sorting. Here, the 
expectation is over the random choice of pivots. 
A second possibility is that we can keep our deterministic algorithm, using the first 
list element as a pivot, but consider a probabilistic model of the inputs. A permuta-
tion of a set of n distinct items is just one of the 1l! orderings of these items. Instead of 
looking for the worst possible input. we assume that the input items are given to us in 
a random order. This may be a reasonable assumption for some applications: alterna-
tively, this could be accomplished by ordering the input list according to a randomly 
chosen permutation before running the deterministic Quicksort algorithm. In this case, 
we have a deterministic algorithm but a probahilistic ana(vsis based on a model of the 
inputs. We again show in this setting that the expected number of comparisons made 
is 2n In n + O(n). Here, the expectation is over the random choice of inputs. 
The same techniques are generally used both in analyses of randomized algorithms 
and in probabilistic analyses of deterministic algorithms. Indeed, in this application the 
analysis of the randomized Quicksort and the probabilistic analysis of the deterministic 
Quicksort under random inputs are essentially the same. 
Let us first analyze Random Quicksort. the randomized algorithm version of Quick-
sort. 
Theorem 2.11: Suppose that, Wlzel1e\'er (l pivot is chosen for Random Quicksort, it is 
chosen independently and 111l~lormly at mndomjrom all possibilities. Then, for any in-
put, the expected llumberojco111parisol1s made by Random Quicksort is 2n In n + O(n). 
Proof: Let YI, Y2, ... , YII be the same values as the input values XI, X2, ... , XII but sorted 
in increasing order. For i < j, let Xij be a random variable that takes on the value 1 if 
,Vi and Yj are compared at any time over the course of the algorithm, and 0 otherwise. 
Then the total number of comparisons X satisfies 
II-I 
II 
X= L LXi), 
i=l )=i+1 
and 
36 

2.5 APPLICATION: THE EXPECTED RUN-TIME OF QUICKSORT 
E[Xl = E[ ~ t XU] 
i=1 j=i+1 
/I-I 
11 
= L L E[Xij ] 
i=1 j=i+1 
by the linearity of expectations. 
Since Xi) is an indicator random variable that takes on only the values 0 and I, E [Xi)] 
is equal to the probability that Xu is 1. Hence all we need to do is compute the prob-
ability that two elements Yi and Yj are compared. Now, Yi and Yj are compared if and 
only if either Yi or Yj is the first pivot selected by Random Quicksort from the set yU = 
{Yi,Yi+l, ···,Yj-I.Yj}· This is because if Yi (or Yj) is the first pivot selected from this 
set, then Yi and Yj must still be in the same sublist, and hence they will be compared. 
Similarly, if neither is the first pivot from this set, then .Vi and )j will be separated into 
distinct sublists and so will not be compared. 
Since our pivots are chosen independently and unifonnly at random from each sub-
list, it follows that, the first time a pivot is chosen from yij, it is equally likely to be any 
element from this set. Thus the probability that Yi or Yj is the first pivot selected from 
yiJ, which is the probability that Xij = 1, is 2j(j - i + 1). Using the substitution k = 
j - i + 1 then yields 
n-I 
n 
2 
E[Xl = L L· . 
}-l+l 
i=1 j=i+1 
n-I n-i+1 
=L L 
~ 
i=1 
k=2 
/I 
11+I-k 2 
=L L 
k 
k=2 
i=1 
11 
2 
=L(n+l-k)k 
k=2 
( 
11 2) 
= (n + 1) L k - 2(n -
1) 
k=2 
11 
1 
= (2n + 2) L k - 4n. 
k=1 
)Jotice that we used a rearrangement of the double summation to obtain a clean form 
for the expectation. 
Recalling that the summation H(n) = L:Z=lljk satisfies H(n) = Inn + 80), we 
have E[Xl = 2n In n + ('"')(n). 
• 
37 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
Next we consider the deterministic version of Quicksort, on random input. We assume 
that the order of the elements in each recursively constructed sublist is the same as in 
the initial list. 
Theorem 2.12: Suppose that, whenever a pivot is chosen for Quicksort, the first ele-
ment (~fthe sublist is chosen. If the input is chosen ul1{formly at randomfrom all possible 
permutations of the values, then the expected number of comparisons made by Deter-
ministic Quicksort is 2n In n + O(n). 
Proof: The proof is essentially the same as for Random Quicksort. Again, Yi and Yj 
are compared if and only if either Yi or Yj is the first pivot selected by Quicksort from 
the set yiJ. Since the order of elements in each sublist is the same as in the original 
list. the first pivot selected from the set yij is just the first element from yij in the in-
put list, and since all possible permutations of the input values are equally likely, every 
element in yij is equally likely to be first. From this. we can again use linearity of ex-
pectations in the same way as in the analysis of Random Quicksort to obtain the same 
expression for E[X]. 
• 
2.6. Exercises 
Exercise 2.1: Suppose we roll a fair k-sided die with the numbers I through k on the 
die's faces. If X is the number that appears. what is E[X]? 
Exercise 2.2: A monkey types on a 26-letter keyboard that has lowercase letters only. 
Each letter is chosen independently and uniformly at random from the alphabet. If the 
monkey types 1,000,000 letters. what is the expected number of times the sequence 
"proof" appears? 
Exercise 2.3: Give examples offunctions f and random variables X where E [f( X)] ::::: 
f(E[X]), E[f(X)] = f(E[X]). and E[f(X)] ~ f(E[X]). 
Exercise 2.4: Prove that E[X"] ~ E[X J" for any even integer k ~ 1. 
Exercise 2.5: If X is a B(n.I/2) random variable with n ~ 1, show that the probabil-
ity that X is even is 1 12. 
Exercise 2.6: Suppose that we independently roll two standard six-sided dice. Let Xl 
be the number that shows on the first die, X 2 the number on the second die, and X the 
sum of the numbers on the two dice. 
(a) What is E[X I Xl is even]? 
(b) What is E[X I Xl = X2 1? 
(c) What is E[XI I X = 9]? 
(d) What is E[XI - X2 I X = k] for k in the range [2, 12]? 
38 

2.6 EXERCISES 
Exercise 2.7: Let X and Y be independent geometric random variables, where X has 
parameter p and Y has parameter q. 
(a) What is the probability that X = Y? 
(b) What is E[max(X, Y)]? 
(c) What is Pr(min(X, Y) = k)? 
(d) What is E[X I X :s Y]? 
You may find it helpful to keep in mind the memoryless property of geometric random 
variables. 
Exercise 2.8: (a) Alice and Bob decide to have children until either they have their 
first girl or they have k ~ 1 children. Assume that each child is a boy or girl indepen-
dently with probability 1/2 and that there are no multiple births. What is the expected 
number of female children that they have? What is the expected number of male chil-
dren that they have? 
(b) Suppose Alice and Bob simply decide to keep having children until they have 
their first girl. Assuming that this is possible, what is the expected number of boys that 
they have? 
Exercise 2.9: (a) Suppose that we roll twice a fair k-sided die with the numbers 1 
through k on the die's faces, obtaining values XI and X 2 . What is E[max(X 1.X2 )]'? 
What is E[min(XI, X 2 )]? 
(b) Show from your calculations in part (a) that 
(c) Explain why Egn. (2.1) must be true by using the linearity of expectations instead 
of a direct computation. 
Exercise 2.10: (a) Show by induction that if f: R ---+ R is convex then, for any 
x I , X:2, ... , X n and A I , A 2, ... , A 11 with L ;1= I Ai = 1, 
(2.2) 
(b) Use Egn. (2.2) to prove that if f: R ---+ R is convex then 
E[f(X)] ~ f(E[X]) 
for any random variable X that takes on only finitely many values. 
Exercise 2.11: Prove Lemma 2.6. 
Exercise 2.12: We draw cards uniformly at random with replacement from a deck of 
II cards. What is the expected number of cards we must draw until we have seen all n 
cards in the deck? If we draw 2n cards, what is the expected number of cards in the 
deck that are not chosen at all? Chosen exactly once? 
39 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
Exercise 2.13: (a) Consider the following variation of the coupon collector's problem. 
Each box of cereal contains one of 2n different coupons. The coupons are organized 
into n pairs, so that coupons 1 and 2 are a pair, coupons 3 and 4 are a pair, and so on. 
Once you obtain one coupon from every pair, you can obtain a prize. Assuming that 
the coupon in each box is chosen independently and uniformly at random from the 2n 
possibilities, what is the expected number of boxes you must buy before you can claim 
the prize? 
(b) Generalize the result of the problem in part (a) for the case where there are kn 
different coupons, organized into n disjoint sets of k coupons, so that you need one 
coupon from every set. 
Exercise 2.14: The geometric distribution arises as the distribution of the number of 
times we flip a coin until it comes up heads. Consider now the distribution of the 
number of flips X until the kth head appears, where each coin flip comes up heads in-
dependently with probability p. Prove that this distribution is given by 
(
n - I) 
Pr(X = n) = 
k _ I 
pl;(1 -
1')11-1; 
for n 2: k. (This is known as the negative billomial distribution.) 
Exercise 2.15: For a coin that comes up heads independently with probability p on 
each flip, what is the expected number of flips until the kth heads? 
Exercise 2.16: Suppose we flip a coin 11 times to obtain a sequence of flips Xl, X 2, ... , 
XII. A streak of flips is a consecutive subsequence of flips that are all the same. For ex-
ample. if X.i. X4 . and X:" are all heads. there is a streak of length 3 starting at the third 
flip. (If X 6 is also heads. then there is also a streak of length 4 starting at the third flip.) 
(a) Let 11 be a power of 2. Show that the expected number of streaks of length log2 n + 1 
is I -
o( I). 
(b) Show that. for sufficiently large 11. the probability that there is no streak of length 
at least 1l0g211 - 210g 21og 2 11j is less than I/n. (Hint: Break the sequence of flips 
up into disjoint blocks of LIOg211 - 210g 2 10g2 nJ consecutive flips, and use that 
the event that one block is a streak is independent of the event that any other block 
is a streak.) 
Exercise 2.17: Recall the recursive spawning process described in Section 2.3. Sup-
pose that each call to process S recursively spawns new copies of the process S, where 
the number of new copies is 2 with probability p and 0 with probability I -
p. If li 
denotes the number of copies of S in the ith generation, determine E[YiJ. For what 
values of p is the expected total number of copies bounded? 
Exercise 2.18: The following approach is often called reservoir sampling. Suppose 
we have a sequence of items passing by one at a time. We want to maintain a sample 
of one item with the property that it is uniformly distributed over all the items that we 
40 

2.6 EXERCISES 
have seen at each step. Moreover, we want to accomplish this without knowing the 
total number of items in advance or storing all of the items that we see. 
Consider the following algorithm, which stores just one item in memory at all times. 
When the first item appears, it is stored in the memory. When the kth item appears, it 
replaces the item in memory with probability 1/ k. Explain why this algorithm solves 
the problem. 
Exercise 2.19: Suppose that we modify the reservoir sampling algorithm of Exer-
cise 2.18 so that. when the kth item appears, it replaces the item in memory with 
probability 1/2. Describe the distribution of the item in memory. 
Exercise 2.20: A permutation on the numbers [I,ll J can be represented as a function 
:T: [I, n] ~ [I, n J, where :rr (i) is the position of i in the ordering given by the permuta-
tion. A fixed point of a permutation :rr: [1, n] ~ [1,11] is a value for which :rr (x) = x. 
Find the expected number of fixed points of a permutation chosen uniformly at random 
from all permutations. 
Exercise 2.21: Let al,a2, ... ,a ll be a random permutation of {I.2 . .... Il}, equally 
likely to be any of the n! possible permutations. When sorting the list (11.(12 •...• all , 
the element ai must move a distance of I Cli - i I places from its current position to reach 
its position in the sorted order. Find 
the expected total distance that elements will have to be moved. 
Exercise 2.22: Let a I, ([2, ... , all be a list of n distinct numbers. We say that ai and 
Cl) are inverted if i < j but ai > {lj. The Buhblesort sorting algorithm swaps pairwise 
adjacent inverted numbers in the list until there are no more inversions, so the list is 
in sorted order. Suppose that the input to Bubblesort is a random permutation. equally 
likely to be any of the 11! permutations of 11 distinct numbers. Determine the expected 
number of inversions that need to be corrected by Bubblesort. 
Exercise 2.23: Linear insertion sort can sort an array of numbers in place. The first 
and second numbers are compared; if they are out of order, they are swapped so that 
they are in sorted order. The third number is then placed in the appropriate place in the 
'iorted order. It is first compared with the second: if it is not in the proper order. it is 
'iwapped and compared with the first. Iteratively, the kth number is handled by swap-
ping it downward until the first k numbers are in sorted order. Determine the expected 
number of swaps that need to be made with a linear insertion sort when the input is a 
random permutation of n distinct numbers. 
Exercise 2.24: We roll a standard fair die over and over. What is the expected number 
of rolls until the first pair of consecutive sixes appears? (Hint: The answer is not 36.) 
41 

DISCRETE RANDOM VARIABLES AND EXPECTATION 
Exercise 2.25: A blood test is being performed on n individuals. Each person can be 
tested separately. but this is expensive. Pooling can decrease the cost. The blood sam-
ples of k people can be pooled and analyzed together. If the test is negative, this one 
test suffices for the group of k individuals. If the test is positive, then each of the k per-
sons must be tested separately and thus k + 1 total tests are required for the k people. 
Suppose that we create n/k disjoint groups of k people (where k divides n) and use 
the pooling method. Assume that each person has a positive result on the test indepen-
dently with probability p. 
(a) What is the probability that the test for a pooled sample of k people will be posi-
tive? 
(b) What is the expected number of tests necessary? 
(c) Describe how to find the best value of k. 
(d) Give an inequality that shows for what values of p pooling is better than just test-
ing every individual. 
Exercise 2.26: A permutation J[: [1, n J ~ [1, n] can be represented as a set of cycles 
as follows. Let there be one vertex for each number i. i = I, ... , n. If the permuta-
tion maps the number i to the number J[ (i). then a directed arc is drawn from vertex i 
to vertex J[ (i). This leads to a graph that is a set of disjoint cycles. Notice that some 
of the cycles could be self-loops. What is the expected number of cycles in a random 
permutation of n numbers? 
Exercise 2.27: Consider the following distribution on the integers x 2: 1: Pre X = x) = 
(6/J[2)x- 2. This is a valid distribution, since L~l k-2 = J[2/6. What is its ex-
pectation? 
Exercise 2.28: Consider a simplified version of roulette in which you wager x dollars 
on either red or black. The wheel is spun, and you receive your original wager plus 
another x dollars if the ball lands on your color; if the ball doesn't land on your color, 
you lose your wager. Each color occurs independently with probability 1/2. (This is a 
simplification because real roulette wheels have one or two spaces that are neither red 
nor black, so the probability of guessing the correct color is actually less than 1/2.) 
The following gambling strategy is a popular one. On the first spin, bet 1 dollar. If 
you lose, bet 2 dollars on the next spin. In general, if you have lost on the first k - 1 
spins, bet 2k- 1 dollars on the kth spin. Argue that by following this strategy you will 
eventually win a dollar. Now let X be the random variable that measures your maxi-
mum loss before winning (i.e., the amount of money you have lost before the play on 
which you win). Show that E[X] is unbounded. What does it imply about the practi-
cality of this strategy? 
Exercise 2.29: Prove that, if Xo, Xl, ... is a sequence of random variables such that 
oc 
LE[IXjl] 
j=O 
42 

2.6 EXERCISES 
converges, then the linearity of expectations holds: 
Exercise 2.30: In the roulette problem of Exercise 2.28, we found that with probabil-
ity I you eventually win a dollar. Let Xi be the amount you win on the jth bet. (This 
might be 0 if you have already won a previous bet.) Determine E[Xj ] and show that, 
by applying the linearity of expectations, you find your expected winnings are O. Does 
the linearity of expectations hold in this case? (Compare with Exercise 2.29.) 
Exercise 2.31: A variation on the roulette problem of Exercise 2.28 is the following. 
We repeatedly flip a fair coin. You pay j dollars to play the game. If the first head 
comes up on the kth flip, you win 2kjk dollars. What are your expected winnings? 
How much would you be willing to pay to play the game? 
Exercise 2.32: You need a new staff assistant, and you have 11 people to interview. You 
want to hire the best candidate for the position. When you interview a candidate, you 
can give them a score, with the highest score being the best and no ties being possible. 
You interview the candidates one by one. Because of your company's hiring practices, 
after you interview the kth candidate, you either offer the candidate the job before the 
next interview or you forever lose the chance to hire that candidate. We suppose the 
candidates are interviewed in a random order, chosen uniformly at random from all 11 ~ 
possible orderings. 
We consider the following strategy. First, interview m candidates but reject them alL 
these candidates give you an idea of how strong the field is. After the mth candidate. 
hire the first candidate you interview who is better than all of the previous candidates 
you have interviewed. 
(a) Let E be the event that we hire the best assistant, and let Ei be the event that ith 
candidate is the best and we hire him. Determine Pr(Ei ), and show that 
m 
II 
I 
Pr(E) = - L -. -. 
n 
J-I 
j=m+l 
(b) Bound L:J=m+l j~l to obtain 
m 
m 
- On n - In m) .::::: Pr ( E) .::::: - On (n -
I) - In (m -
I)). 
n 
n 
(c) Show that mOnn - Inm)jn is maximized when m = nje, and explain why this 
means Pr(E) 2: Ije for this choice of m. 
43 

CHAPTER THREE 
Moments and Deviations 
In this and the next chapter we examine techniques for bounding the tail distribution, 
the probability that a random variable assumes values that are far from its expectation. 
In the context of analysis of algorithms, these bounds are the major tool for estimat-
ing the failure probability of algorithms and for establishing high probability bounds 
on their run-time. In this chapter we study Markov's and Chebyshev's inequalities and 
demonstrate their application in an analysis of a randomized median algorithm. The 
next chapter is devoted to the Chernoff bound and its applications. 
3.1. Markov's Inequality 
Markov's inequality, formulated in the next theorem, is often too weak to yield useful 
results, but it is a fundamental tool in developing more sophisticated bounds. 
Theorem 3.1 [Markov's Inequality]: Let X be a random variable that assumes only 
nonnegative values. Then, for all a > 0, 
Proof: For a > 0, let 
and note that, since X 2: 0, 
E[X] 
Pr(X 2: a) .::::: --. 
a 
1
1 if X 2: a, 
1= ° otherwise, 
X 
I.::::: -. 
a 
Because I is a 0-1 random variable, E[I] = Pr(l = 1) = Pr(X 2: a). 
Taking expectations in (3.1) thus yields 
[X] 
E[X] 
Pr(X 2: a) = E[/] .::::: E -;; = -a-· 
44 
(3.1) 
• 

3.2 VARIANCE AND MOMENTS OF A RANDOM VARIABLE 
For example, suppose we use Markov's inequality to bound the probability of obtain-
ing more than 3n 14 heads in a sequence of n fair coin flips. Let 
{
I 
if the ith coin flip is heads, 
Xi= o otherwise, 
and let X = L:7=1 Xi denote the number of heads in the n coin flips. Since E[XiJ = 
Pr(Xi = 1) = 1/2, it follows that E[X] = L:7=1 E[XiJ = n12. Applying Markov's 
inequality, we obtain 
E[X] 
nl2 
2 
Pr(X > 3n14) < -- = -- = -. 
-
-
3nl4 
3nl4 
3 
3.2. Variance and Moments of a Random Variable 
Markov's inequality gives the best tail bound possible when all we know is the expec-
tation of the random variable and that the variable is nonnegative (see Exercise 3.16). 
It can be improved upon if more information about the distribution of the random vari-
able is available. 
Additional information about a random variable is often expressed in terms of its 
moments. The expectation is also called the first moment of a random variable. More 
generally, we define the moments of a random variable as follows. 
Definition 3.1: The kth moment of a random variable X is E [Xk]. 
A significantly stronger tail bound is obtained when the second moment (E [X2]) is also 
available. Given the first and second moments, one can compute the variance and stan-
dard deviation of the random variable. Intuitively, the variance and standard deviation 
otfer a measure of how far the random variable is likely to be from its expectation. 
Definition 3.2: The variance ofa random variable X is defined as 
The standard deviation of a random variable X is 
cr[X] = JVar[X]. 
The two forms of the variance in the definition are equivalent, as is easily seen by using 
the linearity of expectations. Keeping in mind that E [X] is a constant, we have 
E[(X - E[X])2] = E[X2 - 2XE[X] + E[Xf] 
= E[X2] - 2E[XE[XJl + E[Xf 
= E[X 21 - 2E[X]E[X] + E[X]2 
= E[X2] - (E[X])2. 
45 

MOMENTS AND DEVIATIONS 
If a random variable X is constant - so that it always assumes the same value -
then its variance and standard deviation are zero. More generally, if a random vari-
able X takes on the value kE[X] with probability 11k and the value 0 with probability 
1 - 11k, then its variance is (k - I)(E[X])2 and its standard deviation is -Jk=}E[X]. 
These cases help demonstrate the intuition that the variance (and standard deviation) 
of a random variable are small when the random variable assumes values close to its 
expectation and are large when it assumes values far from its expectation. 
We have previously seen that the expectation of the sum of two random variables is 
equal to the sum of their individual expectations. It is natural to ask whether the same 
is true for the variance. We find that the variance of the sum of two random variable 
has an extra term, called the covariance. 
Definition 3.3: The covariance of two random l'([riahles X and Y is 
Cov(X, Y) = E[(X - E[X])( Y - E[ Y])]. 
Theorem 3.2: For any two random variables X ([nd Y. 
Var[X + Y] = Var[X] + Var[Y] + 2 Cov(X, Y). 
Proof: 
VarIX + Y] = E[(X + Y - E[X + y])21 
= E[(X + Y - E[X] - E[Y] )21 
= E[(X - E[X])2 + (Y - E[y])2 + 2(X - E[X])(Y - E[Y])] 
= E[(X - E[X])2] + E[( Y - E[y])2] + 2E[(X - E[X])(Y - E[Y])] 
= Var[X] + Var[Y 1 +.2 Cov( X, Y). 
• 
The extension of this theorem to a sum of any finite number of random variables is 
proven in Exercise 3.14. 
The variance of the sum of two (or any finite number of) random variables does 
equal the sum of the variances when the random variables are independent. Equiva-
lently, if X and Yare independent random variables, then their covariance is equal to 
zero. To prove this result, we first need a result about the expectation of the product of 
independent random variables. 
Theorem 3.3: If X and Yare two independent random variables, then 
E[X . Y] = E[X] . E[Y1. 
Proof: In the summations that follow, let i take on all values in the range of X, and let 
j take on all values in the range of Y: 
46 

3.2 VARIANCE AND MOMENTS OF A RANDOM VARIABLE 
E [X . Y] = L L (i . j) . Pr ( (X = i) n (Y = j)) 
= L L (i . j) . Pr (X = i) . Pr (Y = j) 
= (L i . 
Pre X = i)) ( L j . Pre Y = j)) 
I 
J 
= E[Xl . E[Y], 
where the independence of X and Y is used in the second line. 
• 
Unlike the linearity of expectations, which holds for the sum of random variables 
whether they are independent or not, the result that the expectation of the product of 
two (or more) random variables is equal to the product of their ex pectations does not 
necessarily hold if the random variables are dependent. To see this. let Y and Z each 
correspond to fair coin flips, with Y and Z taking on the value () if the flip is heads and 
1 if the flip is tails. Then E[Y] = EfZj = 1/2. If the two flips are independent. then 
Y . Z is 1 with probability 1/4 and 0 otherwise. so indeed E [Y . Z J = E [Y] . E[Z]. 
Suppose instead that the coin flips are dependent in the following way: the coins are 
tied together, so Y and Z either both come up heads or both come up tails together. 
Each coin considered individually is still a fair coin flip. but now Y . Z is 1 with prob-
ability 1/2 and so ElY· Zj #- E[Yl· E[Z]. 
Corollary 3.4: fr X and Yare independent random variables, then 
and 
Proof: 
Cov(X, Y) = 0 
VarIX + Yl = Var[X J + Var[Yl. 
Cov(X, Y) = E[(X - E[X])(Y - E[Y])] 
= E[X - E[X]] . E[Y - E[Yll 
= o. 
In the second equation we have used the fact that, since X and Yare independent, so 
are X - E[Xl and Y - E[Yl and hence Theorem 3.3 applies. For the last equation we 
use the fact that, for any random variable Z, 
E[(Z - E[ZJ)] = E[Z] - E[E[Z]J = o. 
Since Cov(X, Y) = 0, we have VarIX + Yl = VarIX] + Var[Yl. 
• 
By induction we can extend the result of Corollary 3.4 to show that the variance of 
the sum of any finite number of independent random variables equals the sum of their 
variances. 
47 

MOMENTS AND DEVIATIONS 
Theorem 3.5: Let XI, X2, ... , X/1 be mutually independent random variables. Then 
[ 
11 
] 
11 
Var LXi = LVar[Xil. 
i=1 
i=1 
3.2.1. Example: Variance of a Binomial Random Variable 
The variance of a binomial random variable X with parameters nand p can be deter-
mined directly by computing E[X2]: 
11 
, 
= L 
n... pi(l - p)"-j(C(- - j) + j) 
. 
(n-))!)! 
1=0 
11 
, ( .2 
.) 
11, • 
= L n. ) .- ). pi (l -
p) /1- j + L 
n.!. p.i (1 _ p) /1-i 
. 
(n-))!/! 
. 
(n-))!)! 
I=()' 
I=() 
') 
11 
(n -
2)! 
. ') 
. 
= n(n -
I)p~ L 
pl--(l- p)I1-1 
. 
(n-j)!(j-2)! 
1=2 
/1 
(n-I)'. 
. 
" 
. 
I-I (1 
)11- 1 
+np L 
(n -j)!(j -I)!P' 
- P 
1=1 
= n(n -
I)p2 + np. 
Here we have simplified the summations by using the binomial theorem. We con-
clude that 
Var[X] = E[X2] - (E[X])2 
= n(n -
I)p2 + np - n2p2 
') 
= I1p -
np~ 
= np(l -
p). 
An alternative derivation makes use of independence. Recall from Section 2.2 that a 
binomial random variable X can be represented as the sum of n independent Bernoulli 
trials, each with success probability p. Such a Bernoulli trial Y has variance 
2 
') 
') 
') 
E[(Y - E[Y]) 1 = p(l - pt + (1 - p)(-p)- = p - p- = p(l- p). 
By Corollary 3.4, the variance of X is then np(l - p). 
3.3. Chebyshev's Inequality 
U sing the expectation and the variance of the random variable, one can derive a signif-
icantly stronger tail bound known as Chebyshev's inequality. 
48 

3.3 CHEBYSHEV'S INEQUALITY 
Theorem 3.6 [Chebyshev's Inequality]: For any a > 0, 
Var[X] 
Pr(IX - E[X]I 2: a) :s 
2' 
a 
Proof· We first observe that 
Pr ( I X - E [X] I 2: a) = Pr ( (X - E [X]) 2 2: a 2 ) . 
Since (X - E[X])2 is a nonnegative random variable, we can apply Markov's inequal-
ity to prove: 
Pr((X - E[X])2 2: a 2) :s E[(X -
~[X])2] 
Var[X] 
a-
a 2 
• 
The following useful variants of Chebyshev's inequality bound the deviation of the ran-
dom variable from its expectation in terms of a constant factor of its standard deviation 
or expectation. 
Corollary 3.7: For any t > 1, 
1 
Pr(IX - E[X]I 2: t . a[X]) :s 7 
and 
t-
Var[X] 
Pr (I X - E [X] I 2: t . E [ X]):s 
') 
') . 
t-(E[X])-
Let us again consider our coin-flipping example, and this time use Chebyshev's in-
equality to bound the probability of obtaining more than 3n /4 heads in a sequence of 
n fair coin flips. Recall that Xi = 1 if the ith coin flip is heads and ° 
otherwise, and 
that X = L:;1=1 Xi denotes the number of heads in the n coin flips. To use Chebyshev's 
inequality we need to compute the variance of X. Observe first that, since Xi is a 0-1 
random variable, 
') 
1 
E[(Xit] = E[XiJ = -. 
2 
Thus, 
2 
2 
1 
I 
1 
Var [X iJ = E [ ( Xi) ] - (E [X iJ ) = 2: - 4 = 4' 
Now, since X = L:7=1 Xi and the Xi are independent, we can use Theorem 3.5 to 
compute 
Var[Xl = var[ t x;] = tVar[X;l = ~. 
i=l 
i=l 
Applying Chebyshev's inequality then yields 
Pr (X 2: 3 n /4) :s Pr ( I X - E [X] I 2: n /4) 
Var[X] 
<---
-
(n/4)2 
n/4 
(n/4)2 
4 
n 
49 

MOMENTS AND DEVIATIONS 
In fact, we can do slightly better. Chebyshev's inequality yields that 4 In is actu-
ally a bound on the probability that X is either smaller than nl4 or larger than 3n14, so 
by symmetry the probability that X is greater than 3nl4 is actually 21n. Chebyshev's 
inequality gives a significantly better bound than Markov's inequality for large n. 
3.3.1. Example: Coupon Collector's Problem 
We apply Markov's and Chebyshev's inequalities to the coupon collector's prob-
lem. Recall that the time X to collect n coupons has expectation nHn, where Hn 
L:;1=1 lin = In n + 0(1). Hence Markov's inequality yields 
To use Chebyshev's inequality, we need to find the variance of X. Recall again from 
Section 2.4.1 that X = L:;1=1 Xi, where the Xi are geometric random variables with 
parameter (n - i + 1)ln. In this case, the Xi are independent because the time to col-
lect the ith coupon does not depend on how long it took to collect the previous i-I 
coupons. Hence 
[ 
11 
] 
11 
Var[X] = Var LXi = LVar[Xd, 
i=1 
i=1 
so we need to find the variance of a geometric random variable. 
Let Y be a geometric random variable with parameter p. As we saw in Section 2.4, 
E[X] = lip. We calculate E[y2]. The following trick proves useful. We know that, 
for 0 < x < 1, 
1 
~. 
l_x=L x1 . 
i=O 
Taking derivatives, we find: 
\\-e can conclude that 
1 
x 
-O---x-)-:: = L i x i-I 
i=O 
x 
= L(i + I)x
i
; 
i=O 
2 
--- = "i(i - 1)x i - 2 
(l-x)3 
L 
i=O 
x 
= L (i + l)(i + 2)x i . 
i=O 
50 

3.3 CHEBYSHEV'S INEQUALITY 
2 
I 
1 
----3 
+---
(l-x)3 
(I-x):? 
(I-x) 
x 2 +x 
(I-x)3· 
We now use this to find 
ex; 
E[y2] = L p(l -
p)i~1(2 
i=l 
P 
1 - P 
2-p 
') 
p" 
Finall y, we reach 
Var[Y] = E[y2] - E[yf 
2 - p 
I 
We have just proven the following useful lemma. 
Lemma 3.8: The variance of a geometric random variable with parameter p IS 
(1 -
p)jp2. 
For a geometric random variable Y, E[y2] can also be derived using conditional expec-
tations. We use that Y corresponds to the number of flips until the first heads, where 
each flip is heads with probability p. Let X = 0 if the first flip is tails and X = I if the 
first flip is heads. By Lemma 2.5, 
E[y2] = Pr(X = 0)E[y2 I X = 0] + Pr(X = I)E[y2 I X = 1] 
= (I - p)E[y2 I X = 0] + pE[y2 I X = 1]. 
If X = I, then Y = I and so E [Y 2 
I X = 1] = 1. If X = 0, then Y > 1. In this case, 
let the number of remaining flips after the first flip until the first head be Z. Then 
51 

MOMENTS AND DEVIATIONS 
E[y2] = (1 - p)E[(Z + 1)2] + p . 1 
= (1 - p)E[Z2] + 2(1 - p)E[Z] + 1 
(3.2) 
by the linearity of expectations. By the memoryless property of geometric random vari-
ables, Z is also a geometric random variable with parameter p. Hence E[Z] = lip 
and E[Z2] = E[y2]. Plugging these values into Eqn. (3.2), we have 
2(1-p) 
2-p 
E[y2] = (1 -
p)E[y2] + 
+ 1 = (1 - p)E[y2] + --, 
p 
p 
which yields E[y2] = (2 -
p)lp2, matching our other derivation. 
We return now to the question of the variance in the coupon collector's problem. 
We simplify the argument by using the upper bound Var[Y] ~ Ilp2 for a geometric 
random variable, instead of the exact result of Lemma 3.8. Then 
11 
11 ( 
)2 
11 (1)2 
rr2 n2 
Var[X] = ~Var[Xil ~ ~ 
'~ 
= 112 ~ -:-
<--
~ 
~ 11-/+1 
~ 1 
6 
i=] 
i=] 
i=] 
Here we have used the identity 
6 
Now, by Chebyshev's inequality, 
In this case, Chebyshev's inequality again gives a much better bound than Markov's 
inequality. But it is still a fairly weak bound, as we can see by considering instead a 
fairly simple union bound argument. 
Consider the probability of not obtaining the ith coupon after n In n + cn steps. This 
probability is 
(
1 _ ~)11( ]nl1~c) < e~(]nll+C) = _I_ 
n 
eCn 
By a union bound, the probability that some coupon has not been collected after 
n In n + cn steps is only e~(. In particular, the probability that all coupons are not 
collected after 2n In n steps is at most 1 In, a bound that is significantl y better than what 
can be achieved even with Chebyshev's inequality. 
3.4. Application: A Randomized Algorithm for 
Computing the Median 
Given a set S of n elements drawn from a totally ordered universe, the median of S is 
an element m of S such that at least Ln 12J elements in S are less than or equal to m 
52 

3.4 APPLICATION: A RANDOMIZED ALGORITHM FOR COMPUTING THE MEDIAN 
and at least Ln j2J + I elements in S are greater than or equal to m. If the elements in 
S are distinct, then m is the (111 j2l )th element in the sorted order of S. 
The median can be easily found deterministically in O( 11 log 11) steps by sorting, and 
there is a relatively complex deterministic algorithm that computes the median in O(n) 
time. Here we analyze a randomized linear time algorithm that is significantly sim-
pler than the deterministic one and yields a smaller constant factor in the linear running 
time. To simplify the presentation, we assume that n is odd and that the elements in the 
input set S are distinct. The algorithm and analysis can be easily modified to include 
the case of a multi-set S (see Exercise 3.23) and a set with an even number of elements. 
3.4.1. The Algorithm 
The main idea of the algorithm involves sampling. which we first discussed in Sec-
tion 1.2. The goal is to find two elements that are close together in the sorted order of S 
and that have the median lie between them. Specifically. we seek tv. 0 elements d, U E S 
such that: 
1. d :s 111 :s u (the median m is between d and u): and 
2. for C = {s E S : d :s s :s u}, ICI = o(njlogn) (the total number of elements 
between d and u is small). 
Sampling gives us a simple and efficient method for finding two such elements. 
We claim that, once these two elements are identified, the median can easily be 
found in linear time with the following steps. The algorithm counts (in linear time) 
the number td of elements of S that are smaller than d and then sorts ( in sublinear. or 
o(n), time) the set C. Notice that, since ICI = o(njlogn), the set C can be sorted in 
time o( n) using any standard sorting algorithm that requires O( III log Ill) time to sort 
171 elements. The (Ln j2J - ed + l)th element in the sorted order of C is Ill. since there 
are exactly Lnj2J elements in S that are smaller than that value (Lnj2 -
LdJ in the set 
C and td in S - C). 
To find the elements d and u, we sample with replacement a multi-set R of In 3/-I-l 
elements from S. Recall that sampling with replacement means each element in R is 
chosen uniformly at random from the set S, independent of previous choices. Thus. the 
same element of S might appear more than once in the multi-set R. Sampling without 
replacement might give marginally better bounds, but both implementing and analyz-
ing it are significantly harder. It is worth noting that we assume that an element can be 
sampled from S in constant time. 
Since R is a random sample of S we expect m, the median element of S. to be close 
to the median element of R. We therefore choose d and u to be elements of R sur-
rounding the median of R. 
We require all the steps of our algorithm to work 'with high probability. by which 
we mean with probability at least I -
0(1 jn
C
) for some constant c > O. To guarantee 
that with high probability the set C includes the median m. we fix d and 11 to be re-
spectively the Ln 3/-I-j2 - y01J th and the Ln 3/-I-j 2 + y01J th elements in the sorted order of 
R. With this choice, the set C includes all the elements of S that are between the 2y01 
53 

MOMENTS AND DEVIATIONS 
Randomized Median Algorithm: 
Input: A set S of n elements over a totally ordered universe. 
Output: The median element of S, denoted by m. 
1. Pick a (multi-)set R of in 3/ 4l elements in S. chosen independently and 
uniformly at random with replacement. 
2. Sort the set R. 
3. Let d be the (l ~n3/4 - ~ 
J )th smallest element in the sorted set R. 
4. Let u be the (I ~n3/4 + ~l)th smallest element in the sorted set R. 
5. By comparing every element in S to d and u. compute the set 
C = {x E S : d ::::; x ::::; u} and the numbers tel = I{.r E S : x < d}1 and 
~11 = I {x E S : x > u} I. 
6. If ~d > n/2 or ~1I > n/2 then FAIL. 
7. If ICI ::::; 411 3/ 4 then sort the set C, otherwise FAIL. 
8. Output the (Ln/2J -
~d + l)th element in the sorted order of C. 
Algorithm 3.1: Randomized median algorithm. 
sample points surrounding the median of R. The analysis will clarify that the choice of 
the size of R and the choices for d and u are tailored to guarantee both that (a) the set 
C is large enough to include m with high probability and (b) the set C is sufficiently 
small so that it can be sorted in sublinear time with high probability. 
A formal description of the procedure is presented as Algorithm 3.1. In what fol-
lows. for convenience we treat ~ 
and 1/34 as integers. 
3.4.2. Analysis of the Algorithm 
Based on our previous discussion. we first prove that - regardless of the random choices 
made throughout the procedure - the algorithm (a) always terminates in linear time and 
(b) outputs either the correct result or FAIL. 
Theorem 3.9: The randomi::.ed median algorithm terminates in linear time, and if it 
does not output FAIL then it outputs the correct median element of the input set S. 
Proof: Correctness follows because the algorithm could only give an incorrect answer 
if the median were not in the set C. But then either ~d > n/2 or ~u > n/2 and thus 
step 6 of the algorithm guarantees that, in these cases, the algorithm outputs FAIL. 
Similarly, as long as C is sufficiently small, the total work is only linear in the size of 
S. Step 7 of the algorithm therefore guarantees that the algorithm does not take more 
than linear time; if the sorting might take too long, the algorithm outputs FAIL with-
out sorting. 
• 
The interesting part of the anal ysis that remains after Theorem 3.9 is bounding the prob-
ability that the algorithm outputs FAIL. We bound this probability by identifying three 
54 

3.4 APPLICATION: A RANDOMIZED ALGORITHM FOR COMPUTING THE MEDIAN 
"bad" events such that, if none of these bad events occurs, the algorithm does not fail. 
In a series of lemmas, we then bound the probability of each of these events and show 
that the sum of these probabilities is only O(n~I/4). 
Consider the following three events: 
£1: YI = l{rER I r .:=:;m}1 < ~n3/4_~; 
£2: Y2 = I{r ER I r 2: m}1 < ~n3/4 -~; 
£3: lei> 4n 3/4. 
Lemma 3.10: The randomized median algorithm fails (f and only if at least one of £1, 
£2, or £3 occurs. 
Proof: Failure in step 7 of the algorithm is equivalent to the event £ 3. Failure in step 6 
of the algorithm occurs if and only if f!d > n /2 or til> II / 2. But for f!d > n /2, the 
(~n3/4 - ~ 
)th smallest element of R must be larger than Ill: this is equivalent to the 
event £1. Similarly, f!u > 11/2 is equivalent to the event £2. 
• 
Lemma 3.11: 
Proof: Define a random variable Xi by 
x, = {~ 
if the ith sample is less than or equal to the median. 
otherwise. 
The Xi are independent, since the sampling is done with replacement. Because there 
are (n -
1)/2 + I elements in S that are less than or equal to the median, the proba-
bility that a randomly chosen element of S is less than or equal to the median can be 
written as 
(n - 1)/2 + I 
1 
1 
Pr(Xi = 1) = 
= - + -. 
n 
2 
2n 
The event £1 is equivalent to 
/13/4 
YI = LXi < ~n3/4 -~. 
i=1 
Since YI is the sum of Bernoulli trials, it is a binomial random variable with param-
eters n3/4 and 1/2 + 1/2n. Hence, using the result of Section 3.2.1 yields 
Var[YI ] = n3/ 4 (~ + ~) (~ - ~) 
2 
2n 
2 
2n 
= ~n3/4 __ 
1_ 
4 
4n 5/4 
55 

MOMENTS AND DEVIATIONS 
Applying Chebyshev's inequality then yields 
Pr(."]) = pr( YI < ~n)/4 - Jfl) 
:s Pr(IY1 - E[YIlI > ~) 
Var[YIl 
<---
n 
In3/4 
1 
< _4_ = _n-I/4 
n 
4 
• 
We similarly obtain the same bound for the probability of the event £2. We now bound 
the probability of the third bad event, £3. 
Lemma 3.12: 
Pr(£ 1,) < ~n -1/4. 
-
-
2 
Proof: If £3 occurs, so I C I > 4n 3/4, then at least one of the following two events 
occurs: 
£ 3. ( at least 2n 3/4 elements of C are greater than the median; 
£3.2: at least 2n 3/ 4 elements of C are smaller than the median. 
Let us bound the probability that the first event occurs; the second will have the same 
bound by symmetry. If there are at least 2n 3/ 4 elements of C above the median, then 
the order of u in the sorted order of S was at least ~n + 2n3/4 and thus the set R has at 
least 1n3/4 - ~ 
samples among the ~n - 2n 3/4 largest elements in S. 
Let X be the number of samples among the ~n - 2n 3/4 largest elements in S. Let 
""'l1v+ 
-
X = L-i=1 Xi, where 
-II if the ith sample is among the ~n - 2n 3/4 largest elements in S, 
Xi -
o otherwise. 
Again, X is a binomial random variable, and we find 
and 
I 
E[X] = _n 3/ 4 -
2~ 
2 
Applying Chebyshev's inequality yields 
Pr(£3,1) = Pr(X 2: 1n3/4 - ~) 
(3.3) 
Var[X] 
InV~ 
I 
:s Pr(IX - E[X]I 2:~) :s 
< _4_ = _n- I/ 4. 
(3.4) 
n 
n 
4 
Similarly, 
56 

3.5 EXERCISES 
and 
• 
Combining the bounds just derived, we conclude that the probability that the algorithm 
outputs FAIL is bounded by 
Pr(£d + Pr(£2) + Pr(£ 3) :s II-L~. 
This yields the following theorem. 
Theorem 3.13: The probability that the randomi-;.ed mediall algorithmfails is bounded 
by n-1/4. 
By repeating Algorithm 3.1 until it succeeds in finding the median. we can obtain an 
iterative algorithm that never fails but has a random running time. The samples taken 
in successive runs of the algorithm are independent, so the success of each run is in-
dependent of other runs, and hence the number of runs until success is achieved is a 
geometric random variable. As an exercise, you may wish to show that this \ariation of 
the algorithm (that runs until it finds a solution) still has linear expected running time. 
Randomized algorithms that may fail or return an incorrect answer are called :'v/ollte 
Carlo algorithms. The running time of a Monte Carlo algorithm often doe,,> not depend 
on the random choices made. For example, we showed in Theorem 3.9 that the ran-
domized median algorithm always terminates in linear time, regardless of its random 
choices. 
A randomized algorithm that always returns the right answer is called a Las Vegas 
algorithm. We have seen that the Monte Carlo randomized algorithm for the median 
can be turned into a Las Vegas algorithm by running it repeatedly until it succeeds. 
Again, turning it into a Las Vegas algorithm means the running time is variable, al-
though the expected running time is still linear. 
3.5. Exercises 
Exercise 3.1: Let X be a number chosen uniformly at random from [1.111. Find Var[X J. 
Exercise 3.2: Let X be a number chosen uniformly at random from [-k. k 1. Find 
Var[X]. 
Exercise 3.3: Suppose that we roll a standard fair die 100 times. Let X be the sum 
of the numbers that appear over the 100 rolls. Use Chebyshev's inequality to bound 
Pr(IX -
3501 ~ 50). 
Exercise 3.4: Prove that, for any real number c and any discrete random variable X, 
Var[cX] = c 2 Var[Xl. 
57 

MOMENTS AND DEVIATIONS 
Exercise 3.5: Given any two random variables X and Y, by the linearity of expecta-
tions we have E[X - Y] = E[X] - E[Y]. Prove that, when X and Yare independent, 
Var[X -
Y] = Var[X] + Var[Y]. 
Exercise 3.6: For a coin that comes up heads independently with probability p on each 
flip, what is the variance in the number of flips until the kth head appears? 
Exercise 3.7: A simple model of the stock market suggests that, each day, a stock with 
price q will increase by a factor r > I to q r with probability p and will fall to q jr with 
probability I -
p. Assuming we start with a stock with price I, find a formula for the 
expected value and the variance of the price of the stock after d days. 
Exercise 3.8: Suppose that we have an algorithm that takes as input a string of n bits. 
We are told that the expected running time is 0(11 2) if the input bits are chosen inde-
pendently and uniformly at random. What can Markm''s inequality tell us about the 
worst-case running time of this algorithm on inputs of size II'? 
Exercise 3.9: (a) Let X be the sum of Bernoulli random variables, X = L::?=l Xi. The 
Xi do not need to be independent. Show that 
11 
E [X 2] = L Pr ( Xi = I) E [ X I Xi = I]. 
(3.5) 
i=l 
Hint: Start by showing that 
II 
l=! 
and then apply conditional expectations. 
(b) Use Eqn. (3.5) to provide another derivation for the variance of a binomial ran-
dom variable with parameters II and p. 
Exercise 3.10: For a geometric random variable X, find E[X 3 ] and E[X4]. (Hint: Use 
Lemma 2.5.) 
Exercise 3.11: Recall the B ubblesort algorithm of Exercise 2.22. Determine the vari-
ance of the number of inversions that need to be corrected by Bubblesort. 
Exercise 3.12: Find an example of a random variable with finite expectation and un-
bounded variance. Give a clear argument showing that your choice has these properties. 
Exercise 3.13: Find an example of a random variable with finite jth moments for 1 ~ 
j ~ k but an unbounded (k + l)th moment. Give a clear argument showing that your 
choice has these properties. 
58 

3.5 EXERCISES 
Exercise 3.14: Prove that, for any finite collection of random variables Xl, X 2 ,··., Xn , 
Exercise 3.15: Let the random variable X be representable as a sum of random vari-
ables X = L::?=l Xi· Show that, if E[XiXj 1 = E[XiJE[Xj] for every pair of i and j 
with 1 ~ i < j :s n, then Var[X] = L:;1=1 Var[Xi ]. 
Exercise 3.16: This problem shows that Markov's inequality is as tight as it could pos-
sibly be. Given a positive integer k, describe a random variable X that assumes only 
nonnegative values such that 
I 
Pr(X 2: kE[X]) = k' 
Exercise 3.17: Can you give an example (similar to that for Markov's inequality in 
Exercise 3.16) that shows that Chebyshev's inequality is tight? If not. explain why not. 
Exercise 3.18: Show that, for a random variable X with standard deviation a [X] and 
any positive real number t: 
(a) 
(b) 
I 
Pr(X - E[X] 2: ta[X]) ~ --'); 
1 + t~ 
2 
Pr(IX - E[X]I 2: ta[X]) :s -1 -2' 
+t 
Exercise 3.19: Let Y be a nonnegative integer-valued random variable with positive 
expectation. Prove 
E[y]2 
-- < Pr[Y --,L 0] < E[Y]. 
E[y2] -
1-
Exercise 3.20: (a) Chebyshev's inequality uses the variance of a random \ariable to 
bound its deviation from its expectation. We can also use higher moments. Suppose 
that we have a random variable X and an even integer k for which E[( X - E[X] )"1 is 
finite. Show that 
1 
Pr(IX - E[X] I > t {/E[(X - E[X])"J) ~ k' 
t 
(b) Why is it difficult to derive a similar inequality when k is odd? 
Exercise 3.21: A fixed point of a permutation rr: [1, nJ --+ D, 1/] is a value for which 
rr(x) = x. Find the variance in the number of fixed points of a permutation chosen 
uniformly at random from all permutations. (Hint: Let Xi be 1 if rr(i) = i, so that 
L:7=1 Xi is the number of fixed points. You cannot use linearity to find Var[L:7=, Xi], 
but you can calculate it directly.) 
59 

MOMENTS AND DEVIATIONS 
Exercise 3.22: Suppose that we flip a fair coin n times to obtain n random bits. Con-
sider all m = G) pairs of these bits in some order. Let ~. be the exclusive-or of the ith 
pair of bits, and let Y = L:l~1 ~. be the number of ~. that equal 1. 
(a) Show that each ~. is 0 with probability 1/2 and 1 with probability 1/2. 
(b) Show that the }j are not mutually independent. 
(c) Show that the ~. satisfy the property that E [ ~ 0] = E [}j J E [Jj]. 
(d) Using Exercise 3.15, find Var[Y]. 
(e) Using Chebyshev's inequality, prove a bound on Pr(IY - E[YJI 2: n). 
Exercise 3.23: Generalize the median-finding algorithm for the case where the input 
5 is a multi-set. Prove that your resulting algorithm is correct. and bound its running 
time. 
Exercise 3.24: Generalize the median-finding algorithm to find the kth largest item in 
a set of n items for any given value of k. Prove that your resulting algorithm is correct, 
and bound its running time. 
Exercise 3.25: The weak law of large numbers states that. if Xl, X 2 , X3, ... are inde-
pendent and identically distributed random variables with mean f1 and standard devia-
tion a, then for any constant s > 0 we have 
. 
(I Xl + X:: + ... + XII 
1 ) 
hm Pr 
-
11 > s 
= o. 
11--* ex; 
II 
Use Chebyshev's inequality to prove the weak law of large numbers. 
60 

CHAPTER FOUR 
Chernoff Bounds 
This chapter introduces what are commonly called Chernoff bounds. Chernoff bounds 
are extremely powerful, giving exponentially decreasing bounds on the tail distribu-
tion. These bounds are derived by using Markov's inequality on the moment gener-
ating function of a random variable. We start this chapter by defining and discussing 
the properties of the moment generating function. We then derive Chernoff bounds for 
the binomial distribution and other related distributions. using a set-balancing prob-
lem as an example. To demonstrate the power of Chernotl bounds. we apply them 
to the analysis of randomized packet routing schemes on the hypercube and butterfly 
networks. 
4.1. Moment Generating Functions 
Before developing Chernoff bounds, we discuss the special role of the moment gener-
ating function E[etx ]. 
Definition 4.1: The moment generating function of a random variable X is 
We are mainly interested in the existence and properties of this function in the neigh-
borhood of zero. 
The function M x (t) captures all of the moments of X. 
Theorem 4.1: Let X be a random variable with moment generating fZlIlction Mx (t). 
Under the assumption that exchanging the expectation and d(f{erentiation operands is 
legitimate, for all n > I we then have 
E[XII] = Mlll)(o)' 
where M~Il)(O) is the nth derivative ofMx(t) evaluated at t = O. 
61 

CHERNOFF BOUNDS 
Proof: Assuming that we can exchange the expectation and differentiation operands, 
then 
Computed at t = 0, this expression yields 
M~I)(O) = E[X'T 
• 
The assumption that expectation and ditlerentiation operands can be exchanged holds 
whenever the moment generating function exists in a neighborhood of zero, which will 
be the case for all distributions considered in this book. 
As a specific example, consider a geometric random variable X with parameter p, 
as in Definition 2.8. Then, for t < -In(1 -
p). 
It follows that 
Mx(t) = E[etx ] 
x 
= L(1 -
p)k-Ipe tk 
k=1 
x 
= -'-) -
L 
(1 -
p)ketk 
1- p A:=I 
= -p_((1 - (1- p)et)-I -1). 
l-p 
M~I)(t) = p(l -
(1 -
p)et )-2et 
and 
M~2)(t) = 2p(l - p)(l -
(1 -
p)et )-3e 2t + p(l -
(1 -
p)et )-2et. 
Evaluating these derivatives at t = 0 and using Theorem 4.1 gives E[X] = ljp and 
E[X2] = (2 -
p)jp2, matching our previous calculations from Section 2.4 and Sec-
tion 3.3.1. 
Another useful property is that the moment generating function of a random variable 
(or, equivalently, all of the moments of the variable) uniquely defines its distribution. 
However, the proof of the following theorem is beyond the scope of this book. 
Theorem 4.2: Let X and Y be two random variables. If 
Mx(t) = My(t) 
for all t E (-0,0) for some 0 > 0, then X and Y have the same distribution. 
One application of Theorem 4.2 is in determining the distribution of a sum of indepen-
dent random variables. 
Theorem 4.3: If X and Yare independent random variables, then 
Mx+y(t) = Mx(t)My(t). 
62 

4.2 DERIVING AND APPLYING CHERNOFF BOUNDS 
Proof: 
Here we have used that X and Yare independent - and hence etX and etY are indepen-
dent - to conclude that E[etXe tY ] = E[etx]E[e tY ]. 
• 
Thus, if we know M x (t) and My (t) and if we recognize the function M x (t) My (t) as 
the moment generating function of a known distribution, then that must be the distribu-
tion of X + Y when Theorem 4.2 applies. We will see examples of this in subsequent 
sections and in the exercises. 
4.2. Deriving and Applying Chernoff Bounds 
The Chernoff bound for a random variable X is obtained by applying Markov's in-
equality to etX for some well-chosen value t. From MarkO\'\ inequality. we can derive 
the following useful inequality: for any t > 0, 
In particular, 
E[e'Yl 
Pr(X > a) = Pr(e tX > etU) < --. 
-
-
-
elU 
E[e tx ] 
Pr(X :::: a) ~ min ---. 
1>0 
etu 
Similarly, for any t < 0, 
Hence 
E[e tx ] 
Pr(X < a) < min ---. 
-
-
td) 
etu 
Bounds for specific distributions are obtained by choosing appropriate values for t. 
While the value of t that minimizes E[etXl/e{([ gives the best possible bounds, often 
one chooses a value of t that gives a convenient form. Bounds derived from this ap-
proach are generally referred to collectively as Chernoff bounds. When we speak of a 
Chernoff bound for a random variable, it could actually be one of many bounds derived 
in this fashion. 
4.2.1. Chernoff Bounds for the Sum of Poisson Trials 
We now develop the most commonly used version of the Chernoff bound: for the tail 
distribution of a sum of independent 0-1 random variables, which are also known as 
Poisson trials. (Poisson trials differ from Poisson random variables, which will be dis-
cussed in Section 5.3.) The distributions of the random variables in Poisson trials are 
not necessarily identical. Bernoulli trials are a special case of Poisson trials where the 
independent 0-1 random variables have the same distribution; in other words, all trials 
are Poisson random variables that take on the value 1 with the same probability. Also 
63 

CHERNOFF BOUNDS 
recall that the binomial distribution gives the number of successes in n independent 
Bernoulli trials. Our Chernoff bound will hold for the binomial distribution and also 
for the more general setting of the sum of Poisson trials. 
Let XI, ... , Xn be a sequence of independent Poisson trials with Pr(Xi = I) = Pi. 
Let X = L7=1 Xi, and let 
For a given (5 > 0, we are interested in bounds on Pr(X 2: (1 + (5)/1) and Pr(X ~ 
(1- (5)/1) - that is, the probability that X deviates from its expectation /1 by (5/1 or more. 
To develop a Chernoff bound we need to compute the moment generating function of 
X. We start with the moment generating function of each Xi: 
Mx; (t) = E[e!x/l 
= Pie! + (1 - Pi) 
=1+Pi(el-l) 
where in the last inequality we have used the fact that. for any}', I + y ~ eY. Applying 
Theorem 4.3, we take the product of the n generating functions to obtain 
Il 
Mx(t) = n M.\",(t) 
i=1 
Il 
~ n 
e/)/leI-11 
i=1 
= CXp{ t l'i(C' - I) } 
1=1 
Now that we have determined a bound on the moment generating function, we are 
ready to develop concrete versions of the Chernoff bound for a sum of Poisson trials. 
We start with bounds on the deviation above the mean. 
Theorem 4.4: Let XI ..... X Il be independent Poisson trials such that Pr(Xi ) = Pi. 
Let X = L;I=I Xi and /1 = E[X]. Then the following Chernoffbounds hold: 
1. for any (5 > 0, 
(4.1) 
2. for ° 
< (5 ~ I, 
(4.2) 
3. for R 2: 6/1. 
(4.3) 
64 

4.2 DERIVING AND APPLYING CHERNOFF BOUNDS 
The first bound of the theorem is the strongest, and it is from this bound that we derive 
the other two bounds, which have the advantage of being easier to state and compute 
with in many situations. 
Proof: Applying Markov's inequality, for any t > 0 we have 
Pr(X 2: (1 + (5)/1) = Pr(e tX 2: et (l+O)I1) 
E[e!x] 
<---
-
et(l+O)fi 
e(el-I)/i 
<---
-
et(l+c\)/i· 
For any /5 > 0, we can set t = In(l + (5) > 0 to get (4.1): 
( 
e'l 
)/i 
Pr (X 2: (1 + (5) /1).:s 
(1 + /5 ) II + 6 ) 
To obtain (4.2) we need to show that, for 0 < /5 .:s 1, 
Taking the logarithm of both sides, we obtain the equivalent condition 
/5 2 
f(/5) = /5 -
(1 + (5) ln(l + (5) + 3 .:s O. 
Computing the derivatives of f(/5), we have: 
, 
1+/5 
2 
f (/5) = I -
I + /5 - In(1 + (5) + :3/5 
2 
= -In(1 + (5) + :3/5; 
H 
1 
2 
f (/5) = -1 + /5 +:3. 
We see that fH(/5) < 0 for 0 .:s /5 < 1/2 and that fH(/5) > 0 for /5 > 1/2. Hence 
f'(/5) first decreases and then increases over the interval [0,1]. Since f'(O) = 0 and 
f'(1) < 0, we can conclude that f'(/5) .:s 0 in the interval [0, I]. Since f{O) = O. it 
follows that f(/5) .:s 0 in that interval, proving (4.2). 
To prove (4.3), let R = (l + (5)/1. Then, for R 2: 6/1, /5 = R//1 -
I 2: 5. Hence, 
using (4.1), 
• 
65 

CHERNOFF BOUNDS 
We obtain similar results bounding the deviation below the mean. 
Theorem 4.5: Let Xl, ... , XII be independent Poisson trials such that Pre Xi) = Pi. 
Let X = L;l=1 Xi and /1 = E[XJ. Then, for 0 < 8 < 1: 
1. 
Pr X < 1 - 8 
< 
. 
( 
e-8 
)Il 
(-( 
)/1)-
(1-8)(1-8) 
, 
(4.4) 
2. 
(4.5) 
Again, the bound of Eqn. (4.4) is stronger than Eqn. (4.5). but the latter is generally 
easier to use and sufficient in most applications. 
Proof: Using Markov's inequality, for any t < 0 we have 
Pr(X :s (1 -
8)/1) = Pr(e'x 2: elll-())Il) 
E[e'x ] 
<---
-
elll-61/l 
elel-ll/l 
<---
-
e1il-(sl/l' 
For 0 < 8 < 1, we set t = In (l - 8) < 0 to get (4.4): 
Pre X :" (! - 8) 11) :" ((1 _ 
C~;I1_ 81 )" 
To prove (4.5) we must show that. for 0 < 8 < 1, 
,2/, 
----:s e-
o 
-
(1 -
8)11--(1) 
Taking the logarithm of both sides, we obtain the equivalent condition 
82 
f' (8) = - 8 -
(1 -
8) In (1 -
8) + -
< 0 
. 
2 -
for 0 < 8 < 1. 
Differentiating f (8) yields 
f'(8) = In(l- 8) + 8, 
1 
('(8) = --- + 1. 
. 
1 - 8 
Since fl/(8) < 0 in the range (0,1) and since f'(O) = 0, we have f'(8) :s 0 in the 
range [0,1). Therefore, f(8) is nonincreasing in that interval. Since f(O) = 0, it fol-
lows that f(8) :s 0 when 0 < 8 < L as required. 
• 
Often the following form of the Chernoff bound, which is derived immediately from 
Eqn. (4.2) and Eqn. (4.4), is used. 
66 

4.2 DERIVING AND APPLYING CHERNOFF BOUNDS 
Corollary 4.6: Let XI, ... , XII be independent Poisson trials such that Pr(Xi) = Pi. 
Let X = L;·l=1 Xi and /1 = E[X]. For 0 < 6 < 1, 
Pre 1 X -
/11 2: 6/1) ~ 2e -1)
02°. 
(4.6) 
In practice we often do not have the exact value of E [X]. Instead we can use /1 2: E [X] 
in Theorem 4.4 and /1 :s E[X] in Theorem 4.5 (see Exercise 4.7). 
4.2.2. Example: Coin Flips 
Let X be the number of heads in a sequence of n independent fair coin flips. Applying 
the Chernoff bound of Eqn. (4.6), we have 
(I 
nl 
l~) 
Iln61nn) 
Pr 
X -
-
2: - v 6n In n :s 2 exp - - -=) --
2 
2 
3 _ 
n 
2 
n 
This demonstrates that the concentration of the number of heads around the mean 
n /2 is very tight; most of the time, the deviations from the mean are on the order of 
O(~). 
To compare the power of this bound to Chebyshev's bound. consider the probabil-
ity of having no more than n /4 heads or no fewer than 3n /4 heads in a sequence of n 
independent fair coin flips. In the previous chapter, we used Chebyshe\''s inequality to 
show that 
Already, this bound is worse than the Chernoff bound just calculated for a significantly 
larger event! Using the Chernoff bound in this case, we find that 
Thus, Chernoff's technique gives a bound that is exponentially smaller than the bound 
obtained using Chebyshev's inequality. 
4.2.3. Application: Estimating a Parameter 
Suppose that we are interested in evaluating the probability that a particular gene muta-
tion occurs in the population. Given a DNA sample, a lab test can determine if it carries 
the mutation. However, the test is expensive and we would like to obtain a relatively 
reliable estimate from a small number of samples. 
67 

CHERNOFF BOUNDS 
Let p be the unknown value that we are trying to estimate. Assume that we have 
n samples and that X = 1m of these samples have the mutation. Given a sufficiently 
large number of samples, we expect the value p to be close to the sampled value p. We 
express this intuition using the concept of a confidence interval. 
Definition 4.2: A I -
y confidence interval for a parameter p is an interval [p - 0, 
p + 0] such that 
Pr (p E [p - 0, p + 0]) 2: I -
y. 
Notice that, instead of predicting a single value for the parameter, we give an interval 
that is likely to contain the parameter. If p can take on any real value, it may not make 
sense to try to pin down its exact value from a finite sample, but it does make sense to 
estimate it within some small range. 
Naturally we want both the interval size 20 and the error probability y to be as small 
as possible. We derive a trade-off between these two parameters and the number of 
samples 11. In particular, given that among 11 samples (chosen uniformly at random 
from the entire population) we find the mutation in exactly X = PI1 samples, we need 
to find values of 0 and y for which 
Pr (p E [p - 0, p + 0]) = Pr ( n p E [n ( jj - 0). n (ij + 0)]) 2: I - y. 
Now X = np has a binomial distribution with parameters 11 and p, so E[X] = np. 
If p t/:- [p - 0, p + 0] then we have one of the following two events: 
1. if p < P - 0, then X = njj > n(p + 0) = E[Xl(l + 0lp); 
2. if p > p + 0, then I1j'j < n( p - 0) = E[Xl(l - olp). 
We can apply the Chernotl bounds (4.2) and (4.5) to compute 
Pr (p \1' [p - 8. /' +8]) = Pr (X < lip (1 - % ) ) + Pr (X > n p (1 + D ) (4.7) 
< e-llfil,) fi)2 :: + e-llfi(0/fi)2j3 
(4.8) 
= e _1I()2 ~fi + e -1I0 2/3fi. 
(4.9) 
The bound given in Eqn. (4.9) is not useful because the value of p is unknown. A 
simple solution is to use the fact that p ~ 1, yielding 
Setting y = e-1I02/ 2 + e-n02j3, we obtain a trade-off between 0, n, and the error proba-
bility y. 
We can apply other Chernoff bounds, such as those in Exercises 4.13 and 4.l6, to ob-
tain better bounds. We return to the subject of parameter estimation when we discuss 
the Monte Carlo method in Chapter 10. 
68 

4.3 BETTER BOUNDS FOR SOME SPECIAL CASES 
4.3. Better Bounds for Some Special Cases 
We can obtain stronger bounds using a simpler proof technique for some special cases 
of symmetric random variables. 
We consider first the sum of independent random variables when each variable as-
sumes the value 1 or -1 with equal probability. 
Theorem 4.7: Let Xl, ... , XII be independent random ,'ariables with 
Proof' For any t > 0, 
1 
Pr(Xi = 1) = Pr(Xi = -1) = -, 
') 
To estimate E[e tx,], we observe that 
t 2 
t i 
et = 1 + t + - + ' .. + - + ' , , 
2' 
" 
, 
1. 
and 
t 2 
. t i 
e -t = 1 - t + - + ' , , + (-1) I - + .. , 
2' 
,,' 
, 
I, 
using the Taylor series expansion for et , Thus, 
Using this estimate yields 
1/ 
E[e IX ] = n 
E[e tx,] :s et"2I1/2 
i=l 
and 
69 

CHERNOFF BOUNDS 
Setting t = a/n, we obtain 
By symmetry we also have 
Combining the two results yields our next corollary. 
Corollary 4.8: Let Xl, ... , XII be independent random variables with 
I 
Pr(Xi = 1) = Pr(Xi = -1) = -. 
2 
Let X = L;l=1 Xi. Then, for any a > 0, 
Pre I X I 2: a) ~ 2e-
ucj2ll
. 
Applying the transformation Yi = (Xi + 1)/2 allows us to prove the following. 
Corollary 4.9: Let Y l , ••• , Yll be independent random variables with 
1 
PrOi = 1) = Pr(Yi = 0) = -. 
2 
Let Y = L;1=1 }j and fL = E[Y] = n/2. 
1. For an.v a > 0, 
2. For any 0 > 0, 
Proof: Using the notation of Theorem 4.7, we have 
Applying Theorem 4.7 yields 
Prey 2: fL + a) = Pr(X ~ 2a) ~ e-4u2/211, 
• 
(4.10) 
proving the first part of the corollary. The second part follows from setting a = OfL = 
on /2. Again applying Theorem 4.7, we have 
• 
Note that the constant in the exponent of the bound (4.10) is 1 instead of the 1/3 in the 
bound of (4.2). 
Similarly, we have the following result. 
70 

4.4 APPLICATION: SET BALANCING 
Corollary 4.10: Let Y I , ••• , Yn be independent random variables with 
1 
Pr(Yi = 1) = Pr(Yi = 0) = 2' 
Let Y = L;l=1 Yi and /1 = E[Y] = n/2. 
1. Foran}'O < a < /1, 
2. For any 0 < 8 < 1, 
4.4. Application: Set Balancing 
Given an n x m matrix A with entries in {O, I}, let 
Cl 
al2 
a21 
an 
a:Jl 
an2 
alm)(b
l
) 
(CI) 
a2m 
b 2 
C2 
a"" 
h", 
c,,· 
(4.11) 
Suppose that we are looking for a vector b with entries in {- L I} that minimizes 
IIAbllXl = max ICi I· 
i=I ..... 11 
This problem arises in designing statistical experiments. Each column of the matrix A 
represents a subject in the experiment and each row represents a feature. The vector h 
partitions the subjects into two disjoint groups, so that each feature is roughly as bal-
anced as possible between the two groups. One of the groups serves as a control group 
for an experiment that is run on the other group. 
Our randomized algorithm for computing a vector b is extremely simple. We ran-
domly choose the entries of b, with Pr(bi = 1) = Pr(b i = -1) = 1/2. The choices 
for different entries are independent. Surprisingly, although this algorithm ignores the 
entries of the matrix A, the following theorem shows that IIAbllXl is likely to be only 
o (v' mIn n ). This bound is fairly tight. In Exercise 4.15 you are asked to show that. 
when m = n, there exists a matrix A for which IIAhll x is n(y'n) for any choice of h. 
Theorem 4.11: For a random vector h with entries chosen independent(\' and }vith 
equal probability from the set {-I, I}, 
Pr(IIAhIIXl ~ v'4m Inn) :s~. 
n 
Proof: Consider the ith row ai = ai,I, ... ,ai.m, and let k be the number of Is in that 
row. If k :s v'4m Inn, then clearly lai . hi = lei I :s v'4m Inn. On the other hand, if 
k > v' 4m In n then we note that the k nonzero terms in the sum 
71 

CHERNOFF BOUNDS 
m 
Zi = Lai,jbj 
j=l 
are independent random variables. each with probability 1/2 of being either + I or -l. 
Now using the Chernoff bound of Corollary 4.8 and the fact that m 2: k, 
By the union bound, the probability that the bound fails for any row is at most 2/n. 
• 
4.5. * Application: Packet Routing in Sparse Networks 
A fundamental problem in parallel computing is how to communicate efficiently over 
sparse communication networks. We model a communication network by a directed 
graph on N nodes. Each node is a routing s\vitch. A directed edge models a com-
munication channel, which connects two adjacent routing switches. We consider a 
synchronous computing model in which (a) an edge can carry one packet in each time 
step and (b) a packet can traverse no more than one edge per step. We assume that 
switches have butfers or queues to store packets waiting for transmission through each 
of the switch's outgoing edges. 
Given a network topology, a rouring algorithm specifies. for each pair of nodes, a 
route - or a sequence of edges - connecting the pair in the network. The algorithm 
may also specify a queuing policy for ordering packets in the switches' queues. For 
example. the First In First Out (FIFO) policy orders packets by their order of arrival. 
The Furthest To Go (FTG) policy orders packets in decreasing order of the number of 
edges they must sti II cross in the network. 
Our measure of the performance of a routing algorithm on a given network topology 
is the maximum time - measured as the number of parallel steps - required to route an 
arbitrary permutation routing problem. where each node sends exactly one packet and 
each node is the address of exactly one packet. 
Of course, routing a permutation can be done in just one parallel step if the network 
is a complete graph connecting all of the nodes to each other. Practical considerations. 
however, dictate that a network for a large-scale parallel machine must be sparse. Each 
node can be connected directly to only a few neighbors, and most packets must traverse 
intermediate nodes en route to their final destination. Since an edge may be on the path 
of more than one packet and since each edge can process only one packet per step, par-
allel packet routing on sparse networks may lead to congestion and bottlenecks. The 
practical problem of designing an efficient communication scheme for parallel comput-
ers leads to an interesting combinatorial and algorithmic problem: designing a family 
of sparse networks connecting any number of processors, together with a routing algo-
rithm that routes an arbitrary permutation request in a small number of parallel steps. 
We discuss here a simple and elegant randomized routing technique and then use 
Chernoff bounds to analyze its performance on the hypercube network and the butterfly 
network. We first analyze the case of routing a permutation on a hypercube. a network 
72 

4.5* APPLICATION: PACKET ROUTING IN SPARSE NETWORKS 
o 
00 
10 
01 
11 
(a) 11 = I. 
(b) 11 = 2. 
(e) 11 = 3. 
0100 
0101 
(d) 11 = 4. 
Figure 4.1: Hypercubes of dimensions 1, 2. 3. and -f. 
with N processors and O(N log N) edges. We then present a tighter argument for the 
butterfly network, which has N nodes and only O(N) edges. 
4.5.1. Permutation Routing on the Hypercube 
Let N = {O :::: i :::: N -
I} be the set of processors in our parallel machine and assume 
that N = 211 for some integer n. Let x = (Xl, ... , XI!) be the binary representation of 
the number 0 :::: X :::: N -
1. 
Definition 4.3: The n-dimensional hypercube (or n-cube) is a nenrork ~I'itlz N = 211 
nodes such that node X has a direct connection to node y if and only {f'." ({nd ,v differ in 
exactly one bit. 
See Figure 4.1. Note that the total number of directed edges in the n-cube is 2n N. since 
each node is adjacent to n outgoing and n ingoing edges. Also, the diameter of the net-
work is n; that is, there is a directed path of length up to n connecting any two nodes 
in the network, and there are pairs of nodes that are not connected by any shorter path. 
The topology of the hypercube allows for a simple bit-fixing routing mechanism, as 
shown in Algorithm 4.1. When determining which edge to cross next, the algorithm 
simply considers each bit in order and crosses the edge if necessary. 
Although it seems quite natural, using only the bit-fixing routes can lead to high 
levels of congestion and poor performance, as shown in Exercise 4.20. There are cer-
tain permutations on which the bit-fixing routes behave poorly. It turns out, as we will 
73 

CHERNOFF BOUNDS 
n-Cube Bit-Fixing Routing Algorithm: 
1. Let a and b be the origin and the destination of the packet. 
2. For i = 1 to n, do: 
(a) If Gi #- bi then traverse the edge (hI, .... bi-I,ai, ... ,all ) ---+ 
(hI . ... , hi-I, bi, ai+l, ... , all)' 
Algorithm 4.1: n-Cube bit-fixing routing algorithm. 
show, that these routes perform well if each packet is being sent from a source to a 
destination chosen uniformly at random. This motivates the following approach: first 
route each packet to a randomly chosen intermediate point, and then route it from this 
intermediate point to its final destination. 
It may seem unusual to first route packets to a random intermediate point. In some 
sense, this is similar in spirit to our analysis of Quicksort in Section 2.5. We found there 
that for a list already sorted in reverse order, Quicksort would take Q(n 2 ) comparisons, 
whereas the expected number of comparisons for a randomly chosen permutation is 
only O( n log n). Randomizing the data can lead to a better running time for Quicksort. 
Here, too, randomizing the routes that packets take - by routing them through a ran-
dom intermediate point - avoids bad initial permutations and leads to good expected 
performance. 
The two-phase routing algorithm (Algorithm 4.2) is executed in parallel by all the 
packets. The random choices are made independently for each packet. Our analysis 
holds for any queueing policy that obeys the following natural requirement: if a queue 
is not empty at the beginning of a time step, some packet is sent along the edge associ-
ated with that queue during that time step. We prove that this routing strategy achieves 
asymptotically optimal parallel time. 
Theorem 4.12: Given an arbitrary permutation routing problem, with probability 
1 - O(N -I) the two-phase routing scheme (~fAlgorithm 4.2 routes all packets to their 
destinations on the n-cube in O(n) = D( log N) parallel steps. 
Proof· We first analyze the run-time of Phase I. To simplify the analysis we assume 
that no packet starts the execution of Phase II before all packets have finished the exe-
cution of Phase I. We show later that this assumption can be removed. 
We emphasize a fact that we use implicitly throughout. If a packet is routed to a 
randomly chosen node x in the network, we can think of.X: = (XI, ... , XII) as being 
generated by setting each Xi independently to be 0 with probability 1/2 and 1 with prob-
ability 1/2. 
For a given packet M, let TI (M) be the number of steps for M to finish Phase I. For 
a given edge e, let XI (e) denote the total number of packets that traverse edge e during 
Phase I. 
In each step of executing Phase I, packet M is either traversing an edge or waiting in a 
queue while some other packet traverses an edge on M's route. This simple observation 
74 

4.5* APPLICATION: PACKET ROUTING IN SPARSE NETWORKS 
Two-Phase Routing Algorithm: 
Phase I - Route the packet to a randomly chosen node in the network using the 
bit-fixing route. 
Phase II - Route the packet from its random location to its final destination using 
the bit-fixing route. 
Algorithm 4.2: Two-phase routing algorithm. 
relates the routing time of M to the total number of packet transitions through edges 
on the path of M, as follows. 
Lemma 4.13: Let el, ... , em be the m :::: n edges tra\'ersed hy a packet M in Phase I. 
Then 
111 
TI(M) :::: L 
XI(ei). 
i=1 
Let us call any path P = (el,e2, ... ,em) of rn :::: n edges that follo\\,s th~ bit-fixing 
algorithm apossihle packet path. We denote the corresponding nod~s by i'll. t'I.·· .. t'/Il 
with ei = (Vi-I, Vi). Following the definition of TI(M), for any possible packet path 
P we let 
111 
TI(P) = LXI(ei). 
i=1 
By Lemma 4.13, the probability that Phase I takes more than T steps is bounded by 
the probability that, for some possible packet path P, TI (P) 2: T. Note that there are 
at most 211 .2 11 = 2211 possible packet paths, since there are 211 possihl~ origins and 211 
possible destinations. 
To prove the theorem, we need a high-probability bound on TJ{ p). Sinc~ TI (P) 
equals the summation 2:;1=1 XI (ei), it would be natural to try to use a Chernoff bound. 
The difficulty here is that the XI (ei) are not independent random \·ariabIes. since a 
packet that traverses an edge is likely to traverse one of its adjacent edg~s. To circum-
vent this difficulty, we first use a Chernoff bound to prove that, with high probability. no 
more than 6n different packets cross any edge of P. We then condition on this e\'ent to 
derive a high-probability bound on the total number of transitions these packets make 
through edges of the path P, again using a Chernoff bound. I 
Let us now fix a specific possible packet path P with In edges. To obtain a high-
probability bound on the number of packets that cross an edge of P. let us call a packet 
active at a node Vi-Ion the path P if it reaches Vi-I and has the possibility of crossing 
I This approach overestimates the time to finish a phase. In fact, there is a deterministic argument showing that, 
in this setting. the delay of a packet on a path is bounded by the number of different packets that traverse edges 
of the path, and hence there is no need to bound the total number of traversals of these packets on the path. 
However, in the spirit of this book we prefer to present the probabilistic argument. 
75 

CHERNOFF BOUNDS 
edge ei to Vi. That is, if Vi-I and Vi differ in the .ith bit then - in order for a packet to 
be active at Vi-I - its .ith bit cannot have been fixed by the bit-fixing algorithm when it 
reaches Vi -I. We may also call a packet active if it is active at some vertex on the path 
P. We bound the total number of active packets. 
For k = 1, ... , N, let Hk be a 0-1 random variable such that Hk = 1 if the packet 
starting at node k is active and Hk = 0 otherwise. Notice that the Hk are indepen-
dent because (a) each Hk depends only on the choice of the intermediate destination of 
the packet starting at node k and (b) these choices are independent for all packets. Let 
H = L~=I Hk be the total number of active packets. 
We first bound E[H]. Consider all the active packets at Vi-I. Assume that Vi-I = 
(b l , ... , bj _ l , aj, aj+l, ... , an) and Vi = (b l , ... , bi- I, bi, aj+l, ... , an). Then only 
packets that start at one of the addresses (*, ... , *. aj,"" an), where * stands for either 
a 0 or a 1, can reach Vi-I before the .ith bit is fixed. Similarly, each of these packets ac-
tually reaches Vi-I only if its random destination is one of the addresses (b l , ••• , bj _ l , 
* , ... ,*). Thus, there are no more than 2)-1 possible active packets at Vi-I, and the 
probability that each of these packets is actually active at Vi-I is 2-(j-1). Hence the ex-
pected number of active packets per vertex is I and, since we need only consider the m 
vertices VQ, ... , V m -], it follows by linearity of expectations that 
E[H] :::: m· I:::: 11. 
Since H is the sum of independent 0-1 random variables, we can apply the Chernoff 
bound (we use (4.3)) to prove 
Pr(H ~ 6n ~ 6E[H]) :::: 2- 611 • 
The high-probability bound for H can help us obtain a bound for TI (P) as follows. 
Using 
Pr ( A) = Pr (A I B) Pr ( B) + Pr (A I B) Pr (B) 
:::: Pr ( B) + Pr (A I B), 
we find for a given possible packet path P that 
Pr(TI(P) ~ 3011) :::: Pr(H ~ 611) + Pr(TI(P) ~ 30n I H < 6n) 
:::: 2-
611 + Pr(TI(P) ~ 30n I H < 6n). 
Hence if we show 
we then have 
which proves sufficient for our purposes. 
We therefore need to bound the conditional probability Pr(TI (P) ~ 30n I H :::: 6n). 
In other words, conditioning on having no more than 6n active packets that might use 
edges of P, we need a bound on the total number of transitions that these packets take 
through edges of P. 
76 

4.5* APPLICATION: PACKET ROUTING IN SPARSE NETWORKS 
We first observe that, if a packet leaves the path, it cannot return to that path in this 
phase of the routing algorithm. Indeed, assume that the active packet was at Vi and 
that it moved to w #- Vi+). The smallest index bit in which Vi+1 and w differ cannot be 
fixed later in this phase, so the route of the packet and the path P cannot meet again in 
this phase. 
Now suppose we have an active packet on our path P at node Vi. What is the prob-
ability that the packet crosses ei? Let us think of our packet as fixing the bits in the 
binary representation of its destination one at a time by independent random coin flips. 
The nodes of the edge ei differ in one bit (say, the .ith bit) in this representation. It is 
therefore clear that the probability of the packet crossing edge ei is at most 1/2, since 
to cross this edge it must choose the appropriate value for the .ith bit. (In fact, the prob-
ability might be less than 112; the packet might cross some other edge before choosing 
the value of the .ith bit.) 
To obtain our bound, let us view as a trial each point in the algorithm where an active 
packet at a node Vi on the path P might cross edge ei. The trial is successful if the packet 
leaves the path but a failure if the packet stays on the path. Since the packet leaves the 
path on a successful trial, if there are at most 6n active packets then there can be at most 
6n successes. Each trial is successful, independently, with probability at least 1/2. The 
number of trials is itself a random variable, which we use in our bound of TI (P). 
We claim that the probability that the active packets cross edges of P more than 30n 
times is less than the probability that a fair coin flipped 3611 times comes up heads fewer 
than 6n times. To see this, think of a coin being flipped for each trial. with heads corre-
sponding to a success. The coin is biased to come up heads with the proper probability 
for each trial, but this probability is always at least 1/2 and the coins are independent 
for each trial. Each failure (tails) corresponds to an active packet crossing an edge, but 
once there have been 6n successes we know there are no more active packets left that 
can cross an edge of the path. Using a fair coin instead of a coin possibly biased in favor 
of success can only lessen the probability that the active packets cross edges of P more 
than 30n times, as can be shown easily by induction (on the number of biased coins). 
Letting Z be the number of heads in 36n fair coin flips, we now apply the Chernoff 
bound (4.5) to prove: 
Pr(T1(P) ~ 30n I H ::::: 611) ::::: Pr(Z ::::: 6n) ::::: e-18n (2j3)2/2 = e--+ II 
::::: 2--'11-1. 
It follows that 
as we wanted to show. Because there are at most 22n possible packet paths in the hy-
percube, the probability that there is an.Y possible packet path for which TI (P) ~ 30n 
is bounded by 
This completes the analysis of Phase I. Consider now the execution of Phase II, as-
suming that all packets completed their Phase I route. In this case, Phase II can be 
viewed as running Phase I backwards: instead of packets starting at a given origin and 
77 

CHERNOFF BOUNDS 
going to a random destination, they start at a random origin and end at a given des-
tination. Hence no packet spends more than 30n steps in Phase II with probability 
1 - O(N- I ). 
In fact, we can remove the assumption that packets begin Phase II only after Phase I 
has completed. The foregoing argument allows us to conclude that the total number of 
packet traversals across the edges of any packet path during Phase I and Phase II to-
gether is bounded by 60n with probability 1 - O(N- I ). Since a packet can be delayed 
only by another packet traversing that edge, we find that every packet completes both 
Phase I and Phase II after 60n steps with probability 1 - O(N -1) regardless of how the 
phases interact, concluding the proof of Theorem 4.12. 
• 
Note that the run-time of the routing algorithm is optimal up to a constant factor, since 
the diameter of the hypercube is n. However. the network is not fully utilized because 
2nN directed edges are used to route just N packets. At any give time, at most 1/2n 
of the edges are actually being used. This issue is addressed in the next section. 
4.5.2. Permutation Routing on the Butterfly 
In this section we adapt the result for permutation routing on the hypercube networks 
to routing on butterfly networks, yielding a significant improvement in network utiliza-
tion. Specifically, our goal in this section is to route a permutation on a network with 
N nodes and O(N) edges in OOog N) parallel time steps. Recall that the hypercube 
network had N nodes but Q (N log N) edges. Although the argument will be similar 
in spirit to that for the hypercube network, there is some additional complexity to the 
argument for the butterfly network. 
We work on the wrapped butterfly network. defined as follows. 
Definition 4.4: The wrapped butterfly network has N = n2
11 nodes. The nodes are 
arranged inn co/umns and 211 rmt's. A node's address is a pair (x, r), where 1 ::: x ::: 
211 is the row number and 0 :::::: r :::::: n - 1 is the co/umn number qfthe node. Node (x, 1') 
is connected to node (y . .'I) i(and only if's = r + 1 mod n and either: 
1. x = y (the "direct" edge): or 
2. x and y d(ffer in precisely the sth hit in their binary representation (the "flip" edge). 
See Figure 4.2. To see the relation between the wrapped butterfly and the hypercube, 
observe that by collapsing the n nodes in each row of the wrapped butterfly into one 
"super node" we obtain an n-cube network. Using this correspondence, one can eas-
ily verify that there is a unique directed path of length n connecting node (x, 1') to any 
other node (w, 1') in the same column. This path is obtained by bit fixing: first fixing 
bits I' + 1 to n, then bits 1 to r. See Algorithm 4.3. Our randomized permutation routing 
algorithm on the butterfly consists of three phases, as shown in Algorithm 4.4. 
Unlike our analysis of the hypercube, our analysis here cannot simply bound the 
number of active packets that possibly traverse edges of a path. Given the path of a 
78 

4.5* APPLICATION: PACKET ROUTING IN SPARSE NETWORKS 
row 000 
row 001 
row 010 
row 011 
row 100 
row 101 
row 110 
row 111 
Figure 4.2: The butterfly network. The wrapped butterfly network contain" additional edge" from 
level 3 back to level O. 
Wrapped Butterfly Bit-Fixing Routing Algorithm: 
1. Let (x, r) and (y, r) be the origin and the destination of a packet. 
2. For i = 0 to n -
1, do: 
(a) j = ((i + r) modn) + 1; 
(b) if aj = hj then traverse the direct edge to column j mod 11. else trayerse 
the flip edge to column j mod n. 
Algorithm 4.3: Wrapped butterfly bit-fixing routing algorithm. 
packet, the expected number of other packets that share edges with this path when rout-
ing a random permutation on the butterfly network is [2(112) and not 0(11) as in the 
n-cube. To obtain an O(n) routing time, we need a more refined analysis technique 
that takes into account the order in which packets traverse edges. 
Because of this, we need to consider the priority policy that the queues use when 
there are several packets waiting to use the edge. A variety of priority policies would 
work here; we assume the following rules. 
79 

CHERNOFF BOUNDS 
Three-Phase Routing Algorithm: 
For a packet sent from node (x, r) to node (y, s): 
Phase I - Choose a random WE [1, ... ,2
11 ]. Route the packet from node (x, r) to 
node (w, r) using the bit-fixing route. 
Phase II - Route the packet to node (w, s) using direct edges. 
Phase III - Route the packet from node (w, s) to node (y, s) using the bit-fixing 
route. 
Algorithm 4.4: Three-phase routing algorithm. 
1. The priority of a packet traversing an edge is (i -
l)n + t, where i is the current 
phase of the packet and t is the number of edge traversals the packet has already 
executed in this phase. 
2. If at any step more than one packet is available to traverse an edge, the packet with 
the smallest priority number is sent first. 
Theorem 4.14: Given an arbitrary permutation routing problem on the wrapped but-
terfly with N = n2n nodes, with probability 1- O( N -I) the three-phase routing scheme 
of Algorithm 4.4 routes all packets to their destinations in O(n) = O( log N) parallel 
steps. 
Proof: The priority rule in the edge queues guarantees that packets in a phase cannot 
delay packets in earlier phases. Because of this, in our forthcoming analysis we can 
consider the time for each phase to complete separately and then add these times to 
bound the total time for the three-phase routing scheme to complete. 
We begin by considering the second phase. We first argue that with high probability 
each row transmits at most 4n packets in the second phase. To see this, let X U1 be the 
number of packets whose intermediate row choice is w in the three-phase routing algo-
rithm. Then XU! is the sum of 0-1 independent random variables, one for each packet, 
and E[Xu.l = n. Hence, we can directly apply the Chernoff bound (4.3) to find 
Pre X". 2: 4/1) :s (~: r 
:s r2n. 
There are 2/1 possible rows U'. By the union bound, the probability that any row has 
more than 4n packets is only 211 .3- 211 = 0(N-
1 ). 
We now argue that, if each row has at most 4n packets for the second phase, then the 
second phase takes at most 5n steps to complete. Combined with our previous observa-
tions, this means the second phase takes at most 5n steps with probability 1 - O(N -I). 
To see this, note that in the second phase the routing has a special structure: each packet 
moves from edge to edge along its row. Because of the priority rule, each packet can 
be delayed only by packets already in a queue when it arrives. Therefore, to place an 
upper bound on the number of packets that delay a packet p, we can bound the total 
number of packets found in each queue when p arrives at the queue. But in Phase II, 
80 

4.5* APPLICATION: PACKET ROUTING IN SPARSE NETWORKS 
the number of other packets that an arriving packet finds in a queue cannot increase 
in size over time, since at each step a queue sends a packet and receives at most one 
packet. (It is worth considering the special case when a queue becomes empty at some 
point in Phase II; this queue can receive another packet at some later step, but the num-
ber of packets an arriving packet will find in the queue after that point is always zero.) 
Since there are at most 4n packets total in the row to begin with, p finds at most 4n 
packets that delay it as it moves from queue to queue. Since each packet moves at most 
n times in the second phase, the total time for the phase is 5n steps. 
We now consider the other phases. The first and third phases are again the same by 
symmetry, so we consider just the first phase. Our analysis will use a delay sequence 
argument. 
Definition 4.5: A delay sequence for an execution of Phase I is ([ sequence of n edges 
el, ... , ell such that either ei = ei+l or ei+l is an outgoing edge ./1"0111 the end vertex of 
ei. The sequence el, ... , en has the further property that ei is \one (~n the last edges to 
transmit packets with priority up to i among ei+l and the two incoming edges (~f ei+l. 
The relation between the delay sequence and the time for Phase I to complete is given 
by the following lemma. 
Lemma 4.15: For ([ given execution of Phase I and dela)' sequence el . .... ell' let ti 
be the number of packets with priority i sent through edge ei. Let Ti be the time that 
edge ei finishes sending all packets with priority number up to i. so that Til is the ear-
liest time at which all packets passing through en during Phase I /1(1\'e passed through 
it. Then: 
I. Til::: L~l=1 ti· 
2. If the execution of Phase I takes T steps, then there is a delay sequence for this exe-
cutionfor which L~l=1 ti :2: T. 
Proof: By the design of the delay sequence, at time Ti the queue of ei-l already holds 
all of the packets that it will need to subsequently transmit with priority i + L and at 
that time it has already finished transmitting all packets with priority numbers up to i. 
Thus, 
Since Tl = tl, we have 
II 
i=l 
proving the first part of the lemma. 
For the second part, assume that Phase I took T steps and let e be an edge that trans-
mitted a packet at time T. We can construct a delay sequence with ell = e by choosing 
81 

CHERNOFF BOUNDS 
e/f-l to be the edge among e and its two incoming edges that last transmits packets of 
priority n -
1, and similarly choosing en-2 down to el. By the first part of the lemma, 
L;I=1 ti 2: T. 
D 
Returning to the proof of Theorem 4.14, we now show that the probability of a de-
lay sequence with T 2: 40n is only O(N-
1 ). We call any sequence of edges el, ... ,ell 
such that either ei = ei+l or ei+l is an outgoing edge from the end vertex of ei a possi-
ble dela.\' sequence. For a given execution and a possible delay sequence, let ti be the 
number of packets with priority i sent through ei. Let T = L;'=l ti. We first bound 
E[T]. Consider the edge ei = v -+ Vi. Packets with priority i pass through this edge 
only if their source is at distance i-I from tl. There are precisely 2 i - 1 nodes that are 
connected to v by a directed path of length i-I. Since packets are sent in Phase I to 
random destinations, the probability that each of these nodes sends a packet that tra-
verses edge ei is 2-i , giving 
n 
and 
E[T] = -. 
2 
The motivation for using the delay sequence argument should now be clear. Each 
possible delay sequence defines a random variable T, where E[T] = n/2. The max-
imum of T over all delay sequences bounds the run-time of the phase. So we need 
a bound on T that holds with sufficiently high probability to cover all possible delay 
sequences. A high-probability bound on T can now be obtained using an argument 
similar to the one used in the proof of Theorem 4.12. We first bound the number of 
different packets that contribute to edge traversals counted in T. 
For.i = 1 ... , N, let Hi = 1 if any traversal of the packet sent by node .i is counted 
in T: otherwise. Hi = O. Clearly. H = Li: j Hi :s T and E[H] :s E[T] = n/2, 
where the Hi are independent random variables. Applying the Chernoff bound (4.3) 
therefore yields 
Conditioning on the e\'ent H :s 511. we now proceed to prove a bound on T, follow-
ing the same line as in the proof of Theorem 4.12. Given a packet u with at least one 
traversal counted in T. we consider how many additional traversals of u are counted in 
T. Specifically, if II is counted in fi then we consider the probability that it is counted 
in ti+l' We distinguish between two cases as follows. 
1. If ei+l = ei then u cannot be counted in ti+l, since its traversal with priority i + 1 is 
in the next column. Similarly. it cannot be counted in any fi' j > i. 
2. If ei+l -=I=- ei, then the probability that u continues through ei+l (and is counted in 
ti+l) is at most 1/2. Ifit does not continue through ei+l, then it cannot intersect with 
the delay sequence in any further traversals in this phase. 
As in the proof of Theorem 4.12. the probability that T 2: 40n is less than the prob-
ability that a fair coin flipped 40n times comes up heads fewer than 5n times. (Keep in 
mind that, in this case. the first traversal by each packet in H must be counted as con-
tributing to T.) Letting Z be the number of heads in 40n fair coin flips, we now apply 
the Chernoff bound (4.5) to prove 
82 

4.6 EXERCISES 
Pr(T ~ 40n I H ::s 5n) ::s Pr(Z ::s 5n) ::s e-20Il (3/4)2/2 ::s 2- 511 . 
We conclude that 
Pr(T ~ 40n) ::s Pr(T ~ 40n I H ::s 511) + Pr(H ~ 5n) ::s 2-
511+1• 
There are no more than 2N3 11- 1 ::s n2113 11 possible delay sequences. Thus, the prob-
ability that, in the execution of Phase I, there is a delay sequence with T > 40n is 
bounded above (using the union bound) by 
112"3112-511+1::s O(N- I ). 
Since Phase III is entirely similar to Phase I and since Phase II also finishes in O(n) 
steps with probability 1 - O( N -I), we have that the three-phase routing algorithm fin-
ishes in O(n) steps with probability 1 - O(N-I). 
• 
4.6. Exercises 
Exercise 4.1: Alice and Bob play checkers often. Alice is a better player. so the proba-
bility that she wins any given game is 0.6, independent of all other games. They decide 
to playa tournament of n games. Bound the probability that Alice loses the tournament 
using a Chernoff bound. 
Exercise 4.2: We have a standard six-sided die. Let X be the number of times that 
a 6 occurs over n throws of the die. Let p be the probability of the event X ~ 11 /-+. 
Compare the best upper bounds on p that you can obtain using Markov's inequality. 
Chebyshev's inequality, and Chernoff bounds. 
Exercise 4.3: (a) Determine the moment generating function for the binomial random 
variable B(n. p). 
(b) Let X be a B(n, p) random variable and Y a B(rn, p) random variable. where X 
and Yare independent. Use part (a) to determine the moment generating function of 
X+Y. 
(c) What can we conclude from the form of the moment generating function of 
X + Y? 
Exercise 4.4: Determine the probability of obtaining 55 or more heads when flipping 
a fair coin 100 times by an explicit calculation, and compare this with the Chernoff 
bound. Do the same for 550 or more heads in 1000 flips. 
Exercise 4.5: We plan to conduct an opinion poll to find out the percentage of people 
in a community who want its president impeached. Assume that every person answers 
either yes or no. If the actual fraction of people who want the president impeached is 
p, we want to find an estimate X of p such that 
Pr(IX-pl ::SEp) > 1-8 
for a given E and 8, with 0 < E,8 < 1. 
83 

CHERNOFF BOUNDS 
We query N people chosen independently and uniformly at random from the com-
munity and output the fraction of them who want the president impeached. How large 
should N be for our result to be a suitable estimator of p? Use Chernoff bounds, and 
express N in terms of p, E, and 8. Calculate the value of N from your bound if E = 0.1 
and 8 = 0.05 and if you know that p is between 0.2 and 0.8. 
Exercise 4.6: (a) In an election with two candidates using paper ballots, each vote 
is independently misrecorded with probability p = 0.02. Use a Chernoff bound to 
bound the probability that more than 4% of the votes are misrecorded in an election of 
1,000,000 ballots. 
(b) Assume that a misrecorded ballot always counts as a vote for the other candi-
date. Suppose that candidate A received 5lO,000 votes and that candidate B received 
490,000 votes. Use Chernoff bounds to bound the probability that candidate B wins 
the election owing to misrecorded ballots. Specifically, let X be the number of votes 
for candidate A that are misrecorded and let Y be the number of votes for candidate B 
that are misrecorded. Bound Pre (X > k) n (Y < e)) for suitable choices of k and e. 
Exercise 4.7: Throughout the chapter we implicitly assumed the following extension 
of the Chernoff bound. Prove that it is true. 
Let X = L:7=1 Xi, where the Xi are independent 0-1 random variables. Let Il = 
E[X]. Choose any ilL and IlH such that ilL ~ Il ~ IlH. Then, for any 8 > 0, 
( 
e() 
)f1.H 
Pr X > 1 + 8 
< 
( 
-
( 
)IlH) -
0+8)0+3) 
Similarly, for any 0 < 8 < 1, 
Exercise 4.8: We show how to construct a random permutation 7T on [1, n], given a 
black box that outputs numbers independently and uniformly at random from [1, k] 
wherek ~ n. If we compute a function f: [1,n] -+ [1,k] with f(i) #- f(j) fori #-}, 
this yields a permutation: simply output the numbers [1, n] according to the order of the 
f(i) values. To construct such a function f, do the following for} = 1, ... , n: choose 
f(j) by repeatedly obtaining numbers from the black box and setting f(j) to the first 
number found such that f(j) #- f(i) for i < }. 
Prove that this approach gives a permutation chosen uniformly at random from all 
permutations. Find the expected number of calls to the black box that are needed when 
k = nand k = 2n. For the case k = 2n, argue that the probability that each call to 
the black box assigns a value of f(}) to some} is at least 1/2. Based on this, use a 
Chernoff bound to bound the probability that the number of calls to the black box is at 
least 4n. 
Exercise 4.9: Suppose that we can obtain independent samples Xl, X 2 , ... of a ran-
dom variable X and that we want to use these samples to estimate E[X]. Using t 
84 

4.6 EXERCISES 
samples, we use (L~=l Xi)/t for our estimate of E[X]. We want the estimate to be 
within sE[X] from the true value of E[X] with probability at least 1 - 8. We may not 
be able to use Chernoff's bound directly to bound how good our estimate is if X is not 
a 0-1 random variable, and we do not know its moment generating function. We de-
velop an alternative approach that requires only having a bound on the variance of X. 
Let r = JVar[X]/E[X]. 
(a) Show using Chebyshev's inequality that OCr 2/ s28) samples are sufficient to solve 
the problem. 
(b) Suppose that we need only a weak estimate that is within sE[X] of E[X] with 
probability at least 3/4. Argue that O(r2/s2) samples are enough for this weak 
estimate. 
(c) Show that, by taking the median of O(log(l/8)) weak estimates, we can obtain an 
estimate within sE [X] of E [X] with probability at least I - 8. Conclude that we 
need only O( (r2 log(l/ 8))/ S2) samples. 
Exercise 4.10: A casino is testing a new class of simple slot machines. Each game, the 
player puts in $1, and the slot machine is supposed to return either $3 to the player with 
probability 4/25, $100 with probability 1/200, or nothing with all remaining probabil-
ity. Each game is supposed to be independent of other games. 
The casino has been surprised to find in testing that the machines have lost $10,000 
over the first million games. Derive a Chernoff bound for the probability of this event. 
You may want to use a calculator or program to help you choose appropriate values as 
you derive your bound. 
Exercise 4.11: Consider a collection Xl, ... , X /1 of n independent integers chosen uni-
formly from the set {O, 1, 2}. Let X = L;'=l Xi and 0 < 8 < 1. Derive a Chernoff 
bound for Pr(X ::::: (1 + 8)n) and Pr(X ~ (l - 8)n). 
Exercise 4.12: Consider a collection Xl, ... , X/1 of n independent geometrically dis-
tributed random variables with mean 2. Let X = L7=1 Xi and 8 > O. 
(a) Derive a bound on Pre X ~ (1 + 8)(2n)) by applying the Chernoff bound to a 
sequence of (1 + 8) (2n) fair coin tosses. 
(b) Directly derive a Chernoff bound on Pr(X ::::: (l + 8)(2n)) using the moment gen-
erating function for geometric random variables. 
(c) Which bound is better? 
Exercise 4.13: Let Xl, .. " X/1 be independent Poisson trials such that Pr(Xi = 1) = 
p. Let X = L7=1 Xi, so that E[X] = pn. Let 
F(x, p) = x In(x/p) + (1 - x) In((1 - x)/(l -
p)). 
(a) Show that, for 1 ::::: x > p, 
85 

CHERNOFF BOUNDS 
(b) Show that, when ° < x, P < 1, we have F(x, p) - 2(x -
p)2 2: 0. (Hint: Take 
the second derivative of F(x, p) - 2(x -
p)2 with respect to x.) 
(c) Using parts (a) and (b), argue that 
Pr (X 2: (p + E) 11) ~ e - 2 II F 
2 
• 
(d) Use symmetry to argue that 
and conclude that 
Exercise 4.14: Modify the proof of Theorem 4.4 to show the following bound for a 
weighted sum of Poisson trials. Let Xl, ... , XII be independent Poisson trials such that 
Pre Xi) = Pi and let al,.·., an be real numbers in [0,1]. Let X = Z=7=1 aiXi and f-1 = 
E[X]. Then the following Chernoff bound holds: for any 8 > 0, 
( 
3 
)11. 
Pr(X 2: (1 + 8)f-1)::::; 
(1 + ~)(l+8) 
Prove a similar bound for the probability that X ~ (1 -
8)f-1 for any ° 
< 8 < 1. 
Exercise 4.15: Let Xl, ... ,XI1 be independent random variables such that 
Pr(Xi = 1 - Pi) = Pi 
and 
Pr(Xj = -Pi) = 1 - Pi· 
Hint: You may need to assume the inequality 
This inequality is difficult to proye directly. 
Exercise 4.16: Let Xl, " " XII be independent Poisson trials such that Pr(Xi ) = Pi. 
Let X = Z=;'=l aiXi and f-1 = E[XJ. Use the result of Exercise 4.15 to prove that, for 
any ° 
< 8 < 1, 
Exercise 4.17: Suppose that we have n jobs to distribute among m processors. For 
simplicity, we assume that m divides n. A job takes 1 step with probability P and k > 1 
steps with probability 1- p. Use Chernoff bounds to determine upper and lower bounds 
(that hold with high probability) on when all jobs will be completed if we randomly 
assign exact! y 11 / m jobs to each processor. 
86 

4.6 EXERCISES 
Exercise 4.18: In many wireless communication systems, each receiver listens on a 
specific frequency. The bit b(t) sent at time t is represented by a 1 or -1. Unfortunately, 
noise from other nearby communications can affect the receiver's signal. A simplified 
model of this noise is as follows. There are n other senders, and the ith has strength 
Pi ::: 1. At any time t, the ith sender is also trying to send a bit bi(t) that is represented 
by 1 or -1. The receiver obtains the signal s (t) given by 
11 
s(t) = b(t) + L Pibi(t). 
;=1 
If s (t) is closer to 1 than -1, the receiver assumes that the bit sent at time t was a 1; 
otherwise, the receiver assumes that it was a-I. 
Assume that all the bits bi(t) can be considered independent. uniform random vari-
ables. Give a Chernoff bound to estimate the probability that the receiver makes an 
error in determining b(t). 
Exercise 4.19: Recall that a function f is said to be com'ex if. for any x I, X2 and for 
0:::),,:::1, 
(a) Let Z be a random variable that takes on a (finite) set of values in the interval [0, I], 
and let P = E[Z]. Define the Bernoulli random variable X by Pr( X = I) = P and 
Pr(X = 0) = I - p. Show that E[f(Z)] ::: E[f(X)] for any convex function f. 
(b) Use the fact that f(x) = etx is convex for any t > 0 to obtain a Chernoff-like 
bound for Z based on a Chernoff bound for X. 
Exercise 4.20: We prove that the Randomized Quicksort algorithm sorts a set of n 
numbers in time O(n log n) with high probability. Consider the following view of 
Randomized Quicksort. Every point in the algorithm where it decides on a pivot ele-
ment is called a node. Suppose the size of the set to be sorted at a particular node is s. 
The node is called good if the pivot element divides the set into two parts. each of size 
not exceeding 2s13. Otherwise the node is called bad. The nodes can be thought of 
as forming a tree in which the root node has the whole set to be sorted and its children 
have the two sets formed after the first pivot step and so on. 
(a) Show that the number of good nodes in any path from the root to a leaf in this tree 
is not greater than c log2 n, where c is some positive constant. 
(b) Show that, with high probability (greater than I - Iln 2 ), the number of nodes in a 
given root to leaf path of the tree is not greater than c'log2 n, where c' is another 
constant. 
(c) Show that, with high probability (greater than 1- lin), the number of nodes in the 
longest root to leaf path is not greater than c' log2 n. (Hint: How many nodes are 
there in the tree?) 
(d) Use your answers to show that the running time of Quicksort is O(n log n) with 
probability at least I - lin. 
87 

CHERNOFF BOUNDS 
Exercise 4.21: Consider the bit-fixing routing algorithm for routing a permutation on 
the n-cube. Suppose that 11 is even. Write each source node s as the concatenation of 
two binary strings a l and b l each of length nj2. Let the destination of s's packet be 
the concatenation of b~ and a~. Show that this permutation causes the bit-fixing routing 
algorithm to take [2 ( vfN) steps. 
Exercise 4.22: Consider the following modification to the bit-fixing routing algorithm 
for routing a permutation on the n-cube. Suppose that, instead of fixing the bits in order 
from 1 to n. each packet chooses a random order (independent of other packets' choices) 
and fixes the bits in that order. Show that there is a permutation for which this algo-
rithm requires 2
Q
(II) steps with high probability. 
Exercise 4.23: Assume that we use the randomized routing algorithm for the n-cube 
network (Algorithm 4.2) to route a total of up to p2" packets. where each node is the 
source of no more than p packets and each node is the destination of no more than p 
packets. 
(a) Give a high-probability bound on the run-time of the algorithm. 
(b) Give a high-probability bound on the maximum number of packets at any node at 
any step of the execution of the routing algorithm. 
Exercise 4.24: Show that the expected number of packets that traverse any edge on 
the path of a given packet when routing a random permutation on the wrapped butterfly 
network of N = 112/1 nodes is [2(11 2 ). 
Exercise 4.25: In this exercise. we design a randomized algorithm for the following 
packet routing problem. We are given a network that is an undirected connected graph 
G, where nodes represent processors and the edges between the nodes represent wires. 
We are also given a set of N packets to route. For each packet we are given a source 
node. a destination node. and the exact route (path in the graph) that the packet should 
take from the source to its destination. (We may assume that there are no loops in the 
path.) In each time step. at most one packet can traverse an edge. A packet can wait at 
any node during any time step. and we assume unbounded queue sizes at each node. 
A schedule for a set of packets specifies the timing for the movement of packets 
along their respective routes. That is. it specifies which packet should move and which 
should wait at each time step. Our goal is to produce a schedule for the packets that 
tries to minimize the total time and the maximum queue size needed to route all the 
packets to their destinations. 
(a) The dilation d is the maximum distance traveled by any packet. The congestion c 
is the maximum number of packets that must traverse a single edge during the en-
tire course of the routing. Argue that the time required for any schedule should be 
at least [2 (c + d). 
(b) Consider the following unconstrained schedule, where many packets may traverse 
an edge during a single time step. Assign each packet an integral delay chosen ran-
domly, independently, and uniformly from the interval [1, 100cjlog(Ndn]' where 
88 

4.6 EXERCISES 
a is a constant. A packet that is assigned a delay of x waits in its source node for 
x time steps; then it moves on to its final destination through its specified route 
without ever stopping. Give an upper bound on the probability that more than 
O( 10g(Nd)) packets use a particular edge e at a particular time step t. 
(c) Again using the unconstrained schedule of part (b), show that the probability that 
more than O(log(Nd)) packets pass through any edge at any time step is at most 
l/(Nd) for a sufficiently large a. 
(d) Use the unconstrained schedule to devise a simple randomized algorithm that, with 
high probability, produces a schedule of length O(c + d 10g(Nd)) using queues of 
size O(log(Nd)) and following the constraint that at most one packet crosses an 
edge per time step. 
89 

CHAPTER FIVE 
Balls, Bins, and RandoDl Graphs 
In this chapter, we focus on one of the most basic of random processes: m balls are 
thrown randomly into n bins, each ball landing in a bin chosen independently and uni-
formly at random. We use the techniques we have developed previously to analyze 
this process and develop a new approach based on what is known as the Poisson ap-
proximation. We demonstrate several applications of this model, including a more 
sophisticated analysis of the coupon collector's problem and an analysis of the Bloom 
filter data structure. After introducing a closely related model of random graphs, we 
show an efficient algorithm for finding a Hamiltonian cycle on a random graph with 
sufficiently many edges. Even though finding a Hamiltonian cycle is NP-hard in gen-
eral, our result shows that, for a randomly chosen graph, the problem is solvable in 
polynomial time with high probability. 
5.1. Example: The Birthday Paradox 
Sitting in lecture, you notice that there are 30 people in the room. Is it more likely that 
some two people in the room share the same birthday or that no two people in the room 
share the same birthday? 
We can model this problem by assuming that the birthday of each person is a random 
day from a 365-day year, chosen independently and uniformly at random for each per-
son. This is obviously a simplification; for example, we assume that a person's birthday 
is equally likely to be any day of the year, we avoid the issue of leap years, and we ig-
nore the possibility of twins! As a model, however, it has the virtue of being easy to 
understand and analyze. 
One way to calculate this probability is to directly count the configurations where 
two people do not share a birthday. It is easier to think about the configurations where 
people do not share a birthday than about configurations where some two people do. 
Thirty days must be chosen from the 365; there are (~~n ways to do this. These 30 
days can be assigned to the people in any of the 30! possible orders. Hence there are 
(~~n30! configurations where no two people share the same birthday, out of the 365 30 
ways the birthdays could occur. Thus, the probability is 
90 

5.1 EXAMPLE: THE BIRTHDAY PARADOX 
(~~n30! 
365 30 
. 
(5.1) 
We can also calculate this probability by considering one person at a time. The first 
person in the room has a birthday. The probability that the second person has a differ-
ent birthday is (1 - 1/365). The probability that the third person in the room then has a 
birthday different from the first two, given that the first two people have different birth-
days, is (1 - 2/365). Continuing on, the probability that the kth person in the room 
has a different birthday than the first k -
I, assuming that the first k -
I have differ-
ent birthdays, is (1 - (k - 1)/365). So the probability that 30 people all have different 
birthdays is the product of these terms, or 
You can check that this matches the expression (5.1). 
Calculations reveal that (to four decimal places) this product is 0.2937. so when 30 
people are in the room there is more than a 70% chance that two share the same birth-
day. A similar calculation shows that only 23 people need to be in the room before it 
is more likely than not that two people share a birthday. 
More generally, if there are m people and n possible birthdays then the probability 
that all m have different birthdays is 
( I) ( 2) ( 3) ( m-I) 
III-I 
j' 
1-~ . I-~ . I-~ ... I--n -
=n(I-~). 
J=I 
Using that 1 - kin ~ e-k/ n when k is small compared to n, we see that if lJ1 is small 
compared to n then 
fi(l - ~) ~ IT c-
j /" 
j=1 
j=1 
= cxp { - ~ 
~ l 
J=I 
= e- m (m-I)/2n 
Hence the value for m at which the probability that m people all have different birth-
days is 1/2 is approximately given by the equation 
m 2 
-=ln2 
2n 
' 
or m = J2n In 2. For the case n = 365, this approximation gives lJ1 = 22.49 to two 
decimal places, matching the exact calculation quite well. 
Quite tight and formal bounds can be established using bounds in place of the ap-
proximations just derived, an option that is considered in Exercise 5.3. The following 
simple arguments, however, give loose bounds and good intuition. Let us consider each 
91 

BALLS, BINS, AND RANDOM GRAPHS 
person one at a time, and let Ek be the event that the kth person's birthday does not 
match any of the birthdays of the first k - 1 people. Then the probability that the first 
k people fail to have distinct birthdays is 
k 
Pr(E 1 U E2 U ... U Ed :s L Pr(Ei ) 
;=1 
~i-I 
<L--
n 
i=l 
k(k -
1) 
211 
If k :s fo this probability is less than 1/2, so with l Jii J people the probability is at 
least 1/2 that all birthdays will be distinct. 
Now assume that the first 1 fo l people all have distinct birthdays. Each person after 
that has probability at least fo /n = 1/ fo of having the same birthday as one of these 
first 1 fo l people. Hence the probability that the next 1 fo l people all have different 
birthdays than the first 1 fo l people is at most 
(
1 __ 
1 )1' III < _ < _ 
Jii 
e 
2 
Hence, once there are 21 fo l people, the probability is at most 1/ e that all birthdays 
will be distinct. 
5.2. Balls into Bins 
5.2.1. The Balls-and-Bins Model 
The birthday paradox is an example of a more general mathematical framework that 
is often formulated in terms of balls and bins. We have m balls that are thrown into 
n bins, with the location of each ball chosen independently and uniformly at random 
from the n possibilities. What does the distribution of the balls in the bins look like? 
The question behind the birthday paradox is whether or not there is a bin with two 
balls. 
There are several interesting questions that we could ask about this random process. 
For example, how many of the bins are empty? How many balls are in the fullest bin? 
Many of these questions have applications to the design and analysis of algorithms. 
Our analysis of the birthday paradox showed that, if m balls are randomly placed 
into n bins then, for some m = Q (Jii), at least one of the bins is likely to have more 
than one ball in it. Another interesting question concerns the maximum number of 
balls in a bin, or the maximum load. Let us consider the case where m = n, so that 
the number of balls equals the number of bins and the average load is 1. Of course the 
maximum possible load is 11, but it is very unlikely that all n balls land in the same 
bin. We seek an upper bound that holds with probability tending to 1 as n grows large. 
92 

5.2 BALLS INTO BINS 
We can show that the maximum load is not more than 3ln n /In In n with probability at 
most lin for sufficiently large n via a direct calculation and a union bound. This is a 
very loose bound; although the maximum load is in fact Q On n / In In n) with probabil-
ity close to 1 (as we show later), the constant factor 3 we use here is chosen to simplify 
the argument and could be reduced with more care. 
Lemma 5.1: When n balls are thrown independently and un~formly at random into n 
bins, the probability that the maximum load is more than 3ln n /In In n is at most lin 
for n sufficiently large. 
Proof: The probability that bin 1 receives at least M balls is at most 
This follows from a union bound; there are (~) distinct sets of M balls, and for any set 
of M balls the probability that all land in bin 1 is (l/n)'H. We now use the inequalities 
( 
n ) ( 1 )M 
1 
( e ) H 
M 
;; 
:s M!:S 
M 
Here the second inequality is a consequence of the following general bound on facto-
rials: since 
we have 
k' > G)' 
Applying a union bound again allows us to find that, for M ~ 31n n /In In 11, the prob-
ability that any bin receives at least M balls is bounded above by 
for n sufficiently large. 
n (~)M :s n ( e In In 11 )3 In n / In In n 
M 
3lnn 
(
In Inn )3 In n / In In 11 
<n --
-
Inn 
= elnn (eln In Inn-In Inn )3Inn/ln In II 
= e-2Inll+3(lnn)(lnlnlnn)/lnlnn 
< 
n 
5.2.2. Application: Bucket Sort 
• 
Bucket sort is an example of a sorting algorithm that, under certain assumptions on the 
input, breaks the Q (n log n) lower bound for standard comparison-based sorting. For 
93 

BALLS, BINS, AND RANDOM GRAPHS 
example, suppose that we have a set of n = 2/11 elements to be sorted and that each 
element is an integer chosen independently and uniformly at random from the range 
[0,2/\), where k ::::: m. Using Bucket sort, we can sort the numbers in expected time 
O(n). Here the expectation is over the choice of the random input, since Bucket sort 
is a completely deterministic algorithm. 
Bucket sort works in two stages. In the first stage, we place the elements into n 
buckets. The jth bucket holds all elements whose first m binary digits correspond to 
the number j. For example, if n = 2 10, bucket 3 contains all elements whose first 10 
binary digits are 0000000011. When.i < L the elements of the .ith bucket all come 
before the elements in the e th bucket in the sorted order. Assuming that each element 
can be placed in the appropriate bucket in 0(1) time, this stage requires only O(n) 
time. Because of the assumption that the elements to be sorted are chosen uniformly, 
the number of elements that land in a specific bucket follows a binomial distribution 
B(I1, 1/11). Buckets can be implemented using linked lists. 
In the second stage, each bucket is sorted using any standard quadratic time algo-
rithm (such as Bubblesort or Insertion sort). Concatenating the sorted lists from each 
bucket in order gives us the sorted order for the elements. It remains to show that the 
expected time spent in the second stage is only O( 11). 
The result relies on our assumption regarding the input distribution. Under the uni-
form distribution, Bucket sort falls naturally into the balls and bins model: the elements 
are balls, buckets are bins, and each ball falls uniformly at random into a bin. 
Let Xj be the number of elements that land in the jth bucket. The time to sort the 
jth bucket is then at most c( Xi)2 for some constant c. The expected time spent sorting 
in the second stage is at most 
[ 
/I 
] 
/I 
E L c( Xi)2 
= C L E[X)~] = cnE[X,2l, 
j=' 
)=, 
where the first equality follows from the linearity of expectations and the second fol-
lows from symmetry, as E [ X? 1 is the same for all buckets. 
Since X, is a binomial random variable B(I1, lin), using the results of Section 3.2.1 
yields 
-, 
11(11-1) 
1 
E[X,-l= 
-, 
+1=2--<2. 
11-
n 
Hence the total expected time spent in the second stage is at most 2cl1, so Bucket sort 
runs in expected linear time. 
5.3. The Poisson Distribution 
We now consider the probability that a given bin is empty in the balls and bins model 
with m balls and n bins as well as the expected number of empty bins. For the first bin 
to be empty, it must be missed by all m balls. Since each ball hits the first bin with 
probability 1 /n, the probability the first bin remains empty is 
94 

5.3 THE POISSON DISTRIBUTION 
( 
1)111 
1 - -
~ e~II1/II; 
n 
of course, by symmetry this probability is the same for all bins. If Xj is a random vari-
able that is 1 when the jth bin is empty and 0 otherwise, then E[Xj 1 = (1 -
Iln)lI1. 
Let X be a random variable that represents the number of empty bins. Then, by the 
linearity of expectations, 
[ 
II 
] 
II 
(1)111 
E[X] = E LXi = L E[Xi 1 = n 1 -
-;; 
~ ne~II1/I1. 
1=1 
1=1 
Thus, the expected fraction of empty bins is approximately e- III
/ II. This approxima-
tion is very good even for moderately size values of 111 and n. and we use it frequently 
throughout the chapter. 
We can generalize the preceding argument to find the expected fraction of bins with 
r balls for any constant r. The probability that a given bin has r balls is 
(
1l1) (~)I'(1 _ 
~)If/-" = ~ 
111 (1l1 - 1) ... (m -
r + 1) (1 _ ~ )11I~1 
r 
n 
n 
r! 
n" 
n 
When 111 and n are large compared to r, the second factor on the right-hand side is ap-
proximately (mlnY, and the third factor is approximately e-
11I ". Hence the probability 
p,. that a given bin has r balls is approximately 
e- IIl / I1(m/nY 
Pr ~ 
, 
r! 
(5.2) 
and the expected number of bins with exactly r balls is approximately n Pr. \Ve formal-
ize this relationship in Section 5,3.1. 
The previous calculation naturally leads us to consider the following distribution. 
Definition 5.1: A discrete Poisson random variable X with parameterj) is gil'en hy the 
following probability distribution on j = 0, 1,2, ... : 
e~flf.1.,j 
Pr(X = j) = -.-,-. 
J. 
(Note that Poisson random variables differ from Poisson trials, discussed in Section 
4.2.1.) 
Let us verify that the definition gives a proper distribution in that the probabilities 
sum to 1: 
:x, 
X! 
~ 11 
j 
LPr(X = j) = L ~ 
j=O 
j=O 
J. 
~ LX! f.1.,i 
=e
fl 
- ., 
j=O J. 
= 1, 
where we have used the Taylor expansion eX = Ll=o(xiIP). 
95 

BALLS, BINS, AND RANDOM GRAPHS 
Next we show that the expectation of this random variable is fJ.,: 
E [X] = L j Pr( X = j) 
j=() 
oc 
. 
_ '""' . e-11fJ.,J 
-~J ., 
j=l 
J. 
oc 
. 
_ 
'""' e-
11
fJ.,.1 
-fJ.,~ 
., 
j=() 
J. 
=fJ.,. 
In the context of throwing m balls into n bins. the distribution of the number of balls 
in a bin is approximately Poisson with fJ., = lJ1 /11. which is exactly the average number 
of balls per bin, as one might expect. 
An important property of Poisson distributions is given in the following lemma. 
Lemma 5.2: The sum of a finite number (~f independent Poisson random variables is 
a Poisson random variable. 
Proof: We consider two independent Poisson random variables X and Y with means 
fJ., I and fJ.,2; the case of more random variables is simply handled by induction. Now 
) 
Pr (X + Y = j) = L Pr ( (X = k) n (Y = j - k)) 
k=() 
In the last equality we used the binomial theorem to simplify the summation. 
• 
We can also prove Lemma 5.2 using moment generating functions. 
96 

5.3 THE POISSON DISTRIBUTION 
Lemma 5.3: The moment generating function of a Poisson random variable with pa-
rameter f.1., is 
Proof: For any t, 
Given two independent Poisson random variables X and Y with means f.1.,1 and f.1.,2, we 
apply Theorem 4.3 to prove 
Mx+y(t) = Mx(t) . My(t) = e(1 1l+1 12)(e
1
-1). 
which is the moment generating function of a Poisson random variable with mean 
f.1.,1 + f.1.,2· By Theorem 4.2, the moment generating function uniquely defines the dis-
tribution, and hence the sum X + Y is a Poisson random variable with mean f.1., I + f.1.,2. 
Next we develop a Chernoff bound for Poisson random variables that we will use 
later in this chapter. 
Theorem 5.4: Let X be a Poisson random variable }vith parameter ~l. 
1. rrx > f.1." then 
2. rr x < f.1.,. then 
Proof: For any t > 0 and x > f.1." 
Plugging in the expression for the moment generating function of the Poisson distribu-
tion, we have 
Choosing t = In(x/f.1.,) > 0 gives 
Pr(X ::::: x) .::: ex-p-xln(X/It} 
XX 
For any t < 0 and x < f.1." 
Hence 
E[e tx ] 
Pr(X < x) = Pr(e tX > etX ) < --.-. 
-
-
-
e'-\ 
97 

BALLS, BINS, AND RANDOM GRAPHS 
Choosing t = In(xlfl) < 0, it follows that 
Pr(X ::::; x) ::::; eX-I1-xln(x/l1) 
e-I'(efl)-' 
x'\ 
5.3.1. Limit of the Binomial Distribution 
• 
We have shown that, when throwing m balls randomly into b bins, the probability p,. 
that a bin has r balls is approximately the Poisson distribution with mean ml b. In gen-
eral, the Poisson distribution is the limit distribution of the binomial distribution with 
parameters nand p, when n is large and p is small. More precisely, we have the fol-
lowing limit result. 
Theorem 5.5: Let XII be a binomial random variable ~vith parameters nand p, where 
pis a function (~fn and lim ll -+ x np = A is a constant that is independent ofn. Then, 
for anyfixed k, 
This theorem directly applies to the balls-and-bins scenario. Consider the situation 
where there are m balls and b bins, where m is a function of band lim 11-+ X mlb = A. 
Let XIl be the number of balls in a specific bin. Then Xn is a binomial random variable 
with parameters m and II b. Theorem 5.5 thus applies and says that 
e- lIl / n (min) r 
lim Pr(X/l = r) = -----
r! 
matching the approximation of Eqn. (5.2). 
Before proving Theorem 5.5, we describe some of its applications. Distributions of 
this type arise frequently and are often modeled by Poisson distributions. For exam-
ple, consider the number of spelling or grammatical mistakes in a book, including this 
book. One model for such mistakes is that each word is likely to have an error with 
some very small probability p. The number of errors is then a binomial random vari-
able with large n and small p that can therefore be treated as a Poisson random variable. 
As another example, consider the number of chocolate chips inside a chocolate chip 
cookie. One possible model is to split the volume of the cookie into a large number of 
small disjoint compartments, so that a chip lands in each compartment with some prob-
ability p. With this model, the number of chips in a cookie roughly follows a Poisson 
distribution. We will see similar applications of the Poisson distribution in continuous 
'cttings in Chapter 8. 
Proof of Theorem 5.5: We can write 
Pr(X" = k) = G 
)pk(l - p)n- k 
98 

5.4 THE POISSON APPROXIMATION 
In what follows, we make use of the bound that, for Ixl :s I, 
eX 0 -
X 2) ~ I + x ~ e\ 
(5.3) 
which follows from the Taylor series expansion of eX. (This is left as Exercise 5.6.) 
Then 
Pr (X = k) < n k p k (1 -
p) II 
n 
-
k! 
(1 _ p)k 
(np)k e-PII 
<-----
k! 
1- pk 
e-pII(npl 
k! 
1- pk 
The second line follows from the first by Eqn. (5.3) and the fact that (1 - p)k 2: 1 - pk 
for k ~ O. Also, 
(n - k + l)k 
Pr (X n = k) 2: 
p k (1 _ p) II 
k! 
> ((n - k + 1) p)k e-PII(l _ p'l)1I 
k! 
e- PI1 ((n - k + l)p)k 
, 
> 
(l-Irn). 
k! 
where in the second inequality we applied Eqn. (5.3) with x = - p. 
Combining, we have 
e-pn(np)k 
I 
e-pn((n - k + I)p)k 
, 
< Pr (X n = k) < 
(I - Ir n ). 
k! 
1 - pk -
-
k! 
In the limit, as n approaches infinity, p approaches zero because the limiting value of 
pn is the constant A. Hence I/O - pk) approaches 1, 1 -
p 2n approaches 1. and the 
difference between (n - k + 1) p and np approaches O. It follows that 
. 
e-pn(np)k 
e-A Ak 
hm 
n----*oo 
k! 
1 - pk 
k! 
and 
e - pl1 ( (n - k + 1) p) k 
2 
e - A A k 
lim 
0 - p n) = --. 
11----*00 
k! 
k! 
Since limn----*oo Pre XII = k) lies between these two values, the theorem follows. 
• 
5.4. The Poisson Approximation 
The main difficulty in analyzing balls-and-bins problems is handling the dependencies 
that naturally arise in such systems. For example, if we throw m balls into n bins and 
find that bin I is empty, then it is less likely that bin 2 is empty because we know that 
the m balls must now be distributed among n -
1 bins. More concretely: if we know 
99 

BALLS, BINS, AND RANDOM GRAPHS 
the number of balls in the first n -
1 bins, then the number of balls in the last bin is 
completely determined. The loads of the various bins are not independent, and inde-
pendent random variables are generally much easier to analyze, since we can apply 
Chernoff bounds. It is therefore useful to have a general way to circumvent these sorts 
of dependencies. 
We have already shown that, after throwing m balls independently and uniformly at 
random into n bins, the distribution of the number of balls in a given bin is approxi-
mately Poisson with mean m/n. We would like to say that the joint distribution of the 
number of balls in all the bins is well approximated by assuming the load at each bin is 
an independent Poisson random variable with mean m/n. This would allow us to treat 
bin loads as independent random variables. We show here that we can do this when 
we are concerned with sufficiently rare events. Specifically, we show in Corollary 5.9 
that taking the probability of an event using this Poisson approximation for all of the 
bins and multiplying it by ey7ii gives an upper bound for the probability of the event 
when m balls are thrown into n bins. For rare events, this extra ey7ii factor will not be 
significant. To achieve this result, we now introduce some technical machinery. 
Suppose that m balls are thrown into n bins independently and uniformly at random, 
and let X/Ill) be the number of balls in the ith bin, where I :s i :s n. Let Yi
lll
), ••• , Y/;II1) 
be independent Poisson random variables with mean m/n. We derive a useful relation-
ship between these two sets of random variables. Tighter bounds for specific problems 
can often be obtained with more detailed analysis, but this approach is quite general 
and easy to apply. 
The difference between throwing m balls randomly and assigning each bin a num-
ber of balls that is Poisson distributed with mean m/n is that, in the first case, we 
know there are m balls in total, whereas in the second case we know only that m is the 
expected number of balls in all of the bins. But suppose when we use the Poisson dis-
tribution we end up with m balls. In this case, we do indeed have that the distribution 
is the same as if we threw m balls into n bins randomly. 
Theorem 5.6: The distribution (d (Y1( Ill) • •••• Y/~ /11)) conditioned on L i r:(lII) = k is the 
x u:) 
Xu:) 
dl 
f' I 
I 
..r 
same as ( 
1 
, ••• , 
II 
), regar, ess () t U:' Wi lie OJ m. 
Proof: When throwing k balls into n bins, the probability that (Xi k), ... , X/;k)) 
(k l , ... , k n ) for any kl , ... , kll satisfying Li ki = k is given by 
(kl:k/ .. :k lJ ) 
n k 
k! 
N ow, for any k I, ... , k II with L i k i = k, consider the probability that 
(y (/II) 
y(IIl)) -
(k 
k ) 
1 
, ... , 
n 
-
I,"" 
II 
100 

5.4 THE POISSON APPROXIMATION 
(
II) 
(III) 
(m) 
_ 
(111)_ 
Pr (Y, 
, ... , Yn 
) -
(k" ... , k ll ) 
I L ~ - k 
1=' 
Pr((y,(/II) = k,) n (y,(I/)) = k2 ) n·.· n (Y,~m) = kll )) 
Pr( 2:;'=, ~(Ill) = k) 
The probability that ~(I1l) = ki is e-m/lI(mln)A.'lk/!. since the ~(m) are independent 
Poisson random variables with mean min. Also. by Lemma 5.2, the sum of the y(m) is 
., 
1 
itself a Poisson random variable with mean m. Hence 
Pr((Y,(III) = k,) n (Yi"
l
) = k2 ) n··· n (Y,~"I) = kll)) 
Pr(2:~I=, 0(m) = k) 
proving the theorem. 
n 
~'=, e - III / II ( min) kif k i ! 
e- lI1 m klk! 
k! 
• 
With this relationship between the two distributions. we can proye strong results about 
any function on the loads of the bins. 
Theorem 5.7: Let f(x" ... , XII) be a nonnegative function. Then 
(5.4) 
Proof: We have that 
E[f(Y,'m', "', Y,;m')] = f E[J(y"m', "', Y,;m') I i);t'n, = k] pr( t Y,"'" = k ) 
k=O 
1=' 
1=' 
2: E[f( Y,'m" ... , Y,;m') I t Y,"'" = m ] pr( t Y,"'" = 111 ) 
1=' 
1=' 
where the last equality follows from the fact that the joint distribution of the ~(III) given 
,,",II 
y(lll) 
. 
I h 
f h 
X(III) 
h 
. Th 
5 6 S' 
,,",II 
yllll) 
L....i=' 
i 
= m IS exact y t at 0 t e 
i 
,as sown 10 
eorem.. 10ce L....i=' 
i 
is Poisson distributed with mean m, we now have 
We use the following loose bound on m!, which we prove as Lemma 5.8: 
( 
)
111 
m! < e..;m : 
This yields 
101 

BALLS, BINS, AND RANDOM GRAPHS 
E[f(Yi m), ... , Y,;m))] :::: E[f(X,(m), ... , x~m))] 
l~, 
eym 
and the theorem is proven. 
• 
We prove the upper bound we used for factorials, which closely matches the loose lower 
bound we used in Lemma 5.1. 
LemmaS.S: 
Proof: We use the fact that 
We first claim that, for i :::: 2, 
n! <s cv'n( ~ r 
II 
In(n!) = LIn i. 
i=1 
I
i 
In (i -
I) + In i 
i-I lnx dx :::: 
2 
(5.5) 
This follows from the fact that lnx is concave, since its second derivative is -ljx 2, 
which is always negative. Therefore, 
or, equivalently, 
(II 
/I 
1 
11 
In x dx :::: Lin i _ ~ n 
I 
i=1 
... 
In n 
n In n -
11 + I > In (n!) - -. 
-
2 
The result now follows simply by exponentiating. 
• 
Theorem 5.7 holds for any nonnegative function on the number of balls in the bins. In 
particular, if the function is the indicator function that is 1 if some event occurs and 0 
otherwise, then the theorem gives bounds on the probability of events. Let us call the 
scenario in which the number of balls in the bins are taken to be independent Poisson 
random variables with mean mjn the Poisson case, and the scenario where m balls are 
thrown into n bins independently and uniformly at random the exact case. 
Corollary S.9: Any event that takes place with probability p in the Poisson case takes 
place with probability at most pe.jiii in the exact case. 
Proof: Let f be the indicator function of the event. In this case, E [f] is just the proba-
bility that the event occurs, and the result follows immediately from Theorem 5.7. 
• 
This is a quite powerful result. It says that any event that happens with small proba-
bility in the Poisson case also happens with small probability in the exact case, where 
balls are thrown into bins. Since in the analysis of algorithms we often want to show 
that certain events happen with small probability, this result says that we can utilize an 
102 

5.4 THE POISSON APPROXIMATION 
analysis of the Poisson approximation to obtain a bound for the exact case. The Pois-
son approximation is easier to analyze because the numbers of balls in each bin are 
independent random variables. 1 
We can actually do even a little bit better in many natural cases. The proof of the 
following theorem is outlined in Exercise 5.14. 
Theorem 5.10: Let f(XI, ... ,xn) be a nonnegative function such that E[f(Xi ml, ... , 
x~ml)] is either monotonically increasing or monotonically decreasing in m. Then 
(5.6) 
The following corollary is immediate. 
Corollary 5.11: Let £ be an event whose probability is either monotonically increas-
ing or monotonically decreasing in the number (4 balls. {f £ has probability p in the 
Poisson case, then £ has probability at most 2 p in the exact case. 
To demonstrate the utility of this corollary, we again consider the maximum load prob-
lem for the case m = n. We have shown via a union bound argument that the maximum 
load is at most 31nnlln Inn with high probability. Using the Poisson approximation, 
we prove the following almost-matching lower bound on the maximum load. 
Lemma 5.12: When n balls are thrown independent(-\! and uniformly at random into 
n bins, the maximum load is at least In n I In In n with probability at least I - II Jl for n 
sufficiently large. 
Proof· In the Poisson case, the probability that bin 1 has load at least M = In Jl I In In n 
is at least I/eM!, which is the probability it has load exactly M. In the Poisson case, 
all bins are independent, so the probability that no bin has load at least M is at most 
(1 __ I_)n < e-n/(eM'l. 
eM! 
-
We now need to choose M so that e- II /(eM'l :s n-2, for then (by Theorem 5.7) we will 
have that the probability that the maximum load is not at least M in the exact case is at 
most eJnln 2 < lin. This will give the lemma. Because the maximum load is clearly 
monotonically increasing in the number of balls, we could also apply the slightly better 
Theorem 5.10, but this would not affect the argument substantially. 
It therefore suffices to show that M! :s nl2elnn, or equivalently that InM! < 
In n - In In n - In(2e). From our bound (5.5), it follows that 
I There are other ways to handle the dependencies in the balls-and-bins model. In Chapter 12 we describe a more 
general way to deal with dependencies (using martingales) that applies here. Also. there is a theory of negative 
dependence that applies to balls-and-bins problems that also allows these dependencies to be dealt with nicely. 
103 

BALLS, BINS, AND RANDOM GRAPHS 
when n (and hence M = In n / In In n) are suitably large. Hence, for n suitably large, 
In M! :s Min M - M + In M 
In n 
Inn 
= -- (In In n - In In In n) - -- + (In In n - In In In n) 
In Inn 
In Inn 
Inn 
< Inn---
-
Inlnn 
:s In n -
In In n - In(2e), 
where in the last two inequalities we have used the fact that In In n = o( In n / In In n) . • 
5.4.1.* Example: Coupon Collector's Problem, Revisited 
The coupon collector's problem introduced in Section 2.4.1 can be thought of as a balls-
and-bins problem. Recall that in this problem there are n different types of coupons, 
each cereal box yields a coupon chosen independently and uniformly at random from 
the n types, and you need to buy cereal boxes until you collect one of each coupon. If 
we think of coupons as bins and cereal boxes as balls, the question becomes: If balls are 
thrown independently and uniformly at random into bins, how many balls are thrown 
until all bins have at least one ball? We showed in Section 2.4.1 that the expected num-
ber of cereal boxes necessary is nH(n) ~ n Inn; in Section 3.3.1 we showed that, if 
there are n In n + en cereal boxes, then the probability that not all coupons are collected 
is at most e-c. These results translate immediately to the balls-and-bins setting. The 
expected number of balls that must be thrown before each bin has at least one ball is 
nH(n), and when nlnn + en balls are thrown the probability that not all bins have at 
least one ball is e- c. 
We have seen in Chapter 4 that Chernoff bounds yield concentration results for sums 
of independent 0-1 random variables. We will use here a Chernoff bound for the Pois-
son distribution to obtain much stronger results for the coupon collector's problem. 
Theorem 5.13: Let X he the number (~f COUPOIlS observed before obtaining one of eaeh 
of n types (d coupons. Then, for any COllstant c, 
lim Pr[X > Jl In n + en] = 1 - e-e-'. 
1/--+ ~ 
This theorem states that. for large n, the number of coupons required should be very 
close to n In n. For example, over 989c of the time the number of coupons required lies 
between n In n - 4n and 11 In n + 4n. This is an example of a sharp threshold, where 
the random variable is closely concentrated around its mean. 
Proof: We look at the problem as a balls-and-bins problem. We begin by consider-
ing the Poisson approximation, and then demonstrate that the Poisson approximation 
gives the correct answer in the limit. For the Poisson approximation, we suppose that 
the number of balls in each bin is a Poisson random variable with mean In n + e, so that 
the expected total number of balls is m = n In n + en. The probability that a specific 
bin is empty is then 
104 

5.4 THE POISSON APPROXIMATION 
e-(lnll+cj = C 
Jl 
Since all bins are independent under the Poisson approximation, the probability that 
no bin is empty is 
The last approximation is appropriate in the limit as II grows large, so we apply it here. 
To show the Poisson approximation is accurate. we undertake the following steps. 
Consider the experiment where each bin has a Poisson number of balls, each with mean 
In n +c. Let E be the event that no bin is empty, and let X be the number of balls thrown. 
We have seen that 
lim Pr(E) = e-e 
/1-+')(, 
We use Pr(E) by splitting it as follows: 
Pr(E) = Pr(E n (IX - ml ~ J2m Inm)) + Pr(E n (IX - 1111> J2111Inm)) 
=Pr(E IIX-ml ~ J2mlnm) .Pr(IX-1I11 ~ J21111nll1) 
+ Pr(E IIX - ml > J2m Inm). Pr(IX -
1111 > J2111Inm). 
(5.7) 
This representation proves helpful once we establish two facts. Fir"t. \\e shO\\' that 
Pr(IX - ml > J2m In 111 ) is 0(1); that is, the probability that in the Poi"soll case the 
number of balls thrown deviates signi ficantly from its mean JJ1 is o( 1 ). Thi" guarantees 
that the second term in the summation on the right of Eqn. (5.7) is o( I). Second. \\e 
show that 
IPr(E IIX - ml ~ J2m In 111 ) - Pr(E I X = m)1 = 0(1). 
That is, the difference between our experiment coming up with exactly 111 balls or just 
almost m balls makes an asymptotically negligible difference in the probability that 
every bin has a ball. With these two facts, Eqn. (5.7) becomes 
Pr(E) = Pr(E IIX -
1111 ~ J2m Inm) . Pr(IX - ml ~ )2111 In 111 ) 
+Pr(E IIX-ml > J2mlnm) .Pr(IX-ml > J21111nll1) 
= Pr(E I IX - ml ~ J2m Inm) . (1 - o(l)) + o(l) 
= Pr (E I X = m) (I -
0 (1)) + 0 (1 ), 
and hence 
lim Pr(E) = lim Pr(E I X = m). 
/I -+ :x. 
/I --+ :x. 
But from Theorem 5.6, the quantity on the right is equal to the probability that every 
bin has at least one ball when m balls are thrown randomly, since conditioning on m 
total balls with the Poisson approximation is equivalent to throwing 111 balls randomly 
into the n bins. As a result, the theorem follows once we have shown these two facts. 
To show that Pr(IX - ml > J2m Inm) is 0(1), consider that X is a Poisson ran-
dom variable with mean m, since it is a sum of independent Poisson random variables. 
105 

BALLS, BINS, AND RANDOM GRAPHS 
We use the Chernoff bound for the Poisson distribution (Theorem 5.4) to bound this 
probability, writing the bound as 
Pr(X 2: x) :s eX-ill-X In(x/m). 
For x = m + J2m In m, we use that In(l + z) 2: z - z2/2 for z 2: 0 to show 
Pr( X > m + J 2m In m ) :s e J2m In 111- (m+J2mlnm ) In(I+J21nm/m ) 
:s eJ2mTtlm-(III+J2mlnm )(J21nm/m-lnm/m) 
= e-lnlll+J2mlnlll(lnm/m) = 0(1). 
A similar argument holds if x < m, so Pr(IX - ml > J2m Inm) = 0(1). 
We now show the second fact, that 
IPr(E I IX - ml :s J2m Inm) - Pr(E I X = m)1 = 0(1). 
Note that Pr(E I X = k) is increasing in k, since this probability corresponds to the 
probability that all bins are nonempty when k balls are thrown independently and uni-
formly at random. The more balls that are thrown, the more likely all bins are nonempty. 
It follows that 
Pr(E I X = m - J2mlnm):s Pr(E IIX -ml:S J2mlnm) 
:s Pr(E I X = m + J2m Inm). 
Hence we have the bound 
IPr(E I IX - ml :s J2m Inm) - Pr(E I X = m)1 
:s Pr(E I X = m + J2m Inm) - Pr(E I X = m - J2m Inm), 
and we show the right-hand side is 0(1). This is the difference between the probability 
that all bins receive at least one ball when 111 - J 2111 In 111 balls are thrown and when 
m + J2m In m balls are thrown. This difference is equivalent to the probability of 
the following experiment: we throw 111 -
J211l In m balls and there is still at least one 
empty bin, but after throwing an additional 2../2m In m balls, all bins are nonempty. 
In order for this to happen, there must be at least one empty bin after m - J2m In m 
balls; the probability that one of the next 2J2m In m balls covers this bin is at most 
(2J2m In m )/n = 0(1) by the union bound. Hence this difference is 0(1) as well. 
• 
5.5. Application: Hashing 
5.5.1. Chain Hashing 
The balls-and-bins-model is also useful for modeling hashing. For example, consider 
the application of a password checker, which prevents people from using common, eas-
ily cracked passwords by keeping a dictionary of unacceptable passwords. When a user 
tries to set up a password, the application would like to check if the requested pass-
word is part of the unacceptable set. One possible approach for a password checker 
would be to store the unacceptable passwords alphabetically and do a binary search on 
106 

5.5 APPLICATION: HASHING 
the dictionary to check if a proposed password is unacceptable. A binary search would 
require G (log m) time for m words. 
Another possibility is to place the words into bins and then search the appropriate 
bin for the word. The words in a bin would be represented by a linked list. The place-
ment of words into bins is accomplished by using a hash function. A hash function f 
from a universe U into a range [0, n -
I] can be thought of as a way of placing items 
from the universe into n bins. Here the universe U would consist of possible password 
strings. The collection of bins is called a hash table. This approach to hashing is called 
chain hashing, since items that fall in the same bin are chained together in a linked list. 
Using a hash table turns the dictionary problem into a balls-and-bins problem. If our 
dictionary of unacceptable passwords consists of JJ1 words and the range of the hash 
function is [0, n - 1], then we can model the distribution of words in bins with the same 
distribution as m balls placed randomly in n bins. We are making a rather strong as-
sumption by presuming that our hash function maps words into bins in a fashion that 
appears random, so that the location of each word is independent and identically dis-
tributed. There is a great deal of theory behind designing hash functions that appear 
random, and we will not delve into that theory here. We simply model the problem by 
assuming that hash functions are random. In other words, we assume that (a) for each 
x E U, the probability that f(x) = j is I/n (for ° :s j :s n -
1) and that (b) the values 
of f(x) for each x are independent of each other. Notice that this does not mean that 
every evaluation of f(x) yields a different random answer! The value of fei) is fixed 
for all time; it is just equally likely to take on any value in the range. 
Let us consider the search time when there are n bins and m words. To search for an 
item, we first hash it to find the bin that it lies in and then search sequentially through the 
linked list for it. If we search for a word that is not in our dictionary, the expected num-
ber of words in the bin the word hashes to is m/n. If we search for a word that is in our 
dictionary, the expected number of other words in that word's bin is (JJ1 -I )/11, so the ex-
pected number of words in the bin is 1 + (m -l)/n. Ifwe choose n = III bins for our hash 
table, then the expected number of words we must search through in a bin is constant. If 
the hashing takes constant time, then the total expected time for the search is constant. 
The maximum time to search for a word, however, is proportional to the maximum 
number of words in a bin. We have shown that when n = m this maximum load is 
GOnn/ln Inn) with probability close to 1, and hence with high probability this is the 
maximum search time in such a hash table. While this is still faster than the required 
time for standard binary search, it is much slower than the average, which can be a 
drawback for many applications. 
Another drawback of chain hashing can be wasted space. If we use II bins for n 
items, several of the bins will be empty, potentially leading to wasted space. The space 
wasted can be traded off against the search time by making the average number of 
words per bin larger than 1. 
5.5.2. Hashing: Bit Strings 
If we want to save space instead of time, we can use hashing in another way. Again, 
we consider the problem of keeping a dictionary of unsuitable passwords. Assume that 
107 

BALLS, BINS, AND RANDOM GRAPHS 
a password is restricted to be eight ASCII characters, which requires 64 bits (8 bytes) 
to represent. Suppose we use a hash function to map each word into a 32-bit string. 
This string will serve as a short fingerprint for the word; just as a fingerprint is a suc-
cinct way of identifying people, the fingerprint string is a succinct way of identifying 
a word. We keep the fingerprints in a sorted list. To check if a proposed password is 
unacceptable, we calculate its fingerprint and look for it on the list, say by a binary 
search.2 If the fingerprint is on the list, we declare the password unacceptable. 
In this case, our password checker may not give the correct answer! It is possible 
for a user to input an acceptable password, only to have it rejected because its finger-
print matches the fingerprint of an unacceptable password. Hence there is some chance 
that hashing will yield afalse positive: it may falsely declare a match when there is 
not an actual match. The problem is that - unlike fingerprints for human beings - our 
fingerprints do not uniquely identify the associated word. This is the only type of mis-
take this algorithm can make; it does not allow a password that is in the dictionary of 
unsuitable passwords. In the password application, allowing false positives means our 
algorithm is overly conservative, which is probably acceptable. Letting easily cracked 
passwords through, however, would probably not be acceptable. 
To place the problem in a more general context. we describe it as an approximate 
set membership problem. Suppose we have a set 5 = {SI,S2, .. "SIII} of m elements 
from a large universe U. We would like to represent the elements in such a way that we 
can quickly answer queries of the form "Is x an element of 57" We would also like the 
representation to take as little space as possible. In order to save space, we would be 
willing to allow occasional mistakes in the form of false positives. Here the unallow-
able passwords correspond to our set S. 
How large should the range of the hash function used to create the fingerprints be? 
Specifically, if we are working with bits, how many bits should we use to create a fin-
gerprint? Obviously, we want to choose the number of bits that gives an acceptable 
probability for a false positive match. The probability that an acceptable password has a 
fingerprint that is different from any specific unallowable password in 5 is (l-1/2 h). It 
follows that if the set 5 has size m and if we use b bits for the fingerprint, then the prob-
ability of a false positive for an acceptable password is I -
(1 - l/iTIi ~ I - e- Ill
/ 2". 
If we want this probability of a false positive to be less than a constant c, we need 
which implies that 
m 
h> log') 
. 
-
'-- In(l/O - c)) 
That is, we need b = Q (log~ 111) bits. On the other hand, if we use b = 210g 2 m bits, 
then the probability of a false positive falls to 
( 
1 )"1 
I 
1-
1- -') 
<-. 
171
4 
111 
In this case the fingerprints will be uniformly distributed over all 32-bit strings. There are faster algorithms 
for searching over sets of number.~ \\ ith this distribution. just as Bucket sort allows faster sorting than stan-
dard comparison-based sorting when the elements to he sorted are from a uniform distribution. but we will not 
concern ourselves with this point here. 
108 

5.5 APPLICATION: HASHING 
Start with an array of Os. 
1010101010101010101010101 
Each element of 5 is hashed k times; each 
hash gives an array location to set to 1. 
XI 
X 1 
••• 
I 0 I 0 II~ oi61I~fID) I 0 I 
To check if y is in 5, check the k hash 
locations. If a 0 appears, y is not in S. 
v 
loloilioililill:ilirol 
1 0 1 
If only 1 s appear, conclude that y is in S. 
This may yield false positives. 
\' 
I 0 I 0 111ITG frogl I 0 II I 0 I 
Figure 5.1: Example of how a Bloom tilter function~. 
In our example, if our dictionary has 2 16 = 65,536 words, then using 32 bits when 
hashing yields a false positive probability of just less than 1/65,536. 
5.5.3. Bloom Filters 
We can generalize the hashing ideas of Sections 5.5.1 and 5.5.2 to achieve more in-
teresting trade-offs between the space required and the false positi\'e probability. The 
resulting data structure for the approximate set membership problem is called a Bloom 
filter. 
A Bloom filter consists of an array of Jl bits, A[O] to A[n -
I]. initially all set 
to 0. A Bloom filter uses k independent random hash functions 11], .... Ilk with range 
{a, ... , n - I}. We make the usual assumption for analysis that these hash functions map 
each element in the universe to a random number uniformly over the range {O, ... . 11 -I}. 
Suppose that we use a Bloom filter to represent a set 5 = {s], S2, ... , S/I/} of JJ1 elements 
from a large universe U. For each element S E 5, the bits A [h i (s) J are set to I for I :s i :s 
k. A bit location can be set to I multiple times, but only the first change has an effect. 
To check ifan element x is in 5, we check whether all array locations A[hi(x)] for 1 :s 
i :s k are set to 1. Ifnot, then clearly x is not a member of 5, because if x were in 5 then 
all locations A[h;(x)] for 1 :s i :s k would be set to I by construction. If all A[h;(x)] 
are set to L we assume that x is in 5, although we could be wrong. We would be wrong 
if all of the positions A[h;(x)J were set to I by elements of 5 even though x is not in 
the set. Hence Bloom filters may yield false positives. Figure 5.1 shows an example. 
109 

BALLS, BINS, AND RANDOM GRAPHS 
The probability of a false positive for an element not in the set - the false positive 
probability - can be calculated in a straightforward fashion, given our assumption that 
the hash functions are random. After all the elements of 5 are hashed into the Bloom 
filter, the probability that a specific bit is still 0 is 
( 
1 )klll 
I - -
:::::::; e-klll / ll • 
n 
We let p = e-km/n . To simplify the analysis, let us temporarily assume that a fraction 
p of the entries are still 0 after all of the elements of 5 are hashed into the Bloom filter. 
The probability of a false positive is then 
( 
( 
1 )kll1)k 
I -
I -
-;; 
:::::::; (I -
e-kll//lI)k = (I - pl. 
We let f = (1- e-km/
l1 )k = (1- p)k. From now on. for convenience we use the asymp-
totic approximations p and f to represent (respectively) the probability that a bit in the 
Bloom filter is 0 and the probability of a false positive. 
Suppose that we are given m and 11 and wish to optimize the number of hash func-
tions k in order to minimize the false positive probability f. There are two competing 
forces: using more hash functions gives us more chances to find a O-bit for an element 
that is not a member of 5, but using fewer hash functions increases the fraction of O-bits 
in the array. The optimal number of hash functions that minimizes f as a function of 
k is easily found taking the derivative. Let g = k In(l - e-km/Il ), so that f = et; and 
minimizing the false positive probability f is equivalent to minimizing g with respect 
to k. We find 
d f? 
.. 
kill 
e-km/Il 
~=In(l-e-bl/I/)+-
. 
dk 
11 I - e-km/II 
It is easy to check that the derivative is zero when k = (In 2) . (njm) and that this 
point is a global minimum. In this case the false positive probability f is (lj2)k :::::::; 
(0.6185)I1/m. The false positive probability falls exponentially in njm, the number of 
bits used per item. In practice. of course, k must be an integer, so the best possible 
choice of k may lead to a slightly higher false positive rate. 
A Bloom filter is like a hash table. but instead of storing set items we simply use one 
bit to keep track of whether or not an item hashed to that location. If k = 1, we have 
just one hash function and the Bloom filter is equivalent to a hashing-based fingerprint 
system, where the list of the fingerprints is stored in a 0-1 bit array. Thus Bloom fil-
ters can be seen as a generalization of the idea of hashing-based fingerprints. As we 
saw when using fingerprints, to get even a small constant probability of a false positive 
required Q (log m) fingerprint bits per item. In many practical applications, Q (log m) 
bits per item can be too many. Bloom filters allow a constant probability of a false pos-
itive while keeping n jm, the number of bits of storage required per item, constant. For 
many applications, the small space requirements make a constant probability of error 
acceptable. For example, in the password application, we may be willing to accept 
false positive rates of I % or 2%. 
110 

5.5 APPLICATION: HASHING 
Bloom filters are highly effective even if n = em for a small constant c, such as 
c = 8. In this case, when k = 5 or k = 6 the false positive probability is just over 0.02. 
This contrasts with the approach of hashing each element into 0) (log m) bits. Bloom 
filters require significantly fewer bits while still achieving a very good false positive 
probability. 
It is also interesting to frame the optimization another way. Consider f, the proba-
bility of a false positive, as a function of p. We find 
= (1 -
p) ( -In I'll/II II) 
(5.8) 
From the symmetry of this expression, it is easy to check that p = 1/2 minimizes the 
false positive probability f. Hence the optimal results are achieved when each bit of the 
Bloom filter is 0 with probability 1/2. An optimized Bloom filter looks like a random 
bit string. 
To conclude, we reconsider our assumption that the fraction of entries that are still 0 
after all of the elements of 5 are hashed into the Bloom filter is p. Each bit in the array 
can be thought of as a bin, and hashing an item is like throwing a ball. The fraction of 
entries that are still 0 after all of the elements of 5 are hashed is therefore equivalent to 
the fraction of empty bins after mk balls are thrown into II bins. Let X be the number 
of such bins when mk balls are thrown. The expected fraction of such hins is 
I 
( 
1 )bl1 
P = 1--
11 
The events of different bins being empty are not independent. but \\c can apply 
Corollary 5.9, along with the Chernoff bound of Eqn. (4.6), to obtain 
Pr(1 X - np'l 2: 611) :s 2evln e-IIF~nJi'. 
Actually, Corollary 5.11 applies as well, since the number of O-entries - which corre-
sponds to the number of empty bins - is monotonically decreasing in the number of 
balls thrown. The bound tells us that the fraction of empty bins is close to p' (when 
n is reasonably large) and that pi is very close to p. Our assumption that the fraction 
of O-entries in the Bloom filter is p is therefore quite accurate for predicting actual 
performance. 
5.5.4. Breaking Symmetry 
As our last application of hashing, we consider how hashing provides a simple way 
to break symmetry. Suppose that n users want to utilize a resource. such as time on a 
supercomputer. They must use the resource sequentially, one at a time. Of course, each 
user wants to be scheduled as early as possible. How can we decide a permutation of 
the users quickly and fairly? 
If each user has an identifying name or number, hashing provides one possible so-
lution. Hash each user's identifier into 2b bits, and then take the permutation given by 
111 

BALLS, BINS, AND RANDOM GRAPHS 
the sorted order of the resulting numbers. That is, the user whose identifier gives the 
smallest number when hashed comes first, and so on. For this approach to work, we 
do not want two users to hash to the same value, since then we must decide again how 
to order these users. 
If b is sufficiently large, then with high probability the users will all obtain distinct 
hash values. One can analyze the probability that two hash values collide by using the 
analysis from Section 5.1 for the birthday paradox; hash values correspond to birth-
days. We here use a simpler analysis similar to that used for analyzing fingerprints in 
Section 5.5.2. Consider the point of view of one user. The probability that some other 
user obtains the same hash value is 
( 
1 )11-1 
II -
I 
1 -
1- -
< --. 
2h 
-
217 
By the union bound, the probability that any user has the same hash value as another is 
at most n (n - 1)/2h . Hence, choosing b = 310g 2 II guarantees success with probabil-
ity at least I - lin. 
This solution is extremely flexible, making it useful for many situations in distrib-
uted computing. For example, new users can easily be added into the schedule at any 
time, as long as they do not hash to the same number as another scheduled user. 
A related problem is leader election. Suppose that instead of trying to order all of 
the users, we simply want to fairly choose a leader from them. Again, if we have a suit-
ably random hash function then we can simply take the user whose hash value is the 
smallest. An analysis of this scheme is left as Exercise 5.25. 
5.6. Random Graphs 
5.6.1. Random Graph Models 
There are many NP-hard computational problems defined on graphs: Hamiltonian cycle, 
independent set, vertex cover, and so forth. One question worth asking is whether these 
problems are hard for most inputs or just for a relatively small fraction of all graphs. 
Random graph models provide a probabilistic setting for studying such questions. 
Most of the work on random graphs has focused on two closely related models, GII • fJ 
and Gil. N. In Gil. ji we consider all undirected graphs on n distinct vertices VI, V2, ... , Vn · 
A graph with a given set of m edges has probability 
plll(1 _ p)C)-m. 
One way to generate a random graph in GII . p is to consider each of the (~) possible 
edges in some order and then independently add each edge to the graph with probabil-
ity p. The expected number of edges in the graph is therefore (~)p, and each vertex 
has expected degree (n -
1) p. 
In the Gil, N model, we consider all undirected graphs on n vertices with exactly N 
edges. There are ((~») possible graphs, each selected with equal probability. One way 
to generate a graph uniformly from the graphs in G n, N is to start with a graph with no 
edges. Choose one of the (~) possible edges uniformly at random and add it to the edges 
112 

5.6 RANDOM GRAPHS 
in the graph. Now choose one of the remaining (~) - 1 possible edges independently 
and uniformly at random and add it to the graph. Similarly, continue choosing one of 
the remaining un chosen edges independently and uniformly at random until there are 
N edges. 
The Gn,p and GII,N models are related; when p = NI(~), the number of edges in a 
random graph in Gn,p is concentrated around N, and conditioned on a graph from G II .p 
having N edges, that graph is uniform over all the graphs from GII .N . The relationship 
is similar to the relationship between throwing m balls into n bins and having each bin 
have a Poisson distributed number of balls with mean min. 
Indeed, there are many similarities between random graphs and the balls-and-bins 
models. Throwing edges into the graph as in the GII,,V model is like throwing balls into 
bins. However, since each edge has two endpoints, each edge is like throwing two balls 
at once into two different bins. The pairing defined by the edges adds a rich structure 
that does not exist in the balls-and-bins model. Yet we can often utilize the relation be-
tween the two models to simplify analysis in random graph models. For example, in the 
coupon collector's problem we found that when we throw II In II + ell balls, the proba-
bility that there are any empty bins converges to e-e-( as II grows to infinity. Similarly, 
we have the following theorem for random graphs, which is left as Exercise 5.19. 
Theorem 5.14: Let N = 
~ (n In n + en). Then the probabilit,y that there are allY iso-
lated vertices (vertices with degree 0) in Gil. N converges to e-e" as II gro~\'s to illfillit)'. 
5.6.2. Application: Hamiltonian Cycles in Random Graphs 
A Hamiltonian path in a graph is a path that traverses each vertex exactl: once. A 
Hamiltonian cycle is a cycle that traverses each vertex exactly once. We show an inter-
esting connection between random graphs and balls-and-bins problems by analyzing a 
simple and efficient algorithm for finding Hamiltonian cycles in random graphs. The 
algorithm is randomized, and its probabilistic analysis is over both the input distribu-
tion and the random choices of the algorithm. Finding a Hamiltonian cycle in a graph 
is an NP-hard problem. However, our analysis of this algorithm shows that finding a 
Hamiltonian cycle is not hard for suitably randomly selected graphs. even though it 
may be hard to solve in general. 
Our algorithm will make use of a simple operation called a rotatioll. Let G be an 
undirected graph. Suppose that 
is a simple path in G and that (v J.:, Vi) is an edge of G. Then 
is also a simple path, which we refer to as the rotation of P with the rotation edge 
(Vk, vd; see Figure 5.2. 
We first consider a simple, natural algorithm that proves challenging to analyze. We 
assume that our input is presented as a list of adjacent edges for each vertex in the graph, 
with the edges of each list being given in a random order according to independent and 
uniform random permutations. Initially, the algorithm chooses an arbitrary vertex to 
113 

BALLS, BINS, AND RANDOM GRAPHS 
Figure 5.2: The rotation of the path VI, V2, V3, V .. , 1'5, 1'6 with the edge (v(" V3) yields a new path 
V I, 1':2 , l' 3, V 6, 1'5, 1' ... 
start the path; this is the initial head of the path. The head is always one of the endpoints 
of the path. From this point on, the algorithm either "grows" the path deterministically 
from the head, or rotates the path - as long as there is an adjacent edge remaining on 
the head's list. See Algorithm 5.1. 
The difficulty in analyzing this algorithm is that, once the algorithm views some 
edges in the edge lists, the distribution of the remaining edges is conditioned on the 
edges the algorithm has already seen. We circumvent this difficulty by considering a 
modified algorithm that, though less efficient, avoids this conditioning issue and so is 
easier to analyze for the random graphs we consider. See Algorithm 5.2. Each vertex 
v keeps two lists. The list used-edges( v) contains edges adjacent to v that have been 
used in the course of the algorithm while L' was the head~ initially this list is empty. 
The list unused-edges( v) contains other edges adjacent to v that have not been used. 
We initially analyze the algorithm assuming a specific model for the initial unused-
edges lists. We subsequently relate this model to the GII'Ii model for random graphs. 
Assume that each of the n - I possible edges connected to a vertex v is initially on the 
unused-edges list for vertex v independently with some probability q. We also assume 
these edges are in a random order. One way to think of this is that, before beginning the 
algorithm, we create the unused-edges list for each vertex v by inserting each possible 
edge (v, u) with probability q: we think of the corresponding graph G as being the graph 
including all edges that were inserted on some unused-edges list. Notice that this means 
an edge (v, u) could initially be on the unused-edges list for v but not for u. Also, when 
an edge (v, u) is first used in the algorithm. if L' is the head then it is removed just from the 
unused-edges list of l': if the edge is on the unused-edges list for Ll, it remains on this list. 
By choosing the rotation edge from either the used-edges list or the unused-edges 
list with appropriate probabilities and then reversing the path with some small proba-
bility in each step, we modify the rotation process so that the next head of the list is 
chosen uniformly at random from among all vertices of the graph. Once we establish 
this property, the progress of the algorithm can be analyzed through a straightforward 
application of our analysis of the coupon collector's problem. 
The modified algorithm appears wastefuL reversing the path or rotating with one of 
the used edges cannot increase the path length. Also, we may not be taking advantage 
of all the possible edges of G at each step. The advantage of the modified algorithm is 
that it proves easier to analyze, owing to the following lemma. 
Lemma 5.15: Suppose the mod(fied Hamiltonian cycle algorithm is run on a graph 
c'hosen using the described model. Let Vr be the head vertex after the tth step. Then, 
for an.v vertex u. as long as at the tth step there is at least one unused edge available 
at the head vertex, 
114 

5.6 RANDOM GRAPHS 
Hamiltonian Cycle Algorithm: 
Input: A graph G = (V, E) with n vertices. 
Output: A Hamiltonian cycle, or failure. 
1. Start with a random vertex as the head of the path. 
2. Repeat the following steps until the rotation edge closes a Hamiltonian cycle 
or the unused-edges list of the head of the path is empty: 
(a) Let the current path be P = VI, V::. •... , Vk, where Vk is the head, and let 
(v b u) be the first edge in the head's list. 
(b) Remove (Vb u) from the head's list and u's list. 
(c) If u #- Vi for I ::s i ::s k, add u = Vk+ I to the end of the path and make it 
the head. 
(d) Otherwise, if u = Vi, rotate the current path with {VA:, vd and set Vi+1 
to be the head. (This step closes the Hamiltonian path if k = n and the 
chosen edge is (v n , VI ).) 
3. Return a Hamiltonian cycle if one was found or failure if no cycle was found. 
Algorithm 5.1: Hamiltonian cycle algorithm. 
Modified Hamiltonian Cycle Algorithm: 
Input: A graph G = (V, E) with n vertices and associated edge lists. 
Output: A Hamiltonian cycle, or failure. 
1. Start with a random vertex as the head of the path. 
2. Repeat the following steps until the rotation edge closes a Hamiltonian cycle 
or the unused-edges list of the head of the path is empty: 
(a) Let the current path be P = VI. V.> ... , L'k, with Vk being the head. 
(b) Execute i, ii, or iii with probabilities I /n, lused-edges( vd I /n, and 
I - l/n -
I used-edges( L'd I /n. respectively: 
i. Reverse the path, and make L'I the head. 
ii. Choose uniformly at random an edge from used-edges(vd; if the 
edge is (Vb Vi), rotate the current path with (Vb vd and set Vi+1 to be 
the head. (If the edge is (l'k, l'A:-d, then no change is made.) 
iii. Select the first edge from unused-edges(vd, call it (VA:, u). If [{ i- Vi 
for I ::s i ::s k, add [{ = l' A: -t-I to the end of the path and make it the 
head. Otherwise, if II = l'i. rotate the current path with (Vk, Vi) and 
set Vi+1 to be the head. (This step closes the Hamiltonian path if 
k = 11 and the chosen edge is (v n , VI ).) 
(c) Update the used-edges and unused-edges lists appropriately. 
3. Return a Hamiltonian cycle if one was found or failure if no cycle was found. 
Algorithm 5.2: Modified Hamiltonian cycle algorithm. 
115 

BALLS, BINS, AND RANDOM GRAPHS 
Pr (\~ + I = u I \I; = U I, \I; - I = Ll t -I, ... , Vo = u 0) = lin. 
That is, the head vertex can be thought qf as heing chosen umformly at random from 
all vertices at each step, regardless qf the history (~f the process. 
Proof' Consider the possible cases when the path is P = VI, Vi., ... , Vk. 
The only way V I can become the head is if the path is reversed, so \1;+ I = V I with 
probability 1 In. 
If U = Vi+1 is a vertex that lies on the path and (Vb Vi) is in used-edges(vd, then 
the probability that \1;+1 = u is 
I used-edges{ Vk) I -------
n 
lused-edges{ Uk) I 
n 
If [{ is not covered by one of the first two cases then we use the fact that, when 
an edge is chosen from unused-edges{ Vk), the adjacent vertex is uniform over all the 
11 -
I used-edges{ vd I - 1 remaining vertices. This follows from the principle of de-
ferred decisions. Our initial setup required the unused-edges list for Vk to be con-
structed by including each possible edge with probability q and randomizing the order 
of the list. This is equivalent to choosing X neighboring vertices for Vb where X is a 
B{ n -
1, q) random variable and the X vertices are chosen uniformly at random with-
out replacement. Because Vk'S list was determined independently from the lists of the 
other vertices, the history of the algorithm tells us nothing about the remaining edges 
in unused-edges{ vd, and the principle of deferred decisions applies. Hence any edge 
in Vk 's unused-edges list that we have not seen is by construction equally likely to con-
nect to any of the n -Iused-edges(vdl -
I remaining possible neighboring vertices. 
If u = 
Vi+1 is a vertex on the path but (l'~. l'l) is not in used-edges(vd, then 
the probability that \1;+1 = u is the probability that the edge (Vb Vi) is chosen from 
unused-edges( vd as the next rotation edge. which is 
( 
I 
lused-edgeS(l'dl)( 
I 
) 
I 
(5.9) 
1- -;; -
n 
n -lused-edges{vtJl -
I =--;;. 
Finally, if u is not on the path, then the probability that \1;+1 = u is the probability 
that the edge (Vk+l, It) is chosen from unused-edges{vtJ. But this has the same proba-
bility as in Eqn. (5.9). 
• 
For Algorithm 5.2, the problem of finding a Hamiltonian path looks exactly like the 
coupon collector's problem: the probability of finding a new vertex to add to the path 
when there are k vertices left to be added is kin. Once all the vertices are on the 
path, the probability that a cycle is closed in each rotation is lin. Hence, if no list 
of unused-edges is exhausted then we can expect a Hamiltonian path to be formed in 
about O(n In n) rotations, with about another O(n In n) rotations to close the path to 
form a Hamiltonian cycle. More concretely, we can prove the following theorem. 
Theorem 5.16: Suppose the input to the modified Hamiltonian cycle algorithm initially 
has unused-edge lists where each edge (v, u) \-vith u i- V is placed on v's list indepen-
dently with probability q 2: 20 In n In. Then the algorithm succe,\\~fullyfinds a Hamilton-
ian c.'vcle in O(n In n) iterations of the repeat loop (step 2) with probability 1- O(n-I). 
116 

5.6 RANDOM GRAPHS 
Note that we did not assume that the input random graph has a Hamiltonian cycle. A 
corollary of the theorem is that, with high probability, a random graph chosen in this 
way has a Hamiltonian cycle. 
Proof of Theorem 5.16: Consider the following two events. 
[I: The algorithm ran for 311 In 11 steps with no unused-edges list becoming empty, but 
it failed to construct a Hamiltonian cycle. 
[2: At least one unused-edges list became empty during the first 311 In 11 iterations of 
the loop. 
For the algorithm to fail, either event [lor [2 must occur. We first bound the proba-
bility of [I. Lemma 5.15 implies that, as long as there is no empty unused-edges list in 
the first 311 In 11 iterations of step 2 of Algorithm 5.~. in each iteration the next head of 
the path is uniform among the 11 vertices of the graph. To bound [I. we therefore con-
sider the probability that more than 311 In 11 iterations are required to find a Hamiltonian 
cycle when the head is chosen uniformly at random each iteration. 
The probability that the algorithm takes more than ~II In II iterations to find a Hamil-
tonian path is exactly the probability that a coupon collector's problem on II types 
requires more than 211 In 11 coupons. The probability that any specific coupon type has 
not been found among 2n In 11 random coupons is 
( 
I )211111 II 
'I 
I 
1- -
~ e-- nil = -;:;-. 
11 
11-
By the union bound, the probability that any coupon type is not found is at most 1/11. 
In order to complete a Hamiltonian path to a cycle the path must close. which it does 
at each step with probability 1/11. Hence the probability that the path does not become 
a cycle within the next 11 In II iterations is 
Thus we have shown that 
( 
I )1111111 
I 
1- -
~ e- Inll = -. 
n 
n 
2 
Pr([d ~ -. 
n 
Next we bound Pr([2). the probability that an unused-edges list is empty in the first 
3n In n iterations. We consider two subevents as follows. 
[2u: At least 91n 11 edges were removed from the unused-edges list of at least one ver-
tex in the first 311 In n iterations of the loop. 
[217: At least one vertex had fewer than 10 In 11 edges initially in its unused-edges list. 
For [2 to occur, either [2u or [2b must occur. Hence 
Let us first bound Pr([2a)' Exactly one edge is used in each iteration of the loop. 
From the proof of Lemma 5.15 we have that, at each iteration, the probability that a 
117 

BALLS, BINS, AND RANDOM GRAPHS 
given vertex v is the head of the path is l/n, independently at each step. Hence the 
number of times X that v is the head during the first 3n In n steps is a binomial ran-
dom variable B(3n Inn, l/n), and this dominates the number of edges taken from v's 
unused-edges list. 
Using the Chernoff bound of Eqn. (4.1) with 8 = 2 and 11 = 31n n for the binomial 
random variable B(3n Inn, l/n), we have 
Pr(X '" 91n n) <S G~ f"" <s nil' 
By taking a union bound over all vertices, we find Pr(E'2a) :s l/n. 
Next we bound Pr(E'2h). The expected number of edges Y initially in a vertex's 
unused-edgeslistisatleast(n-1)q 2: 20(n-l)lnll/n 2: 191nnforsufficientlylarge 
n. Using Chernoff bounds again (Eqn. (4.5)), the probability that any vertex initially 
has 10 In n edges or fewer on its list is at most 
Pr(Y:S 101nn) :s e-191111119il9).:'j2 :s ~, 
n~ 
and by the union bound the probability that any vertex has too few adjacent edges is at 
most l/n. Thus, 
and hence 
I 
Pr(E'2/J) :s -
n 
2 
Pr(E'::,) :s -. 
n 
In total, the probability that the algorithm fails to find a Hamiltonian cycle in 3n In n 
iterations is bounded by 
• 
We did not make an ett'ort to optimize the constants in the proof. There is, however, a 
clear trade-ott'; with more edges, one could achieve a lower probability of failure. 
We are left with showing how our algorithm can be applied to graphs in Gn,p. We 
show that, as long as p is known. we can partition the edges of the graph into edge lists 
that satisfy the requirements of Theorem 5.16. 
Corollary 5.17: B.v initiali;jng edges on the unused-edges lists appropriately, Algo-
rithm 5.2 will find a Hamiltonian (',vde on a graph chosen randomly from Gn,p with 
probability 1 - O(1/n) whenever p 2: 40 In n /n. 
Proof: We partition the edges of our input graph from GII • P as follows. Let q E [0, 1] be 
such that p = 2q - q2. Consider any edge (u, v) in the input graph. We execute exactly 
one of the following three possibilities: with probability q(1 - q)/(2q - q2) we place 
the edge on u 's unused-edges list but not on v's; with probability q (1- q )/(2q - q2) we 
initially place the edge on v's unused-edges list but not on u's; and with the remaining 
probability q2/(2q - q2) the edge is placed on both unused-edges lists. 
118 

5.7 EXERCISES 
Now, for any possible edge (u, v), the probability that it is initially placed in the 
unused-edges list for v is 
(
q(l - q) 
q2) 
p 
2 + 
") 
= q. 
2q - q 
2q - q-
Moreover, the probability that an edge (u, v) is initially placed on the unused-edges 
list for both u and v is pq2/(2q - q2) = q2, so these two placements are independent 
events. Since each edge (u, v) is treated independently, this partitioning fulfills the re-
quirements of Theorem 5.16 provided the resulting q is at least 20Inn/n. When p 2: 
40lnn/n we have q 2: p/2 2: 20Inn/n, and the result follows. 
• 
In Exercise 5.26, we consider how to use Algorithm 5.2 even in the case where p is not 
known in advance, so that the edge lists must be initialized without knowledge of p. 
5.7. Exercises 
Exercise 5.1: For what values ofn is (1 + l/n)1I within 1% of e? Within 0.0001% of e? 
Similarly, for what values of n is (1 -
l/n)1I within 1% of lie? Within 0.0001%'1 
Exercise 5.2: Suppose that Social Security numbers were issued uniformly at random, 
with replacement. That is, your Social Security number would consist of just nine ran-
domly generated digits, and no check would be made to ensure that the same number 
was not issued twice. Sometimes, the last four digits of a Social Security number are 
used as a password. How many people would you need to have in a room before it was 
more likely than not that two had the same last four digits? How many numbers could 
be issued before it would be more likely than not that there is a duplicate number? How 
would you answer these two questions if Social Security numbers had 13 digits? Try 
to give exact numerical answers. 
Exercise 5.3: Suppose that balls are thrown randomly into n bins. Show, for some 
constant Cl, that if there are Cl ft balls then the probability that no two land in the same 
bin is at most l/e. Similarly, show for some constant C2 (and sufficiently large n) that, 
if there are c2ft balls, then the probability that no two land in the same bin is at least 
1/2. Make these constants as close to optimal as possible. Hint: You may want to use 
the facts that 
and 
I 
for x < -. 
-
2 
Exercise 5.4: In a lecture hall containing 100 people, you consider whether or not there 
are three people in the room who share the same birthday. Explain how to calculate 
this probability exactly, using the same assumptions as in our previous analysis. 
Exercise 5.5: Let X be a Poisson random variable with mean /1, representing the num-
ber of errors on a page of this book. Each error is independently a grammatical error 
119 

BALLS, BINS, AND RANDOM GRAPHS 
with probability p and a spelling error with probability I - p. If Y and Z are random 
variables representing the number of grammatical and spelling errors (respectively) on 
a page of this book, prove that Y and Z are Poisson random variables with means f1P 
and f1(l -
p), respectively. Also, prove that Y and Z are independent. 
Exercise 5.6: Use the Taylor expansion 
x 2 
.r-~ 
x.f 
InO + x) = x -
-
+ - - - + ... 
2 
3 
4 
to prove that, for any x with Ixl ~ l, 
Exercise 5.7: Suppose that n balls are thrown independently and uniformly at random 
into n bins. 
(a) Find the conditional probability that bin I has one ball given that exactly one ball 
fell into the first three bins. 
(b) Find the conditional expectation of the number of balls in bin I under the condition 
that bin 2 received no balls. 
(c) Write an expression for the probability that bin I receives more balls than bin 2. 
Exercise 5.8: Our analysis of Bucket sort in Section 5.2.2 assumed that n elements 
were chosen independently and uniformly at random from the range [0, 2k). Suppose 
instead that 11 elements are chosen independently from the range [0,2 k) according to a 
distribution with the property that any number x E [0, 2k) is chosen with probability at 
most O/2k for some fixed constant a > O. Show that, under these conditions, Bucket 
sort still requires linear expected time. 
Exercise 5.9: Consider the probability that every bin receives exactly one ball when 
11 balls are thrown randomly into II bins. 
(a) Give an upper bound on this probability using the Poisson approximation. 
(b) Determine the exact probability of this event. 
(c) Show that these two probabilities differ by a multiplicative factor that equals the 
probability that a Poisson random variable with parameter II takes on the value n. 
Explain why this is implied by Theorem 5.6. 
Exercise 5.10: Consider throwing 111 balls into n bins, and for convenience let the 
bins be numbered from 0 to II -
l. We say there is a k-gap starting at bin i if bins 
i, i + I, ... ,i + k - I are all empty. 
(a) Determine the expected number of k-gaps. 
(b) Prove a Chernoff-like bound for the number of k-gaps. (Hint: If you let Xi = I 
when there is a k-gap starting at bin i, then there are dependencies between Xi and 
Xi+l; to avoid these dependencies, you might consider Xi and X i+k .) 
120 

5.7 EXERCISES 
Exercise 5.11: The following problem models a simple distributed system wherein 
agents contend for resources but "back off" in the face of contention. Balls represent 
agents, and bins represent resources. 
The system evolves over rounds. Every round. balls are thrown independently and 
uniformly at random into 11 bins. Any ball that lands in a bin by itself is served and 
removed from consideration. The remaining balls are thrown again in the next round. 
We begin with n balls in the first round, and we finish when every ball is served. 
(a) If there are b balls at the start of a round. what is the expected number of balls at 
the start of the next round? 
(b) Suppose that every round the number of balls sen'ed was exactly the expected num-
ber of balls to be served. Show that all the balls would be sen'ed in O( log log n) 
rounds. (Hint: If Xj is the expected number of balls left after .i rounds, show and 
use that Xj+! :s x//n.) 
Exercise 5.12: Suppose that we vary the balls-and-bins process as fullows. For conve-
nience let the bins be numbered from 0 to n -
1. There are log.:' II players, Each player 
randomly chooses a starting location e uniformly from [0. II -
1] and then places one 
ball in each of the bins numbered f. mod 11, £ + 1 mod II • •••• t -
II lug.:' II -
I mod II. 
Argue that the maximum load in this case is only O( log log II / log log log II I with prob-
ability that approaches I as II --+ 00. 
Exercise 5.13: We prove that if Z is a Poisson random variable of mean /1... \\ here /1.. > 
1 is an integer, then Pr(Z ~ /1) ~ 1/2 and Pr(Z ~ /l) ~ 1/2, 
(a) ShowthatPr(Z = 11 +h) ~ Pr(Z = 11-h -1) forO ~ h ~ /J..-1. 
(b) Using part (a), argue that Pr(Z ~ /l) ~ 1/2. 
(c) ShowthatPr(Z = /l-h) ~ Pr(Z = /l +h + I) forO ~ Iz ~ /1.., 
(d) Determine a lower bound on Pr(Z = /l -
h) - Pr(Z = /l + Iz -i- I), 
(e) Determine an upper bound on Pr(Z ~ 2/l + 2). 
(f) Using parts (c)-(e), argue that Pr(Z ~ /l) ~ 1/2. 
Exercise 5.14: (a) In Theorem 5.7 we showed that. for any nonnegati\e functions f, 
Prove that ifE[f(X/
III
), ••• , X,;In)] is monotonically increasing in Ill. then 
again under the condition that f is nonnegative. Make a similar statement for the case 
when E[f( xi
lll
), ••• , X,;III)) J is monotonically decreasing in m. 
(b) Using part (a) and Exercise 5.13, Prove Theorem 5.10. 
Exercise 5.15: We consider another way to obtain Chernoff-like bounds in the setting 
of balls and bins without using Theorem 5.7. Consider 11 balls thrown randomly into 
121 

BALLS, BINS, AND RANDOM GRAPHS 
n bins. Let Xi = 1 if the ith bin is empty and 0 otherwise. Let X = L;I=I Xi. Let 
lj, i = I, ... , n, be independent Bernoulli random variables that are I with probability 
p = (l - l/n)n. Let Y = L;I=I }j. 
(a) Show that E[X1X2'" X k ] ~ E[Y1Y2 ..• Yd for any k ~ l. 
(b) Show that E[e tx ] :s E[e tY J for all t ~ O. (Hint: Use the expansion for eX and 
compare E[Xk] to E[yk].) 
(c) Derive a Chernoff bound for Pr(X ~ (l + 8)E[XJ). 
Exercise 5.16: Let G be a random graph generated using the GII,p model. 
(a) A clique of k vertices in a graph is a subset of k vertices such that all (;) edges be-
tween these vertices lie in the graph. For what value of p, as a function of n, is the 
expected number of cliques of five vertices in G equal to I? 
(b) A K 3,3 graph is a complete bipartite graph with three vertices on each side. In 
other words, it is a graph with six vertices and nine edges; the six distinct vertices 
are arranged in two groups of three, and the nine edges connect each of the nine 
pairs of vertices with one vertex in each group. For what value of p, as a function 
of n, is the expected number of K3.3 subgraphs of G equal to I? 
(c) For what value of p, as a function of n, is the expected number of Hamiltonian 
cycles in the graph equal to I? 
Exercise 5.17: Theorem 5.7 shows that any event that occurs with small probability in 
the balls-and-bins setting where the number of balls in each bin is an independent Pois-
son random variable also occurs with small probability in the standard balls-and-bins 
model. Prove a similar statement for random graphs: Every event that happens with 
small probability in the Gn,l' model also happens with small probability in the GIl,N 
model for N = G) p. 
Exercise 5.18: An undirected graph on n vertices is disconnected if there exists a set 
of k < n vertices such that there is no edge between this set and the rest of the graph. 
Otherwise, the graph is said to be connected. Show that there exists a constant c such 
that if N ~ en log n then, with probability G(e- II ), a graph randomly chosen from 
G,l, N is connected. 
Exercise 5.19: Prove Theorem 5.14. 
Exercise 5.20: (a) Let f(n) be the expected number of random edges that must be 
added before an empty undirected graph with n vertices becomes connected. (Con-
nectedness is defined in Exercise 5.18.) That is, suppose that we start with a graph on 
n vertices with zero edges and then repeatedly add an edge, chosen uniformly at ran-
dom from all edges not currently in the graph, until the graph becomes connected. If 
Xn represents the number of edges added, then fen) = E[Xn J. 
Write a program to estimate fen) for a given value of n. Your program should track 
the connected components of the graph as you add edges until the graph becomes con-
nected. You will probably want to use a disjoint set data structure, a topic covered in 
122 

5.7 EXERCISES 
standard undergraduate algorithms texts. You should try n = 100, 200, 300, 400, 500, 
600, 700, 800, 900, and 1000. Repeat each experiment 100 times, and for each value of 
n compute the average number of edges needed. Based on your experiments, suggest 
a function h (n) that you think is a good estimate for f( n). 
(b) Modify your program for the problem in part (a) so that it also keeps track of 
isolated vertices. Let g(n) be the expected number of edges added before there are no 
more isolated vertices. What seems to be the relationship between fen) and g(n)? 
Exercise 5.21: In hashing with open addressing. the hash table is implemented as an 
array and there are no linked lists or chaining. Each entry in the array either contains 
one hashed item or is empty. The hash function defines, for each key k, a probe se-
quence h(k,O),h(k, I), '" of table locations. To insert the k.ey k. we first examine the 
sequence of table locations in the order defined by the k.ey· s probe sequence until we 
find an empty location; then we insert the item at that position. \\/hen searching for 
an item in the hash table, we examine the sequence of table locations in the order de-
fined by the key's probe sequence until either the item i:-. found or we ha\'e found an 
empty location in the sequence. If an empty location is found. thi:-. means the item is 
not present in the table. 
An open-address hash table with 2n entries is used to store II item:-., Assume that 
the table location h(k, j) is uniform over the 2n possible table location:-. and that all 
h (k, j) are independent. 
(a) Show that, under these conditions, the probability of an insertiun rCl{uiring more 
than k probes is at most 2--k• 
(b) Show that, for i = 1,2, ... , n, the probability that the itb in:-.ertion rel{uires more 
than 210g n probes is at most l/n 2. 
Let the random variable Xi denote the number of probes required h~ the ith insertion. 
You have shown in part (b) that Pre Xi > 2 log n) ::: I /n 2. Let the random \ariahle X = 
maxI.:':i.:':1l Xi denote the maximum number of probes required by any of the II in-;ertions. 
(c) Show that Pr(X > 210gn) :s lin. 
(d) Show that the expected length of the longest probe sequence is E [X 1 = o( log II). 
Exercise 5.22: Bloom filters can be used to estimate set differences. Suppose you 
have a set X and I have a set y, both with 11 elements. For example, the sets might rep-
resent our 100 favorite songs. We both create Bloom filters of our sets. using the same 
number of bits In and the same k hash functions. Determine the expected number of 
bits where our Bloom filters differ as a function of In, 11, k, and IX r, Y!, Explain how 
this could be used as a tool to find people with the same taste in music more easily than 
comparing lists of songs directly. 
Exercise 5.23: Suppose that we wanted to extend Bloom filters to allO\v deletions as 
well as insertions of items into the underlying set. We could modify the Bloom fil-
ter to be an array of counters instead of an array of bits. Each time an item is inserted 
into a Bloom filter, the counters given by the hashes of the item are increased by one. 
123 

BALLS, BINS, AND RANDOM GRAPHS 
To delete an item, one can simply decrement the counters. To keep space small, the 
counters should be a fixed length, such as 4 bits. 
Explain how errors can arise when using fixed-length counters. Assuming a setting 
where one has at most n elements in the set at any time, m counters, k hash functions, 
and counters with b bits, explain how to bound the probability that an error occurs over 
the course of t insertions or deletions. 
Exercise 5.24: Suppose that you built a Bloom filter for a dictionary of words with 
m = 217 bits. A co-worker building an application wants to use your Bloom filter but 
has only 2b-
j bits available. Explain how your colleague can use your Bloom filter to 
avoid rebuilding a new Bloom filter using the original dictionary of words. 
Exercise 5.25: For the leader election problem alluded to in Section 5.5.4, we have n 
users, each with an identifier. The hash function takes as input the identifier and out-
puts a b-bit hash value, and we assume that these values are independent and uniformly 
distributed. Each user hashes its identifier, and the leader is the user with the smallest 
hash value. Give lower and upper bounds on the number of bits b necessary to ensure 
that a unique leader is successfully chosen with probability p. Make your bounds as 
tight as possible. 
Exercise 5.26: Consider Algorithm 5.2, the modified algorithm for finding Hamilton-
ian cycles. We have shown that the algorithm can be applied to find a Hamiltonian 
cycle with high probability in a graph chosen randomly from GIl,p, when p is known 
and sufficiently large, by initially placing edges in the edge lists appropriately. Argue 
that the algorithm can similarly be applied to find a Hamiltonian cycle with high prob-
ability on a graph chosen randomly from Gil . .\' when N = Cj n In n for a suitably large 
constant Cj. Argue also that the modified algorithm can be applied even when p is not 
known in advance as long as p is at least CJ In n /n for a suitably large constant C2. 
5.8. An Exploratory Assignment 
Part of the research process in random processes is first to understand what is going on 
at a high level and then to use this understanding in order to develop formal mathemat-
ical proofs. In this assignment. you will be given several variations on a basic random 
process. To gain insight. you should perform experiments based on writing code to 
simulate the processes. (The code should be very short, a few pages at most.) After the 
experiments, you should use the results of the simulations to guide you to make conjec-
tures and prove statements about the processes. You can apply what you have learned 
up to this point, including probabilistic bounds and analysis of balls-and-bins problems. 
Consider a complete binary tree with N = 211 - 1 nodes. Here n is the depth of the 
tree. Initially, all nodes are unmarked. Over time, via processes that we shall describe, 
nodes becomes marked. 
All of the processes share the same basic form. We can think of the nodes as having 
unique identifying numbers in the range of [1, N]. Each unit of time, I send you the 
124 

5.8 AN EXPLORATORY ASSIGNMENT 
Figure 5.3: The arrival of X causes all other nodes to be marked. 
identifier of a node. When you receive a sent node, you mark it. Also, you invoke the 
following marking rule, which takes effect before I send out the next node. 
• If a node and its sibling are marked, its parent is marked. 
• If a node and its parent are marked, the other sibling is marked. 
The marking rule is applied recursively as much as possible before the next node 
is sent. For example, in Figure 5.3, the marked nodes are filled in. The arrival of the 
node labeled by an X will allow you to mark the remainder of the nodes. as you apply 
the marking rule first up and then down the tree. Keep in mind that you always apply 
the marking rule as much as possible. 
Now let us consider the different ways in which I might be sending you the nodes. 
Process 1: Each unit of time, I send the identifier of a node chosen independently and 
un(formly at random from all (~fthe N nodes. Note that I might send you a node that 
is already marked, and in fact I may send a useless node that I have already sent. 
Process 2: Each unit of time I send the identifier of a node chosen uniformly at ran-
dom from those nodes that I have not yet sent. Again, a node that has already been 
marked might arrive, but each node will be sent at most once. 
Process 3: Each unit of time I send the identifier of a node chosen uniformly at random 
from those nodes that you have not yet marked. 
We want to determine how many time steps are needed before all the nodes are 
marked for each of these processes. Begin by writing programs to simulate the send-
ing processes and the marking rule. Run each process ten times for each value of 11 in 
the range [10,20]. Present the data from your experiments in a clear, easy-to-read fash-
ion and explain your data suitably. A tip: You may find it useful to have your program 
print out the last node that was sent before the tree became completely marked. 
1. For the first process, prove that the expected number of nodes sent is Q (N log N). 
How well does this match your simulations? 
2. For the second process, you should tind that almost all N nodes must be sent before 
the tree is marked. Show that, with constant probability, at least N - 2 IN nodes 
must be sent. 
3. The behavior of the third process might seem a bit unusual. Explain it with a proof. 
After answering these questions, you may wish to consider other facts you could prove 
about these processes. 
125 

CHAPTER SIX 
The Probabilistic Method 
The probabilistic method is a way of proving the existence of objects. The underly-
ing principle is simple: to prove the existence of an object with certain properties, we 
demonstrate a sample space of objects in which the probability is positive that a ran-
domly selected object has the required properties. If the probability of selecting an 
object with the required properties is positive, then the sample space must contain such 
an object, and therefore such an object exists. For example, if there is a positive proba-
bility of winning a million-dollar prize in a raffle, then there must be at least one raffle 
ticket that wins that prize. 
Although the basic principle of the probabilistic method is simple, its application to 
specific problems often involves sophisticated combinatorial arguments. In this chap-
ter we study a number of techniques for constructing proofs based on the probabilistic 
method, starting with simple counting and averaging arguments and then introducing 
two more advanced tools, the Lovasz local lemma and the second moment method. 
In the context of algorithms we are generally interested in explicit constructions 
of objects, not merely in proofs of existence. In many cases the proofs of existence 
obtained by the probabilistic method can be converted into efficient randomized con-
struction algorithms. In some cases. these proofs can be converted into efficient de-
terministic construction algorithms: this process is called derandomizatiol1, since it 
converts a probabilistic argument into a deterministic one. We give examples of both 
randomized and deterministic construction algorithms arising from the probabilistic 
method. 
6.1. The Basic Counting Argument 
To prove the existence of an object with specific properties. we construct an appropri-
ate probability space S of objects and then show that the probability that an object in 
S with the required properties is selected is strictly greater than O. 
For our first example, we consider the problem of coloring the edges of a graph with 
two colors so that there are no large cliques with all edges having the same color. Let 
126 

6.1 THE BASIC COUNTING ARGUMENT 
KI/ be a complete graph (with all G) edges) on 11 vertices. A clique of k vertices in KII 
is a complete subgraph Kk . 
Theorem 6.1: {f (;)2-(~)+1 < 1, then it is possible to color the edges qf KI/ H'ith two 
colors so that it has 110 m011ochromatic K" slfbgraph. 
Proof: Define a sample space consisting of all possible colorings of the edges of KI/ 
using two colors. There are 2CD possible colorings, so if one is chosen uniformly at 
random then the probability of choosing each coloring in our probability space is 2-C). 
A nice way to think about this probability space is: if we color each edge of the graph 
independently, with each edge taking each of the two colors with probability 1/2, then 
we obtain a random coloring chosen uniformly from this sample space. That is, we flip 
an independent fair coin to determine the color of each edge. 
Fix an arbitrary ordering of all of the G) different k-vertex cliques of Kn , and for 
i = 1, ... , G) let A i be the event that clique i is monochromatic. Once the first edge 
in clique i is colored, the remaining (;) - 1 edges must all be given the same color. It 
follows that 
Using a union bound then yields 
where the last inequality follows from the assumptions of the theorem. Hence 
Since the probability of choosing a coloring with no monochromatic k-vertex clique 
from our sample space is strictly greater than 0, there must exist a coloring with no 
monochromatic k-vertex clique. 
• 
As an example, consider whether the edges of K]()()() can be 2-colored in such a way 
that there is no monochromatic K20 . Our calculations are simplified if we note that. for 
n :s 2"/2 and k 2: 3, 
( 11)2-(;)+1 < ~2-(k("-1)/2l+1 
k 
-
k! 
2"12+1 
<--
k! 
< 1. 
Observing that for our example 11 = 1000 :s 2 10 = 2"/2, we see that by Theorem 6.1 
there exists a 2-coloring of the edges of K]()()o with no monochromatic K 20 . 
127 

THE PROBABILISTIC METHOD 
Can we use this proof to design an efficient algorithm to construct such a coloring? 
Let us consider a general approach that gives a randomized construction algorithm. 
First, we require that we can efficiently sample a coloring from the sample space. In 
this case sampling is easy, because we can simply color each edge independently with 
a randomly chosen color. In general, however, there might not be an efficient sampling 
algorithm. 
If we have an efficient sampling algorithm, the next question is: How many sam-
ples must we generate before obtaining a sample that satisfies our requirements? If 
the probability of obtaining a sample with the desired properties is p and if we sample 
independently at each trial, then the number of samples needed before finding a sam-
ple with the required properties is a geometric random variable with expectation lip. 
Hence we need that lip be polynomial in the problem size in order to have an algorithm 
that finds a suitable sample in polynomial expected time. 
If p = I -
0(1), then sampling once gives a Monte Carlo construction algorithm 
that is incorrect with probability 0(1). In our specific example of finding a coloring on 
a graph of 1000 vertices with no monochromatic K 2(), we know that the probability that 
a random coloring has a monochromatic K 20 is at most 
220/2+ 1 
-- < 8.5 . 10- 16 
20! 
Hence we have a Monte Carlo algorithm with a small probability of failure. 
If we want a Las Vegas algorithm - that is, one that always gives a correct construc-
tion - then we need a third ingredient. We require a polynomial time procedure for 
verifying that a sample object satisfies the requirements; then we can test samples until 
we find one that does so. An upper bound on the expected time for this construction 
can be found by multiplying together the expected number of samples lip, an upper 
bound on the time to generate each sample, and an upper bound on the time to check 
each sample. 1 For the coloring problem, there is a polynomial time verification algo-
rithm when k is a constant: simply check all G) cliques and make sure they are not 
monochromatic. It does not seem that this approach can be extended to yield polyno-
mial time algorithms when k grows with n. 
6.2. The Expectation Argument 
As we have seen, in order to prove that an object with certain properties exists, we can 
design a probability space from which an element chosen at random yields an object 
with the desired properties with positive probability. A similar and sometimes easier 
approach for proving that such an object exists is to use an averaging argument. The 
intuition behind this approach is that, in a discrete probability space, a random variable 
must with positive probability assume at least one value that is no greater than its ex-
pectation and at least one value that is not smaller than its expectation. For example, if 
I Sometimes the time to generate or check a sample may itself be a random variable. In this case. Wald 's equation 
(discussed in Chapter 12) may apply. 
128 

6.2 THE EXPECTATION ARGUMENT 
the expected value of a raffle ticket is at least $3, then there must be at least one ticket 
that ends up being worth no more than $3 and at least one that ends up being worth no 
less than $3. 
More formally, we have the following lemma. 
Lemma 6.2: Suppose we have a probability space S and a random variable X defined 
on S such that E[X] = f-l. Then Pr(X :::: /-1) > 0 and Pr(X ::'S /-1) > O. 
Proof: We have 
f-l = E [X 1 = LX Pr (X = x), 
x 
where the summation ranges over all values in the range of X. If Pre X :::: f-l) = 0, then 
f-l = LX Pr (X = x) = L x Pr (X = x) < L f-l Pr (X = x) = f-l, 
x 
X<11 
X<ll 
giving a contradiction. Similarly, ifPr(X ::'S f-l) = 0 then 
f-l = LX Pr (X = x) = L x Pr (X = x) > L f-l Pr (X = x) = f-l, 
x 
X>fl 
X>fl 
again yielding a contradiction. 
• 
Thus, there must be at least one instance in the sample space of S for which the value 
of X is at least /-1 and at least one instance for which the value of X is no greater than /-1. 
6.2.1. Application: Finding a Large Cut 
We consider the problem of finding a large cut in an undirected graph. A cut is a par-
tition of the vertices into two disjoint sets, and the value of a cut is the weight of all 
edges crossing from one side of the partition to the other. Here we consider the case 
where all edges in the graph have the same weight l. The problem of finding a max-
imum cut is NP-hard. Using the probabilistic method, we show that the value of the 
maximum cut must be at least 1/2 the number of edges in the graph. 
Theorem 6.3: Given an undirected graph G with n vertices and m edges, there is a 
partition of V into two disjoint sets A and B such that at least m/2 edges connect a 
\'ertex in A to a vertex in B. That is, there is a cut with value at least m/2. 
Proof: Construct sets A and B by randomly and independently assigning each vertex 
to one of the two sets. Let e], ... , em be an arbitrary enumeration of the edges of G. 
For i = 1, ... , m, define Xi such that 
Xi = {~ 
if edge i connects A to B, 
otherwise. 
The probability that edge ei connects a vertex in A to a vertex in B is 1/2, and thus 
129 

THE PROBABILISTIC METHOD 
Let C(A, B) be a random variable denoting the value of the cut corresponding to the 
sets A and B. Then 
Since the expectation of the random variable C(A, B) is m/2, there exists a partition 
A and B with at least m/2 edges connecting the set A to the set B. 
• 
We can transform this argument into an efficient algorithm for finding a cut with value 
at least m/2. We first show how to obtain a Las Vegas algorithm. In Section 6.3, we 
show how to construct a deterministic polynomial time algorithm. 
It is easy to randomly choose a partition as described in the proof. The expectation 
argument does not give a lower bound on the probability that a random partition has a 
cut of value at least m/2. To derive such a bound, let 
P = pr(C(A, 8) ?: ~), 
and observe that C(A, B) :::; m. Then 
m 
-
= E[C(A, B)] 
2 
L 
i Pr(C(A, B) = i) + L i Pr(C(A, B) = i) 
i~m/2 
which implies that 
P 2: m/2 + l' 
The expected number of samples before finding a cut with value at least m/2 is there-
fore just m/2 + 1. Testing to see if the value of the cut determined by the sample is at 
least m/2 can be done in polynomial time simply by counting the edges crossing the 
cut. We therefore have a Las Vegas algorithm for finding the cut. 
6.2.2. Application: Maximum Satisfiability 
We can apply a similar argument to the maximum satisfiability (MAXSAT) problem. 
In a logical formula, a literal is either a Boolean variable or the negation of a Boolean 
variable. We use x to denote the negation of the variable x. A satisfiability (SAT) prob-
lem, or a SAT formula, is a logical expression that is the conjunction (AND) of a set 
of clauses, where each clause is the disjunction (OR) of literals. For example, the fol-
lowing expression is an instance of SAT: 
130 

6.3 DERANDOMIZATION USING CONDITIONAL EXPECTATIONS 
A solution to an instance of a SAT formula is an assignment of the variables to the val-
ues True and False so that all the clauses are satisfied. That is, there is at least one true 
literal in each clause. For example, assigning XI to True, X2 to False. X3 to False. and X.1-
to True satisfies the preceding SAT formula. In generaL determining if a SAT formula 
has a solution is NP-hard. 
A related goal. given a SAT formula, is satisfying as many of the clauses as pos-
..;ible. In what follows, let us assume that no clause contains both a variable and its 
complement, since in this case the clause is always satisfied. 
Theorem 6.4: Given a set of m clauses, let k i be the number (~f literals in the ith clause 
t(J!' i = I, ... , m. Let k = min;'~1 ki . Then there is a truth assignment that satisfies at 
least 
//I L(l - 2-ki ) 2: m(l - 2- k ) 
i=1 
l'ialfses. 
Proof: Assign values independently and uniformly at random to the variables. The 
probability that the ith clause with k i literals is satisfied is at least (1 - 2-ki ). The ex-
pected number of satisfied clauses is therefore at least 
III L (1 - 2- ki ) 2: m(1 - 2-1\). 
i=1 
~nd there must be an assignment that satisfies at least that many clauses. 
• 
The foregoing argument can also be easily transformed into an efficient randomized 
~Igorithm; the case where all ki = k is left as Exercise 6.1. 
6.3. Derandomization Using Conditional Expectations 
The probabilistic method can yield insight into how to construct deterministic algo-
rithms. As an example, we apply the method of conditional expectations in order to 
,/erandomize the algorithm of Section 6.2.1 for finding a large cut. 
Recall that we find a partition of the 11 vertices V of a graph into sets A and B by 
placing each vertex independently and uniformly at random in one of the two sets. This 
gives a cut with expected value E[C(A, B)] 2: m/2. Now imagine placing the vertices 
Jeterministically, one at a time, in an arbitrary order VI, V2, ... , VII' Let Xi be the set 
\\here Vi is placed (so Xi is either A or B). Suppose that we have placed the first k 
\ crtices, and consider the expected value of the cut if the remaining vertices are then 
placed independently and uniformly into one of the two sets. We write this quantity 
~,E[C(A, B) I XI,X2, ... ,xd; it is the conditional expectation of the value of the cut 
given the locations XI, X2, ... , Xk of the first k vertices. We show inductively how to 
place the next vertex so that 
131 

THE PROBABILISTIC METHOD 
E[C(A,B) I xI,x2, ... ,xd:::: E[C(A,B) I XI,X2, ... ,Xk+l]. 
It follows that 
E[C(A, B)] :::: E[C(A, B) I XI,X2, ... ,xll ]. 
The right-hand side is the value of the cut determined by our placement algorithm, 
since if XI, X2, ... , x" are all determined then we have a cut of the graph. Hence our 
algorithm returns a cut whose value is at least E[C(A, B)] :::: m/2. 
The base case in the induction is 
E[C(A, B) I XI] = E[C(A, B)], 
which holds by symmetry because it does not matter where we place the first vertex. 
We now prove the inductive step, that 
Consider placing Vk+1 randomly, so that it is placed in A or B with probability 1/2 
each, and let Yk+1 be a random variable representing the set where it is placed. Then 
E[C(A,B) I XI,X2, ... ,Xk] = ~E[C(A,B) I XI,X2, ... ,XbYk+1 = A] 
+ ~E[C(A, B) I XI,X2, ... ,Xb Yk+ 1 = B]. 
It follows that 
max(E[C(A,B) I XI,X2, ... ,Xb Yk+1 = A],E[C(A,B) I XI,X2, ... ,XbYk+1 = B]) 
:::: E[C(A, B) I XI,X2, ""Xk]. 
Therefore, all we have to do is compute the two quantities E[C(A, B) I XI,X2, ... , 
Xb Yk+ 1 = A] and E[C(A, B) I XI,X2, .. "XbYk+1 = B] and then place the Vk+1 in 
the set that yields the larger expectation. Once we do this, we will have a placement 
satisfying 
E[C(A,B) I xI,x2, ... ,xd:::: E[C(A,B) I xI,x2"",xk+ll· 
To compute E[C(A, B) I XI, X2, ... , Xb Y k+ 1 = A], note that the conditioning gives 
the placement of the first k + 1 vertices. We can therefore compute the number of edges 
among these vertices that contribute to the value of the cut. For all other edges, the prob-
ability that it will later contribute to the cut is 1/2, since this is the probability its two 
endpoints end up on different sides of the cut. By linearity of expectations, E [C(A, B) I 
XI,X2, ... ,Xk,Yk+ 1 = A] is the number of edges crossing the cut whose endpoints are 
both among the first k + 1 vertices, plus half of the remaining edges. This is easy to 
compute in linear time. The same is true forE[C(A,B) I XI,X2" .. ,XbYk+1 = B]. 
In fact, from this argument, we see that the larger of the two quantities is deter-
mined just by whether Vk+1 has more neighbors in A or in B. All edges that do not have 
L'k+1 as an endpoint contribute the same amount to the two expectations. Our deran-
domized algorithm therefore has the following simple form: Take the vertices in some 
order. Place the first vertex arbitrarily in A. Place each successive vertex to maximize 
the number of edges crossing the cut. Equivalently, place each vertex on the side with 
132 

6.4 SAMPLE AND MODIFY 
fewer neighbors, breaking ties arbitrarily. This is a simple greedy algorithm, and our 
analysis shows that it always guarantees a cut with at least ml2 edges. 
6.4. Sample and Modify 
Thus far we have used the probabilistic method to construct random structures with 
the desired properties directly. In some cases it is easier to work indirectly, breaking 
the argument into two stages. In the first stage we construct a random structure that 
does not have the required properties. In the second stage we then modify the ran-
dom structure so that it does have the required property. We give two examples of this 
sample-and-modify technique. 
6.4.1. Application: Independent Sets 
An independent set in a graph G is a set of vertices with no edges between them. 
Finding the largest independent set in a graph is an NP-hard problem. The following 
theorem shows that the probabilistic method can yield bounds on the size of the largest 
independent set of a graph. 
Theorem 6.5: Let G = (V, £) be a graph on n vertices with m edges. Then G has an 
independent set with at least n 2/4m vertices. 
Proof· Let d = 2mln be the average degree of the vertices in G. Consider the follow-
ing randomized algorithm. 
1. Delete each vertex of G (together with its incident edges) independently with prob-
ability I - lid. 
2. For each remaining edge, remove it and one of its adjacent vertices. 
The remaining vertices form an independent set, since all edges have been removed. 
This is an example of the sample-and-modify technique. We first sample the vertices, 
and then we modify the remaining graph. 
Let X be the number of vertices that survive the first step of the algorithm. Since the 
graph has n vertices and since each vertex survives with probability lid, it follows that 
n 
E[X] = d. 
Let Y be the number of edges that survive the first step. There are n d 12 edges in the 
graph, and an edge survives if and only if its two adjacent vertices survive. Thus 
nd (1)2 
E[Y] = 2 d 
11 
2d 
The second step of the algorithm removes all the remaining edges and at most Y 
\'ertices. When the algorithm terminates, it outputs an independent set of size at least 
X -
Y, and 
11 
n 
n 
E[X - Y] = d - 2d = 2d· 
133 

THE PROBABILISTIC METHOD 
The expected size of the independent set generated by the algorithm is n/2d, so the 
graph has an independent set with at least n / 2d = n2/ 4m vertices. 
• 
6.4.2. Application: Graphs with Large Girth 
As another example we consider the girth of a graph, which is the length of its smallest 
cycle. Intuitively we expect dense graphs to have small girth. We can show, however, 
that there are dense graphs with relatively large girth. 
Theorem 6.6: For any integer k 2: 3 there is a graph with n nodes, at least inl+l/k 
edges, and girth at least k. 
Proof' We first sample a random graph G E Gn •J7 with p = n 1/ k- 1• Let X be the num-
ber of edges in the graph. Then 
Let Y be the number of cycles in the graph of length at most k -
1. Any specific 
possible cycle of length i, where 3 :s i :s k -
I, occurs with probability pi. Also, 
there are (';) (i~l)! possible cycles of length i; to see this, first consider choosing the i 
vertices, then consider the possible orders, and finally keep in mind that reversing the 
order yields the same cycle. Hence, 
We modify the original randomly chosen graph G by eliminating one edge from 
each cycle of length up to k -
I. The modified graph therefore has girth at least k. 
When 11 is sufficiently large. the expected number of edges in the resulting graph is 
Hence there exists a graph with at least ±n 1+1/ k edges and girth at least k. 
• 
6.5. The Second Moment Method 
The second moment method is another useful way to apply the probabilistic method. 
The standard approach typically makes use of the following inequality, which is easily 
derived from Chebyshev's inequality. 
Theorem 6.7: If X is a nonnegative integer-valued random variable, then 
Var[X] 
Pr(X = 0) < ---
-
(E[X])2' 
134 
(6.2) 

Proof: 
6.S THE SECOND MOMENT METHOD 
Var[X] 
Pr (X = 0) :s Pr (I X - E [ X] I ~ E [ X]) :s (E [X]) 2 . 
6.5.1. Application: Threshold Behavior in Random Graphs 
• 
The second moment method can be used to prove the threshold behavior of certain ran-
dom graph properties. That is, in the Gn .!) model it is often the case that there is a 
threshold function f such that: (a) when p is just less than f( n), almost no graph has 
the desired property; whereas (b) when p is just larger than f(n). almost every graph 
has the desired property. We present here a relatively simple example. 
Theorem 6.8: In GIl,p, suppose that p = f(n), ~vhere fen) = o(n~2j3). Then, for 
any c > 0 andfor sufficiently large n, the probability that a random graph chosen from 
Gn.p has a clique offour or more vertices is less than c. Similarly, if fen) = w(n~2/3) 
then, for sufficiently large n, the probability that a random graph chosen from Gn,p 
does not have a clique with four or more vertices is less than c. 
Proof: We first consider the case in which p = fen) and fen) = 
o(n~2/3). Let 
C], ... , C C) be an enumeration of all the subsets of four vertices in G. Let 
Let 
so that 
X; = {~ 
if Ci is a 4-clique, 
otherwise. 
(1) 
X=LXi, 
i=] 
E[Xl=(:)p6 
InthiscaseE[X] = o(l),whichmeansthatE[X] < cforsufficientlylargen. SinceXis 
a nonnegative integer-valued random variable, it follows that Pr(X ~ I) :s E[X J < c. 
Hence, the probability that a random graph chosen from Gn,p has a clique of four or 
more vertices is less than c. 
We now consider the case when p = fen) and fen) = 
w(n~2/3). In this case, 
E l X] -+ 00 as n grows large. This in itself is not sufficient to conclude that, with high 
probability, a graph chosen random from Gn,p has a clique of at least four vertices. 
\\'e can, however, use Theorem 6.7 to prove that Pr(X = 0) = 0(1) in this case. To 
do so we must show that Var[X] = o(E[X])2). Here we shall compute the variance 
directly; an alternative approach is given as Exercise 6.11. 
We begin with the following useful formula. 
Lemma 6.9: Let }j, i = 1, ... , m, be 0-1 random variables, and let Y 
Then 
Var[Y] :s E[Y] + 
Cov ( }j, }j ) . 
135 
"Ill Y. 
Li=] {. 

THE PROBABILISTIC METHOD 
Proof: For any sequence of random variables YI , ••. , Ym , 
[ 
m 
] 
m 
Var L lj = LVar[lj] + L 
Cov(lj, lj). 
i=1 
i=1 
12:i.j~m: ifj 
This is the generalization of Theorem 3.2 to m variables. 
When }j is a 0-1 random variable, E [li 2] = E [lj] and so 
giving the lemma. 
D 
We wish to compute 
(" ) 
Var[Xl = var[ ~ xJ 
Applying Lemma 6.9, we see that we need to consider the covariance of the Xi. If 
I Ci n Cj I = 0 then the corresponding cliques are disjoint, and it follows that Xi and Xj 
are independent. Hence, in this case, E[XiXj ] - E[Xi]E[Xj ] = O. The same is true if 
ICi n Cj I = 1. 
If I Ci n Cj I = 2, then the corresponding cliques share one edge. For both cliques 
to be in the graph, the eleven corresponding edges must appear in the graph. Hence, 
in this case E[XiXj ] - E[Xi]E[Xj ] :s E[XiXj ] :s pll. There are (~) ways to choose 
the six vertices and (2:~:2) ways to split them into Ci and Cj (because we choose two 
vertices for Ci n Cj, two for Ci alone, and two for Cj alone). 
If ICi n Cj I = 3, then the corresponding cliques share three edges. For both cliques 
to be in the graph, the nine corresponding edges must appear in the graph. Hence, in 
this case E[XiXj] - E[Xi]E[Xj ] :s E[XiXj ] :s p9. There are G) ways to choose the 
five vertices, and (3:~:1) ways to split them into Ci and Cj . 
Finally, recall again that E[X] = (~)p6 and p = f(n) = w(n-2/3). Therefore, 
since 
Theorem 6.7 now applies, showing that Pr(X = 0) = 0(1) and thus the second part of 
the theorem. 
• 
6.6. The Conditional Expectation Inequality 
For a sum of Bernoulli random variables, we can derive an alternative to the second 
moment method that is often easier to apply. 
136 

6.6 THE CONDITIONAL EXPECTATION INEQUALITY 
Theorem 6.10: Let X = 2:;1=1 Xi, tvhere each Xi is a 0-1 random variable. Then 
~ Pr(Xi = I) 
Pr (X > 0) ~ f:1 E [X I Xi = I r 
Notice that the Xi need not be independent for Eqn. (6.3) to hold. 
Proof· Let Y = I I X if X > 0, with Y = 0 otherwise. Then 
Pre X > 0) = E[XY]. 
However, 
EfXYj = E[ t XiY] 
1=1 
II 
/{ 
= L (E [X i Y I Xi = I] Pr (Xi = 1) + E [Xi Y I Xi = 0] Pr (Xi = 0)) 
i=1 
1/ 
= L E [Y I Xi = I] Pre Xi = I) 
i=1 
II 
= L ElI/X I Xi = IJ Pr(XI = I) 
i=1 
~ Pr(Xi = I) 
>L 
. 
-. E[X I Xi = I] 
1=1 
(6.3) 
The key step is from the third to the fourth line, where we use conditional expectations 
in a fruitful way by taking advantage of the fact that E[Xi Y I Xi = 0] = O. The last 
line makes use of Jensen's inequality, with the convex function f(x) = I/x. 
• 
\\'e can use Theorem 6.10 to give an alternate proof of Theorem 6.8. Specifically, if 
jJ = fen) = w(n- 2j3 ), we use Theorem 6.10 to show that, for any constant F > 0 and 
for sufficiently large n, the probability that a random graph chosen from Gil.!) does not 
have a clique with four or more vertices is less than c. 
As in the proof of Theorem 6.8, let X = 2:i(2~ Xi, where Xi is 1 if the subset of four 
\ ertices Ci is a 4-clique and 0 otherwise. For a specific X j , we have Pr( XI = 1) = p6. 
l" sing the linearity of expectations, we compute 
(~) 
(~) 
EfX I Xj = II =E[~Xi I X) = I] = ~EfXi I XI = IJ. 
Conditioning on Xj = 1, we now compute E[Xi 
I Xj = 1] by lIsing that, for a 0-1 
ranuom variable, 
E l Xii Xj = 1 J = Pr ( Xi = 1 I Xj = 1). 
137 

THE PROBABILISTIC METHOD 
There are (11~4) sets of vertices Ci that do not intersect Cj . Each corresponding Xi 
is I with probability p6. Similarly, Xi = I with probability p6 for the 4(11]4) sets Ci 
that have one vertex in common with Cj. 
For the remaining cases, we have Pr ( Xi = I I Xi = I) = p 5 for the 6 (11;4) sets 
Ci that have two vertices in common with Cj and Pr( Xi = I I Xj = I) = p3 for the 
4(n~4) sets Ci that have three vertices in common with Cj . Summing, we have 
C) 
E[X I Xi = I] = L E[Xi I Xi = IJ 
i=] 
= 1 + (n ~ 4)p6 + {' ~ 4)p6 +6(n; 4)pS +4(" ~ 4)p3 
Applying Theorem 6.10 yields 
G)p6 
Pr(X > 0) > 
, 
-
I + ('1~4)p6 + 4('1]4)p6 + 6(1l;4)p5 + 4(1l~4)p3 
which approaches I as n grows large when p = f(n) = w(n- 2/3). 
6.7. The Lovasz Local Lemma 
One of the most elegant and useful tools in applying the probabilistic method is the Lo-
vasz local lemma. Let E], ... , Ell be a set of bad events in some probability space. We 
want to show that there is an element in the sample space that is not included in any of 
the bad events. 
This would be easy to do if the events were mutually independent. Recall that events 
E], E 2 , ... , E/1 are mutually independent if and only if, for any subset I ~ [I, n], 
pr( n E;) = n Pr(E;). 
iEI 
iEI 
-
-
Also, if E], ... , E/1 are mutually independent then so are E], ... , Ell. (This was left as 
Exercise 1.20.) IfPr(Ei ) < I for all i, then 
and there is an element of the sample space that is not included in any bad event. 
Mutual independence is too much to ask for in many arguments. The Lovasz local 
lemma generalizes the preceding argument to the case where the n events are not mu-
tually independent but the dependency is limited. Specifically, following from the 
definition of mutual independence, we say that an event E is mutual!.v independent of 
the events E], E 2 , ... , E/1 if, for any subset I ~ [I, n], 
pr( E I n 
Ej ) = Pr(E) 
JEI 
The dependency between events can be represented in terms of a dependency graph. 
138 

6.7 THE LOVASZ LOCAL LEMMA 
Definition 6.1: A dependency graph for a set of events E 1 , ••• , Ell is a graph G = 
(V, E) such that V = {I, ... , n} and, for i = I, ... , n, event Ei is mutually hldependent 
(~ftheevents{Ej I (i,j)tf-E}. 
We discuss first a special case, the symmetric version of the Lovasz local lemma. which 
is more intuitive and is sufficient for most algorithmic applications. 
Theorem 6.11 [Lovasz Local Lemma]: Let E 1 , ••• , EI/ be a set of events, and assume 
that the follmving hold: 
1. for all i, Pr(Ei) :s p: 
2. the degree (~f the dependency graph given by E 1 , ••• , Ell is bounded by d; 
3. 4dp :s l. 
Then 
pr(n E) > o. 
1=1 
Proof: Let 5 C {I, ... , n}. We prove by induction on s = 0, ... , n - I that, if 151 :s s, 
then for all k tf- 5 we have 
For this expression to be well-defined when 5 is not empty, we need Pr( n jES Ej ) > O. 
The base case s = 0 follows from the assumption that Pr(EtJ :s p. To perform the 
indu_ctive step, we first show that Pr( njES Ej ) > O. This is true when s = L because 
Pr( Ej ) 2: I -
p > O. For s > I, without loss of generality let 5 = {I, 2, ... , s}. Then 
pr(OE) = DPr(E; I DEj ) 
= rl (1 -Pr (E; I n 
Ej ) ) 
1=1 
J=1 
2:nO- 2p»0. 
i=1 
I n obtaining the last line we used the induction hypothesis. 
For the rest of the induction, let 51 = {j E 5 I (k, j) E E} and 52 = 5 - 51· If 52 = 
S then Ek is mutually independent of the events Ei , i E 5, and 
pr( Ek I n Ej ) = Pr(E,J :S p. 
JES 
\\'e continue with the case 1521 < s. It will be helpful to introduce the following nota-
tion. Let Fs be defined by 
139 

THE PROBABILISTIC METHOD 
Fs = nij' 
jES 
and similarly define FSI and FS2 ' Notice that Fs = FSI n FS2 ' 
Applying the definition of conditional probability yields 
Pr(E IF.) = Pr(Ek n Fs) . 
k 
S 
Pr(Fs) 
(6.4) 
Applying the definition of conditional probability to the numerator of (6.4), we obtain 
Pr(Ek n Fs) = Pr(Ek n FSI n FS2 ) 
= Pr(Ek n FSI 
I FS2 ) Pr(FS2 )· 
The denominator can be written as 
Pr(Fs) = Pr(Fsl n FS2 ) 
= Pr(Fsl 
I FS2 ) Pr(FS2 )· 
Canceling the common factor, which we have already shown to be nonzero, yields 
(6.5) 
Note that (6.5) is valid even when 52 = 0. 
Since the probability of an intersection of events is bounded by the probability of 
anyone of the events and since Ek is independent of the events in 52, we can bound 
the numerator of (6.5) by 
Because 1521 < 151 = S, we can apply the induction hypothesis to 
Pr(Ei I Fs,) = pr( Ei I n Ej). 
j ES 2 
Using also the fact that 1511 :::; d, we establish a lower bound on the denominator of 
(6.5) as follows: 
Pr(l"l IF,,) = pr( n Ei I n Ej ) 
iESI 
jES2 
:>: 1- LPr(Ei In Ej ) 
iESI 
JESe 
2: 1- 2pd 
1 
> -
-
2 
140 

6.7 THE LOVASZ LOCAL LEMMA 
Using the upper bound for the numerator and the lower bound for the denominator, 
we prove the induction: 
p 
:s I72 = 2p. 
The theorem follows from 
pr( rF) = fI pr( Ei I n 
Ej ) 
1=1 
1=1 
J=1 
= fI (I - Pr (Ei I n 
Ej ) ) 
1=1 
J=1 
11 
2:n(l-2p) >0. 
• 
i=1 
6.7.1. Application: Edge-Disjoint Paths 
.-\ssume that n pairs of users need to communicate using edge-disjoint paths on a given 
network. Each pair i = 1, ... , n can choose a path from a collection Fi of m paths. We 
"how using the Lovasz local lemma that, if the possible paths do not share too many 
edges, then there is a way to choose 11 edge-disjoint paths connecting the 11 pairs. 
Theorem 6.12: If any path in Fi shares edges with no more than k paths ill Fj, ~vhere 
i ~ j and 8nk/m :s I, then there is a way to choose 11 edge-disjoint paths connecting 
rhe II pairs. 
Proof· Consider the probability space defined by each pair choosing a path indepen-
Jently and uniformly at random from its set of m paths. Define Ei.j to represent the 
c\ent that the paths chosen by pairs i and j share at least one edge. Since a path in Fi 
,hares edges with no more than k paths in Fj, 
k 
p = Pre£. .) < -. 
I.} 
-
m 
Let d be the degree of the dependency graph. Since event Ei , j is independent of all 
c\ ents Ei"j' when i' ¢:. {i, j} and j' ¢:. {i, j}, we have d < 211. Since 
8nk 
4dp < -:s 1, 
m 
.. 11 I of the conditions of the Lovasz local lemma are satisfied, proving 
pr(n Ei'J) > O. 
ifJ 
Hence, there is a choice of paths such that the n paths are edge disjoint. 
141 
• 

THE PROBABILISTIC METHOD 
6.7.2. Application: Satisfiability 
As a second example, we return to the satisfiability question. For the k-satisfiability 
(k-SAT) problem, the formula is restricted so that each clause has exactly k literals. 
Again, we assume that no clause contains both a literal and its negation, as these clauses 
are trivial. We prove that any k-SAT formula in which no variable appears in too many 
clauses has a satisfying assignment. 
Theorem 6.13: If no variable in a k-SAT formula appears in more than T = 2kj4k 
clauses, then the formula has a satisfying assignment. 
Proof: Consider the probability space defined by giving a random assignment to the 
variables. For i = I, ... , m, let Ei denote the event that the ith clause is not satisfied 
by the random assignment. Since each clause has k literals, 
The event Ei is mutually independent of all of the events related to clauses that do 
not share variables with clause i. Because each of the k variables in clause i can appear 
in no more than T = 2kj4k clauses, the degree of the dependency graph is bounded by 
d :s kT :s 2k-2. 
In this case, 
so we can apply the Lovasz local lemma to conclude that 
pr(n E) > 0; 
1=1 
hence there is a satisfying assignment for the formula. 
• 
6.8. * Explicit Constructions Using the Local Lemma 
The Lovasz local lemma proves that a random element in an appropriately defined 
sample space has a nonzero probability of satisfying our requirement. However, this 
probability might be too small for an algorithm that is based on simple sampling. The 
number of objects that we need to sample before we find an element that satisfies our 
requirements might be exponential in the problem size. 
In a number of interesting applications, the existential result of the Lovasz local 
lemma can be used to derive efficient construction algorithms. Although the details 
differ in the specific applications, all the known algorithms are based on a common 
two-phase scheme. In the first phase, a subset of the variables of the problem are as-
signed random values; the remaining variables are deferred to the second stage. The 
subset of variables that are assigned values in the first stage is chosen so that 
142 

6.8* EXPLICIT CONSTRUCTIONS USING THE LOCAL LEMMA 
1. using the local lemma, one can show that the random partial solution fixed in the 
first phase can be extended to a full solution of the problem without modifying any 
of the variables fixed in the first phase; and 
2. the dependency graph H between events defined by the variables deferred to the 
second phase has, with high probability, only small connected components. 
When the dependency graph consists of connected components, a solution for the 
variables of one component can be found independently of the other components. Thus, 
the first phase of the two-phase algorithm breaks the original problem into smaller sub-
problems. Each of the smaller subproblems can then be solved independently in the 
second phase by an exhaustive search. 
6.8.1. Application: A Satisfiability Algorithm 
We demonstrate this technique in an algorithm for finding a satisfying assignment for 
a k-SAT formula. The explicit construction result will be significantly weaker than the 
existence result proven in the previous section. In particular. \\e obtain a polynomial 
time algorithm only for the case when k is a constant. This result is still interesting, 
since for k 2: 3 the problem of k-satisfiability is NP-complete. For notational conve-
nience we treat here only the case where k is an even constant: the case \\here k is an 
odd constant is similar. 
Consider a k-SAT formula F, with k an even constant. such that each \ariable ap-
pears in no more than T = 2fYk clauses for some constant Q' > () determined in the 
proof. Let XI, ••• , XI be the t variables and C I , ... , CII1 the 111 clauses of F. 
Following the outline suggested in Section 6.8, our algorithm for finding a satisfy-
ing assignment for F has two phases. Some of the variables are fixed at the first phase. 
and the remaining variables are deferred to the second phase. While executing the first 
phase, we call a clause Ci dangerous if both the following conditions hold: 
1. kj2 literals of the clause Ci have been fixed; and 
2. Ci is not yet satisfied. 
Phase I can be described as follows. Consider the variables XI • ...• X I sequentially. 
If Xi is not in a dangerous clause. assign it independently and uniformly at random a 
value in {O, I}. 
A clause is a surviving clause if it is not satisfied by the variables fixed in phase I. 
Note that a surviving clause has no more than kj2 of its variables fixed in the first 
phase. A deferred variable is a variable that was not assigned a value in the first phase. 
In phase II. we use exhaustive search in order to assign values to the deferred variables 
and so complete a satisfying assignment for the formula. 
In the next two lemmas we show that 
1. the partial solution computed in phase I can be extended to a full satisfying assign-
ment of F, and 
2. with high probability, the exhaustive search in phase II is completed in time that is 
polynomial in 111 • 
143 

THE PROBABILISTIC METHOD 
Lemma 6.14: There is an assignment of values to the deferred variables such that all 
the surviving clauses are satisfied. 
Proof: Let H = (V, E) be a graph on m nodes, where V = {I, ... , m}, and let (i, j) E 
E if and only if Ci n Cj 
=I=- 0. That is, H is the dependency graph for the original prob-
lem. Let H' = (V', E') be a graph with V' S; V and E' S; E such that (a) i E Vi if 
and only if Ci is a surviving clause and (b) (i, j) EE' if and only if Ci and Cj share 
a deferred variable. In the following discussion we do not distinguish between node i 
and clause i. 
Consider the probability space defined by assigning a random value in {O, I} in-
dependently to each deferred variable. The assignment of values to the nondeferred 
variables in phase I, together with the random assignment of values to the deferred vari-
ables, defines an assignment to all the f variables. For i = I, ... , m, let Ei be the event 
that surviving clause C· is not satisfied by this assignment. Associate the event Ei with 
node i in V'. The graph H I is then the dependency graph for this set of events. 
A surviving clause has at least k /2 deferred variables, so 
A variable appears in no more than T clauses; therefore, the degree of the dependency 
graph is bounded by 
For a sufficiently small constant ex > 0, 
and so, by the Lovasz local lemma, there is an assignment for the deferred variables 
that - together with the assignment of values to variables in phase I - satisfies the 
formula. 
• 
The assignment of values to a subset of the variables in phase I partitions the problem 
into as many as m independent subformulas, so that each deferred variable appears in 
only one subformula. The subformulas are given by the connected components of H'. 
If we can show that each connected component in H' has size O(log m), then each sub-
formula will have no more than O(k log m) deferred variables. An exhaustive search 
of all the possible assignments for all variables in each subformula can then be done in 
polynomial time. Hence we focus on the following lemma. 
Lemma 6.15: All connected components in H I are of size O( log m) with probability 
1 -
0(1). 
Proof: Consider a connected component R of r vertices in H. If R is a connected com-
ponent in H I, then all its r nodes are surviving clauses. A surviving clause is either 
a dangerous clause or it shares at least one deferred variable with a dangerous clause 
(i.e., it has a neighbor in H I that is a dangerous clause). The probability that a given 
144 

6.8* EXPLICIT CONSTRUCTIONS USING THE LOCAL LEMMA 
clause is dangerous is at most 2~k/2, since exactly kj2 of its variables were given ran-
dom values in phase I yet none of these values satisfied the clause. The probability 
that a given clause survives is the probability that either this clause or at least one of its 
direct neighbors is dangerous, which is bounded by 
where again d = kT > 1. 
If the survival of individual clauses were independent events then we would be in 
excellent shape. However, from our description here it is evident that such events are 
not independent. Instead, we identify a subset of the vertices in R such that the sur-
vival of the clauses represented by the vertices of this subset are independent events. 
A 4-tree S of a connected component R in H is defined as follows: 
1. S is a rooted tree; 
2. any two nodes in S are at distance at least 4 in H: 
3. there can be an edge in S only between two nodes with distance exactly 4 between 
them in H: 
4. any node of R is either in S or is at distance 3 or less from a node in S. 
Considering the nodes in a 4-tree proves useful because the e\"ent that a node u in 
a 4-tree survives and the event that another node L' in a 4-tree survives are actually in-
dependent. Any clause that could cause II to survive has distance at least :2 from any 
clause that could cause v to survive. Clauses at distance 2 share no variables. and hence 
the events that they are dangerous are independent. We can take advantage of this inde-
pendence to conclude that, for any 4-tree S, the probability that the nodes in the 4-tree 
survive is at most 
A maximal 4-tree S of a connected component R is the 4-tree with the largest pos-
sible number of vertices. Since the degree of the dependency graph is bounded by d, 
there are no more than 
d + d(d -
1) + d(d -
I)(d -
1) :s d 3 -
1 
nodes at distance 3 or less from any given vertex. We therefore claim that a maximal 
4-tree of R must have at least rjd 3 vertices. Otherwise, when we consider the vertices 
of the maximal 4-tree S and all neighbors within distance 3 or less of these vertices, 
we obtain fewer than r vertices. Hence there must be a vertex of distance at least 4 
from all vertices in S. If this vertex has distance exactly 4 from some vertex in S, then 
it can be added to S and thus S is not maximal, yielding a contradiction. If its distance 
is larger than 4 from all vertices in S, consider any path that brings it closer to S; such 
a path must eventually pass through a vertex of distance at least 4 from all vertices in 
S and of distance 4 from some vertex in S, again contradicting the maximality of S. 
To show that with probability I -
0(1) there is no connected component R of size 
r ~ c log2 m for some constant c in H', we show that there is no 4-tree of H of size 
rjd 3 that survives with probability I -
0(1). Since a surviving connected component 
145 

THE PROBABILISTIC METHOD 
R would have a maximal 4-tree of size r/d 3, the absence of such a 4-tree implies the 
absence of such a component. 
We need to count the number of 4-trees of size s = r/d 3 in H. We can choose the 
root of the 4-tree in m ways. A tree with root v is uniquely defined by an Eulerian tour 
that starts and ends at v and traverses each edge of the tree twice, once in each direc-
tion. Since an edge of S represents a path of length 4 in H, at each vertex in the 4-tree 
the Eulerian path can continue in as many as d 4 different ways, and therefore the num-
ber of 4-trees of size s = r/d 3 in H is bounded by 
m(d 4 )2s = md 8r/d3 . 
The probability that the nodes of each such 4-tree survive in H I is at most 
((d + 1)2-k / 2)\ = ((d + 1)2-k/2r/d3 . 
Hence the probability that H I has a connected component of size r is bounded by 
md 8r/dJ ((d + 1)2-k/ 2 rid' ::s m2(rk/d' )(8a+2a-I/2) = 0(1) 
for r 2: c log2 m and for a suitably large constant c and a sufficiently small constant 
Ci > 0. 
• 
Thus, we have the following theorem. 
Theorem 6.16: Consider a k-SAT formula with m clauses, where k is an even con-
stant and each variable appears in up to 2 ak clauses for a sufficiently small constant 
Ci > 0. Then there is an algorithm that finds a satisfying assignmentfor the formula in 
expected time that is polynomial in m. 
Proof: As we have described, if the first phase partitions the problem into subformu-
las involving only O(k log m) variables, then a solution can be found by solving each 
subformula exhaustively in time that is polynomial in m. The probability of the first 
phase partitioning the problem appropriately is 1 -
0(1), so we need only run phase I a 
constant number of times on average before obtaining a good partition. The theorem 
follows. 
• 
6.9. Lovasz Local Lemma: The General Case 
For completeness we include the statement and proof of the general case of the Lovasz 
local lemma. 
Theorem 6.17: Let E I , " ., E/1 be a set of events in an arbitrary probability space, 
and let G = (V, E) be the dependency graph for these events. Assume there exist 
XI •.. . ,X/1 E [0, I] such that, for all 1 :s i :s n, 
Pr(Ei ) :s Xi n (1 -
Xj). 
(i.j)EE 
146 

6.9 LOVASZ LOCAL LEMMA: THE GENERAL CASE 
Then 
Proof: Let 5 S; {I, ... ,11}. We prove by induction on s = 0, ... ,11 that, if 151 :s .'I, then 
for all k we have 
A.s in the case of the symmetric version of the local lemma, we must be careful that the 
conditional probability is well-defined. This follows using the same approach as in the 
symmetric case, so we focus on the rest of the induction. 
The base case s = ° 
follows from the assumption that 
Pr(Ed :s x" n (1 -
Xj) :s Xk· 
(k,j)EE 
For the inductive step, let 51 = {j E 5 
1 (k, j) E E} and 52 = 5 -
51. If 52 = 5 
then Ek is mutually independent of the events Ei , i E 5, and 
pr( Ek I n Ei) = Pr(Ekl <: Xk· 
jES 
\\'e continue with the case 1521 < s. We again use the notation 
Fs=nEj 
jES 
J.nd define FSJ and F'l2 similarly, so that Fs = F'lJ n FS2 . 
Applying the definition of conditional probability yields 
Pr(E 
1 F ) = Pr(E" n Fs) . 
k 
S 
Pr(Fs) 
(6.6) 
B~ once again applying the definition of conditional probability, the numerator of (6.6) 
.. ·.in be written as 
JnJ the denominator as 
CJ.nceling the common factor then yields 
Pr(E" 
1 F:J = Pr(E" n FSJ 
1 Fs2). 
Pre FSJ 
1 FS2 ) 
(6.7) 
Since the probability of an intersection of events is bounded by the probability of 
;: •. lI:h of the events and since E" is independent of the events in 52. we can bound the 
numerator of (6.7) by 
Pr(Ek n FSJ 
1 F S2 ) :s Pr(E" 
1 F S2 ) = Pr(Ek ) :s x" n (1 -
Xj). 
(".j)EE 
147 

THE PROBABILISTIC METHOD 
To bound the denominator of (6.7), let 51 = UI, ... , jr}. Applying the induction hy-
pothesis, we have 
Pr(Fs, I Fs,) = pr( n Ej I n Ej ) 
jESI 
jES2 
i=1 
~ n (1- Xj). 
(f.:,jJEE 
U sing the upper bound for the numerator and the lower bound for the denominator, 
we can prove the induction hypothesis: 
pr( Ek I n E;) = Pr(Ek I Fs) 
jES 
Pr(Ek n PSI 
I FS2 ) 
Pr(Fsl 
I FS2 ) 
< Xf.: n(f.:,j)EE(1 -
Xj) 
n(k,j)EE(1 -
Xj) 
= Xk· 
The theorem now follows from: 
6.10. Exercises 
f1 
Pr(£I,"" £1/) = n 
Pr(Ei I £1,"" Ei- I ) 
i=1 
f1 
= n 
(1 - Pr(Ei I £1, ... , £i-I)) 
i=1 
11 
~n(l-Xi»O. 
i=1 
• 
Exercise 6.1: Consider an instance of SAT with m clauses, where every clause has 
exactly k literals. 
(a) Give a Las Vegas algorithm that finds an assignment satisfying at least m(1 - 2-k ) 
clauses, and analyze its expected running time. 
(b) Give a derandomization of the randomized algorithm using the method of condi-
tional expectations. 
148 

6.10 EXERCISES 
Exercise 6.2: (a) Prove that, for every integer 11, there exists a coloring of the edges of 
the complete graph KIl by two colors so that the total number of monochromatic copies 
of K4 is at most (~)2~5. 
(b) Give a randomized algorithm for finding a coloring with at most C)2~5 mono-
chromatic copies of K 4 that runs in expected time polynomial in n. 
(c) Show how to construct such a coloring deterministically in polynomial time using 
the method of conditional expectations. 
Exercise 6.3: Given an n-vertex undirected graph G = (V, E), consider the follow-
ing method of generating an independent set. Given a permutation a of the vertices, 
define a subset S(a) of the vertices as follows: for each vertex i, i E S(a) if and only 
if no neighbor j of i precedes i in the permutation a. 
(a) Show that each Sea) is an independent set in G. 
(b) Suggest a natural randomized algorithm to produce a for which you can show that 
the expected cardinality of S(a) is 
11 
1 
L d + I ' 
i=l 
I 
where di denotes the degree of vertex i. 
(c) Prove that G has an independent set of size at least L:1=11/(di + 1). 
Exercise 6.4: Consider the following two-player game. The game begins with k to-
kens placed at the number 0 on the integer number line spanning lO,I1J. Each round, 
une player, called the chooser, selects two disjoint and nonempty sets of tokens A and 
B. (The sets A and B need not cover all the remaining tokens; they only need to be 
disjoint.) The second player, called the remover, takes all the tokens from one of the 
,ets off the board. The tokens from the other set all move up one space on the num-
ber line from their current position. The chooser wins if any token ever reaches n. The 
remover wins if the chooser finishes with one token that has not reached n. 
t a) Give a winning strategy for the chooser when k ~ 211. 
t h) Use the probabilistic method to show that there must exist a winning strategy for 
the remover when k < 211. 
t c) Explain how to use the method of conditional expectations to derandomize the 
winning strategy for the remover when k < 211. 
Exercise 6.5: We have shown using the probabilistic method that. if a graph G has n 
:1\ )des and 171 edges, then there exists a partition of the n nodes into sets A and B such 
:hat at least 171/2 edges cross the partition. Improve this result slightly: show that there 
-.'\i'its a partition such that at least 17111/(211 -
I) edges cross the partition. 
Exercise 6.6: We can generalize the problem of finding a large cut to finding a large 
.. -I.'ut. A k-cut is a partition of the vertices into k disjoint sets, and the value of a cut is 
::le weight of all edges crossing from one of the k sets to another. In Section 6.2.1 we 
149 

THE PROBABILISTIC METHOD 
considered 2-cuts when all edges had the same weight I, showing via the probabilistic 
method that any graph G with m edges has a cut with value at least m/2. Generalize 
this argument to show that any graph G with m edges has a k-cut with value at least 
(k - l)m/ k. Show how to use derandomization (following the argument of Section 6.3) 
to give a deterministic algorithm for finding such a cut. 
Exercise 6.7: A hypergraph H is a pair of sets (V, E), where V is the set of vertices 
and E is the set of hyperedges. Every hyperedge in E is a subset of V. In particular, an 
r-uniform hypergraph is one where the size of each edge is r. For example, a 2-uniform 
hypergraph is just a standard graph. A dominating set in a hypergraph H is a set of 
vertices S C V such that enS =I=- '" for every edge e E E. That is, S hits every edge of 
the hypergraph. 
Let H = (V, E) be an r-uniform hypergraph with n vertices and m edges. Show 
that there is a dominating set of size at most np + (1- p)"m for every real number 0 :s 
p :s l. Also, show that there is a dominating set of size at most (m + n In r)/r. 
Exercise 6.8: Prove that, for every integer n, there exists a way to 2-color the edges 
of Kr so that there is no monochromatic clique of size k when 
(Hint: Start by 2-coloring the edges of K n , then fix things up.) 
Exercise 6.9: A tournament is a graph on n vertices with exactly one directed edge be-
tween each pair of vertices. If vertices represent players, then each edge can be thought 
of as the result of a match between the two players: the edge points to the winner. A 
ranking is an ordering of the n players from best to worst (ties are not allowed). Given 
the outcome of a tournament, one might wish to determine a ranking of the players. A 
ranking is said to disagree with a directed edge from y to x if y is ahead of x in the 
ranking (since x beat y in the tournament). 
(a) Prove that, for every tournament, there exists a ranking that disagrees with at most 
50% of the edges. 
(b) Prove that, for sufficiently large n, there exists a tournament such that every rank-
ing disagrees with at least 49% of the edges in the tournament. 
Exercise 6.10: A family of subsets F of {I, 2, ... , n} is called an antichain if there is 
no pair of sets A and B in F satisfying A C B. 
(a) Give an example of F where IFI = (In/2J). 
(b) Let fk be the number of sets in F with size k. Show that 
tA < I. 
k=O G) -
150 

6.10 EXERCISES 
(Hint: Choose a random permutation of the numbers from 1 to n, and let X k = 1 
if the first k numbers in your permutation yield a set in F. If X = L~=o X k, what 
can you say about X?) 
(c) Argue that IFI :s (In/2J) for any antichain F. 
Exercise 6.11: In Section 6.5.1, we bounded the variance of the number of 4-cliques in 
a random graph in order to demonstrate the second moment method. Show how to cal-
culate the variance directly by using the equality from Exercise 3.9: for X = L;I=l Xi 
the sum of Bernoulli random variables, 
11 
E [X 2] = L Pr ( Xi = I) E [X 
I XI = I]. 
i=l 
Exercise 6.12: Consider the problem of whether graphs in Gil!) have cliques of con-
stant size k. Suggest an appropriate threshold function for this property. Generalize 
the argument used for cliques of size 4, using either the second moment method or the 
conditional expectation inequality, to prove that your threshold function is correct for 
cliques of size 5. 
Exercise 6.13: Consider a graph in GII.f!' with p = c In n /n. C se the second moment 
method or the conditional expectation inequality to prove that if c < I then. for any 
I..'onstant 8 > 0 and for 11 sufficiently large, the graph has isolated \ertices with proba-
hility at least 1 -
8. 
Exercise 6.14: Consider a graph in GI/.f!' with p = I/n. Let X he the number of tri-
angles in the graph, where a triangle is a clique with three edges. Show that 
Pr (X ~ 1) :s 1/6 
and that 
lim Pr(X ~ 1) ~ 1/7. 
1/---+ X 
I Hint: Use the conditional expectation inequality.) 
Exercise 6.15: Consider the set-balancing problem of Section 4.4. \Ve claim that there 
I'; an n x n matrix A for which II Ab II x is Q (~) for any choice of l~. For convenience 
here we assume that n is even. 
la) We have shown in Eqn. (5.5) that 
U sing similar ideas, show that 
for some positive constant a. 
151 

THE PROBABILISTIC METHOD 
(b) Let b],b2, ... ,bm/2 all equall, and let bll1/2+],bm/2+2, ... ,bm all equal -1. Let 
Y], Y2, ... , Y m each be chosen independently and uniformly at random from {a, I}. 
Show that there exists a positive constant c such that, for sufficiently large m, 
pr(ltb;y;1 > cvlm) > ~. 
1=] 
(Hint: Condition on the number of Yi that are equal to 1.) 
(c) Let b],b2, ... ,bm each be equal to either 1 or -1. Let Y],Y2, ... ,Ym each be cho-
sen independently and uniformly at random from {a, I}. Show that there exists a 
positive constant c such that, for sufficiently large m, 
(d) Prove that there exists a matrix A for which IIAbll oo is n(Jfl) for any choice of b. 
Exercise 6.16: Use the Lovasz local lemma to show that, if 
4(k)( n )2]-(3) < 1 
2 
k-2 
-
, 
then it is possible to color the edges of Kn with two colors so that it has no monochro-
matic Kk subgraph. 
Exercise 6.17: Use the general form of the Lovasz local lemma to prove that the sym-
metric version of Theorem 6.11 can be improved by replacing the condition 4dp :s 1 
by the weaker condition ep(d + 1) :s 1. 
Exercise 6.18: Let G = (V, E) be an undirected graph and suppose each v E V is as-
sociated with a set S( v) of 8r colors, where r ~ I. Suppose, in addition, that for each 
v E V and c E S(v) there are at most r neighbors u of v such that c lies in S(u). Prove 
that there is a proper coloring of G assigning to each vertex v a color from its class 
S (v) such that, for any edge (it, v) E E, the colors assigned to u and v are different. 
You may want to let A Il • l •. C be the event that u and v are both colored with color c and 
then consider the family of such events. 
152 

CHAPTER SEVEN 
Markov Chains and Random Walks 
Markov chains provide a simple but powerful framework for modeling random pro-
cesses. We start this chapter with the basic definitions related to Markov chains and 
then show how Markov chains can be used to analyze simple randomized algorithms 
for the 2-SAT and 3-SAT problems. Next we study the long-term behayior of Markov 
chains, explaining the classifications of states and conditions for conyergence to a sta-
tionary distribution. We apply these techniques to analyzing simple gambling schemes 
and a discrete version of a Markovian queue. Of special interest is the limiting be-
havior of random walks on graphs. We prove bounds on the covering time of a graph 
and use this bound to develop a simple randomized algorithm for the s-t connectiv-
ity problem. Finally, we apply Markov chain techniques to resolve a subtle probability 
problem known as Parrondo's paradox. 
7.1. Markov Chains: Definitions and Representations 
.-\ stochastic process X = {X(t) : t E T} is a collection of random variables. The index 
r often represents time, and in that case the process X models the value of a random 
\ariable X that changes over time. 
We call X(t) the state of the process at time t. In what follows, we use XI inter-
-:hangeably with X(t). If, for all t, XI assumes values from a countably infinite set, then 
\\e say that X is a discrete space process. If XI assumes values from a finite set then the 
process isfinite. If T is a countably infinite set we say that X is a discrete time process. 
In this chapter we focus on a special type of discrete time, discrete space stochastic 
process Xo, XI, X 2 , ... in which the value of Xf depends on the yalue of X I _ I but not 
\)n the sequence of states that led the system to that value. 
Definition 7.1: A discrete time stochastic process X 0, X I, X 2, ... is ([ Markov chain (f I 
Pr( X t = at I Xt- I = at-I, Xt- 2 = a t -2,···, Xu = ao) = Pr( XI = ([I I X t - I = at-I) 
= Pal_l.ill· 
'1trictly speaking. this is a time-homogeneous Markov chain; this will be the only type we study in this book. 
153 

MARKOV CHAINS AND RANDOM WALKS 
This definition expresses that the state X t depends on the previous state X t - I but is 
independent of the particular history of how the process arrived at state X t-I. This is 
called the Markov property or memoryless property, and it is what we mean when we 
say that a chain is Markovian. It is important to note that the Markov property does not 
imply that X t is independent of the random variables Xo, XI, ... , X t - 2 ; it just implies 
that any dependency of X t on the past is captured in the value of Xt-I. 
Without loss of generality, we can assume that the discrete state space of the Mar-
kov chain is {O, 1, 2, ... , n} (or {O, 1, 2, ... } if it is countably infinite). The transition 
probability 
Pi. j = Pr(X t = j 
I X t - I = i) 
is the probability that the process moves from i to j in one step. The Markov property 
implies that the Markov chain is uniquely defined by the one-step transition matrix: 
PO.O 
PO. I 
po.j 
PI.() 
Pl.l 
Pl. j 
p= 
Pi.O Pi. 1 
p .. 
I.J 
That is, the entry in the ith row and jth column is the transition probability Pi.j . It 
follows that, for all i, Lj:::o Pi.j = 1. 
This transition matrix representation of a Markov chain is convenient for computing 
the distribution of future states of the process. Let Pi(t) denote the probability that the 
process is at state i at time t. Let fJ (t) = (Po (t), PI (t), P2 (t), ... ) be the vector giving 
the distribution of the state of the chain at time t. Summing over all possible states at 
time t - 1, we have 
Pi(t) = L Pj(t -
l)Pj.i 
j:::O 
i5(t) = p(t - 1)P. 
We represent the probability distribution as a row vector and multiply pP instead 
of Pp to conform with the interpretation that starting with a distribution p(t - 1) and 
applying the operand P, we arrive at the distribution pet). 
For any m ~ 0, we define the m-step transition probability 
plII = Pr ( X t+1I1 = j I X t = i) 
I. J 
as the probability that the chain moves from state i to state j in exactly m steps. 
Conditioning on the first transition from i, we have 
P
ili 
LP p m - I 
.. = 
i.k 
, ... 
1 • .1 
". J 
k:::O 
Operations on vectors are generalized to a countable number of elements in the natural way. 
154 
(7.1) 

7.1 MARKOV CHAINS: DEFINITIONS AND REPRESENTATIONS 
Figure 7.1: A Markov chain (left) and the corresponding transition matrix (right). 
Let p(m) be the matrix whose entries are the Ill-step transition probabilities, so that the 
entry in the ith row and jth column is pm. Then, applying Eqn. (7.1) yields 
I. } 
by induction on m, 
plm) = pm. 
Thus, for any t 2: 0 and m 2: I, 
Another useful representation of a Markov chain is by a directed, weighted graph 
D = (V, E, w). The set of vertices of the graph is the set of states of the chain. There 
is a directed edge (i, j) E E if and only if Pi . j > 0, in which case the weight w(i, j) 
of the edge (i, j) is given by w(i, j) = Pi. j . Self-loops, where an edge starts and ends 
at the same vertex, are allowed. Again, for each i we require that L i: ( i. j lEE U 1 (i , j) = 
1. A sequence of states visited by the process is represented by a directed path on the 
graph. The probability that the process follows this path is the product of the weights 
of the path's edges. 
Figure 7.1 gives an example of a Markov chain and the correspondence between 
the two representations. Let us consider how we might calculate with each represen-
tation the probability of going from state 0 to state 3 in exactly three steps. With the 
graph, we consider all the paths that go from state 0 to state 3 in exactly three steps. 
There are only four such paths: 0-1-0-3,0-1-3-3,0-3-1-3, and 0-3-3-3. The prob-
abilities that the process follows each of these paths are 3/32,1/96,1/16, and 3/64, 
respectively. Summing these probabilities, we find that the total probability is 41/192 . 
. -\lternatively, we can simply compute 
[
3/16 
p3 = 
5/;8 
1/16 
7/48 
5/24 
o 
13/96 
29/64 
79/144 
1 
107/192 
41/192] 
5/36 
o 
. 
47/192 
The entry P6.3 = 41/192 gives the correct answer. The matrix is also helpful if we want 
to know the probability of ending in state 3 after three steps when we begin in a state 
.:hosen uniformly at random from the four states. This can be computed by calculating 
155 

MARKOV CHAINS AND RANDOM WALKS 
0/4,1/4,1/4, 1/4)P3 = 07/192,47/384,737/1152,43/288); 
here the last entry, 43/288, is the required answer. 
7.1.1. Application: A Randomized Algorithm for 2-Satisfiability 
Recall from Section 6.2.2 that an input to the general satisfiability (SAT) problem is a 
Boolean formula given as the conjunction (AND) of a set of clauses, where each clause 
is the disjunction (OR) of literals and where a literal is a Boolean variable or the nega-
tion of a Boolean variable. A solution to an instance of a SAT formula is an assignment 
of the variables to the values True (T) and False (F) such that all the clauses are sat-
isfied. The general SAT problem is NP-hard. We analyze here a simple randomized 
algorithm for 2-SAT, a restricted case of the problem that is solvable in polynomial 
time. 
For the k-satisfiability (k-SAT) problem, the satisfiability formula is restricted so 
that each clause has exactly k literals. Hence an input for 2-SAT has exactly two literals 
per clause. The following expression is an instance of 2-SAT: 
One natural approach to finding a solution for a 2-SAT formula is to start with an 
assignment, look for a clause that is not satisfied, and change the assignment so that 
the clause becomes satisfied. If there are two literals in the clause, then there are two 
possible changes to the assignment that will satisfy the clause. Our 2-SAT algorithm 
(Algorithm 7.1) decides which of these changes to try randomly. In the algorithm, n 
denotes the number of variables in the formula and In is an integer parameter that de-
termines the probability that the algorithm terminates with a correct answer. 
In the instance given in (7.2), if we begin with all variables set to False then the 
clause (Xl v X2) is not satisfied. The algorithm might therefore choose this clause and 
then select Xl to be set to True. In this case the clause (X4 v XI) would be unsatisfied 
and the algorithm might switch the value of a variable in that clause, and so on. 
If the algorithm terminates with a truth assignment, it clearly returns a correct an-
swer. The case where the algorithm does not find a truth assignment requires some 
care, and we will return to this point later. Assume for now that the formula is satis-
fiable and that the algorithm will actually run as long as necessary to find a satisfying 
truth assignment. 
We are mainly interested in the number of iterations of the while-loop executed by 
the algorithm. We refer to each time the algorithm changes a truth assignment as a step. 
Since a 2-SAT formula has 0(112) distinct clauses, each step can be executed in 0(n2) 
time. Faster implementations are possible but we do not consider them here. Let S 
represent a satisfying assignment for the 11 variables and let Ai represent the variable 
assignment after the ith step of the algorithm. Let Xi denote the number of variables in 
the current assignment Ai that have the same value as in the satisfying assignment S. 
When Xi = n, the algorithm terminates with a satisfying assignment. In fact, the algo-
rithm could terminate before Xi reaches 11 if it finds another satisfying assignment, but 
for our analysis the worst case is that the algorithm only stops when Xi = 11. Starting 
156 

7.1 MARKOV CHAINS: DEFINITIONS AND REPRESENTATIONS 
2-SAT Algorithm: 
1. Start with an arbitrary truth assignment. 
2. Repeat up to 2mn 2 times, terminating if all clauses are satisfied: 
(a) Choose an arbitrary clause that is not satisfied. 
(b) Choose uniformly at random one of the literals in the clause and switch 
the value of its variable. 
3. If a valid truth assignment has been found. return it. 
4. Otherwise, return that the formula is unsatisfiable. 
Algorithm 7.1: 2-SAT algorithm, 
with Xi < n, we consider how Xi evolves over time. and in particular how long it takes 
before Xi reaches n. 
First, if Xi = 0 then, for any change in variable value on the next step, we have 
Xi+ 1 = l. Hence 
Pr ( Xi + I = I I Xi = 0) = 1. 
Suppose now that 1 :s Xi :s n -
1. At each step. we choose a clause that is unsat-
isfied. Since S satisfies the clause, that means that A i and 5 disagree on the value of 
at least one of the variables in this clause. Because the clause has no more than two 
variables, the probability that we increase the number of matches is at least 1/2: the 
probability that we increase the number of matches could be 1 if we are in the case 
where Ai and S disagree on the value of both variables in this clause. It follows that 
the probability that we decrease the number of matches is at most 1/2. Hence, for 1 :s 
j :s n -
1, 
Pr(Xi+ 1 = j + 1 I Xi = j) 2: 1/2: 
Pr(Xi+ 1 = j - 1 I Xi = j) :s 1/2. 
The stochastic process Xo, XI, X 2 , .•. is not necessarily a Markov chain, since the 
probability that Xi increases could depend on whether Ai and S disagree on one or 
two variables in the unsatisfied clause the algorithm chooses at that step. This. in turn, 
might depend on the clauses that have been considered in the past. Howe\'er. consider 
the following Markov chain Yo, YI , Y2 , ... : 
Yo = Xo: 
Pr ( }j + I = 1 I }j = 0) = 1: 
Pr ( }j + I = j + 1 I }j = j) = 1/2: 
Pr (}j + I = j - 1 I }j = j) = 1/2. 
The Markov chain Yo, YI , Y2 , . .. is a pessimistic version of the stochastic process 
.\11. Xl, X 2 , ... in that, whereas Xi increases at the next step with probability at least 
I 2.}j increases with probability exactly 1/2. It is therefore clear that the expected 
time to reach n starting from any point is larger for the Markov chain Y than for the 
157 

MARKOV CHAINS AND RANDOM WALKS 
process X, and we use this fact hereafter. (A stronger formal framework for such ideas 
is developed in Chapter 11.) 
This Markov chain models a random walk on an undirected graph G. (We elabo-
rate further on random walks in Section 7.4.) The vertices of G are the integers 0, ... ,n 
and, for 1 :s i :s n - 1, node i is connected to node i-I and node i + 1. Let hi be the 
expected number of steps to reach n when starting from j. For the 2-SAT algorithm, 
hj is an upper bound on the expected number of steps to fully match S when starting 
from a truth assignment that matches S in j locations. 
Clearly, h/1 = 0 and ho = hi + 1, since from ho we always move to hi in one step. 
We use linearity of expectations to find an expression for other values of hj . Let Zj be 
a random variable representing the number of steps to reach n from state j. Now con-
sider starting from state j, where I :s j :s n -
l. With probability 1/2, the next state 
is j -
1, and in this case Zj = 1 + Zj_l. With probability 1/2, the next step is j + 1, 
and in this case Zj = 1 + Zj+l. Hence 
But E[Zj] = hj and so, by applying the linearity of expectations, we obtain 
h j _ I + 1 
h j + I + I 
hi -I 
h j + I 
hj = 
+ 
= -
+ -
+ I. 
2 
2 
2 
2 
We therefore have the following system of equations: 
hn = 0; 
hj _ 1 
hj + 1 
hj = -2- + -2- + 1, 
1:s j :s n -
1; 
ho=hl+l. 
We can show inductively that, for 0 :s j :s 11 -
1, 
It is true when j = 0, since hi = ho -
1. For other values of j, we use the equation 
to obtain 
hj _ 1 
hj + 1 
h·=-+-+l 
.I 
2 
2 
hj + 1 = 2hi - hj _ 1 -
2 
= 2hj -
(hj + 2(j -
1) + 1) - 2 
= hj - 2j -
1, 
using the induction hypothesis in the second line. We can conclude that 
/1-1 
ho = hi + I = h2 + 1 + 3 = ... = L 2i + 1 = n 2. 
i=O 
158 

7.1 MARKOV CHAINS: DEFINITIONS AND REPRESENTATIONS 
An alternative approach for solving the system of equations for the hj is to guess 
and verify the solution hj = n 2 -
j2. The system has n + 1 linearly independent equa-
tions and n + 1 unknowns, and hence there is a unique solution for each value of n. 
Therefore, if this solution satisfies the foregoing equations then it must be correct. We 
have hn = O. For 1 ~ j ~ n -
1, we check 
n 2 - (j -
1)2 
n 2 - (j + 1)2 
hj = 
2 
+ 
2 
+ 1 
2 
·2 
= n - J 
and 
ho = (n 2 -
1) + 1 
Thus we have proven the following fact. 
Lemma 7.1: Assume that a 2-SATformula with n variables has a sati,~fying assignment 
and that the 2-SAT algorithm is allowed to run until itfinds a satisfying assignment. 
Then the expected number of steps until the algorithm finds an assignment is at most n 2. 
We now return to the issue of dealing with unsatisfiable formulas by forcing the algo-
rithm to stop after a fixed number of steps. 
Theorem 7.2: The 2-SAT algorithm always returns a correct anS~l'e,. zfthefo17nula is 
IInsatisfiable. If the formula is sati!'Jfiable, then with probability at least 1 - 2-
111 the 
algorithm returns a sati!'J:fying assignment. Othenrise. it incorrectly returns that the 
tc)rmula is unsatisfiable. 
Proof' It is clear that if there is no satisfying assignment then the algorithm correctly 
returns that the formula is unsatisfiable. Suppose the formula is satisfiable. Divide the 
execution of the algorithm into segments of 2n 2 steps each. Given that no satisfying as-
"ignment was found in the first i - 1 segments, what is the conditional probability that 
the algorithm did not find a satisfying assignment in the ith segment? By Lemma 7.1, 
the expected time to find a satisfying assignment, regardless of its starting position, 
\'- bounded by n 2. Let Z be the number of steps from the start of segment i until the 
.dgorithm finds a satisfying assignment. Applying Markov's inequality. 
n2 
1 
Pr(Z > 2n2) < -
= -. 
-
2n 2 
2 
Thus the probability that the algorithm fails to find a satisfying assignment after 111 seg-
ments is bounded above by 0/2)111. 
• 
7.1.2. Application: A Randomized Algorithm for 3-Satisfiability 
\\'e now generalize the technique used to develop an algorithm for 2-SAT to obtain a 
~.il1domized algorithm for 3-SAT. This problem is NP-complete. so it would be rather 
159 

MARKOV CHAINS AND RANDOM WALKS 
3-SAT Algorithm: 
1. Start with an arbitrary truth assignment. 
2. Repeat up to In times, terminating if all clauses are satisfied: 
(a) Choose an arbitrary clause that is not satisfied. 
(b) Choose one of the literals uniformly at random, and change the value of 
the variable in the current truth assignment. 
3. If a valid truth assignment has been found, return it. 
4. Otherwise, return that the formula is unsatisfiable. 
Algorithm 7.2: 3-SAT algorithm. 
surprising if a randomized algorithm could solve the problem in expected time polyno-
mial in n. 3 We present a randomized 3-SAT algorithm that solves 3-SAT in expected 
time that is exponential in n, but it is much more efficient than the naiVe approach of 
trying all possible truth assignments for the variables. 
Let us first consider the performance of a variant of the randomized 2-SAT algorithm 
when applied to a 3-SAT problem. The basic approach is the same as in the previous 
section; see Algorithm 7.2. In the algorithm, In is a parameter that controls the prob-
ability of success of the algorithm. We focus on bounding the expected time to reach 
a satisfying assignment (assuming one exists), as the argument of Theorem 7.2 can be 
extended once such a bound is found. 
As in the analysis of the 2-SAT algorithm, assume that the formula is satisfiable and 
let S be a satisfying assignment. Let the assignment after i steps of the process be Ai, 
and let Xi be the number of variables in the current assignment Ai that match S. It 
follows from the same reasoning as for the 2-SAT algorithm that, for 1 :s j :s n -
1, 
Pr ( Xi + I = .i + 1 I Xi = .i) ~ 1/3; 
Pr(Xi+ 1 = .i - 1 I Xi = j) :s 2/3. 
These equations follow because at each step we choose an unsatisfied clause, so A i and 
S must disagree on at least one variable in this clause. With probability at least 1/3, we 
increase the number of matches between the current truth assignment and S. Again we 
can obtain an upper bound on the expected number of steps until Xi = n by analyzing 
a Markov chain Yo, YI , ••• such that Yo = Xo and 
Pr ( }j + I = 1 I }j. = 0) = 1, 
Pr ( Yi + I = j + 1 I }j = j) = 1/3, 
Pr ( }j + I = j -
1 I }j. = j) = 2/3 . 
.\ Technically. this would not settle the P = NP question. since we would be using a randomized algorithm and 
not a deterministic algorithm to solve an NP-hard problem. It would. however. have similar far-reaching im-
plications about the ability to solve all NP-complete problems. 
160 

7.1 MARKOV CHAINS: DEFINITIONS AND REPRESENTATIONS 
In this case, the chain is more likely to go down than up. If we let h) be the ex-
pected number of steps to reach n when starting from j, then the following equations 
hold for h): 
hn = 0; 
2hj _ 1 
hj + 1 
h) = -3- + -3- + 1, 
1:s j :s n -
1; 
ho=hl+l. 
Again, these equations have a unique solution, which is given by 
h j = 2 n+2 -
2)+2 - 3(n - j). 
Alternatively, the solution can be found by using induction to prove the relationship 
h) = h)+1 + 2)+2 -
3. 
We leave it as an exercise to verify that this solution indeed satisfies the foregoing 
equations. 
The algorithm just described takes 8(271) steps on average to find a satisfying as-
signment. This result is not very compelling, since there are only 271 truth assignments 
to try! With some insight, however, we can significantly improve the process. There 
are two key observations. 
1. If we choose an initial truth assignment uniformly at random, then the number of 
variables that match S has a binomial distribution with expectation 11 / 2. With an 
exponentially small but nonnegligible probability, the process starts with an initial 
assignment that matches S in significantly more than Il /2 variables. 
2. Once the algorithm starts, it is more likely to move toward 0 than toward 11. The 
longer we run the process, the more likely it has moved toward O. Therefore, we 
are better off restarting the process with many randomly chosen initial assignments 
and running the process each time for a small number of steps, rather than running 
the process for many steps on the same initial assignment. 
Based on these ideas, we consider the modified procedure of Algorithm 7.3. The mod-
ified algorithm has up to 311 steps to reach a satisfying assignment starting from a 
random assignment. If it fails to find a satisfying assignment in 311 steps, it restarts the 
..;earch with a new randomly chosen assignment. We now determine how many times 
the process needs to restart before it reaches a satisfying assignment. 
Let q represent the probability that the modified process reaches 5 (or some other 
"atisfying assignment) in 311 steps starting with a truth assignment chosen uniformly 
at random. Let qj be a lower bound on the probability that our modified algorithm 
reaches S (or some other satisfying assignment) when it starts with a truth assignment 
that includes exactly j variables that do not agree with S. Consider a particle moving 
~m the integer line, with probability 1/3 of moving up by one and probability 2/3 of 
moving down by one. Notice that 
161 

MARKOV CHAINS AND RANDOM WALKS 
Modified 3-SAT Algorithm: 
1. Repeat up to m times, terminating if all clauses are satisfied: 
(a) Start with a truth assignment chosen uniformly at random. 
(b) Repeat the following up to 311 times, terminating if a satisfying 
assignment is found: 
i. Choose an arbitrary clause that is not satisfied. 
ii. Choose one of the literals uniformly at random, and change the value 
of the variable in the current truth assignment. 
2. If a valid truth assignment has been found, return it. 
3. Otherwise, return that the formula is unsatisfiable. 
Algorithm 7.3: Modified 3-SAT algorithm. 
is the probability of exactly k moves down and k + j moves up in a sequence of j + 2k 
moves. It is therefore a lower bound on the probability that the algorithm reaches a 
satisfying assignment within j + 2k :s 311 steps, starting with an assignment that has 
exactly j variables that did not agree with S. That is, 
(
j + 2k)(2)"( I )/+" 
q > 
max 
--
) -
"=0, .... i 
k 
3 
3 
In particular, consider the case where k = j. In that case we have 
In order to approximate C!) we use Stirling's formula, which is similar to the bounds 
.I 
(5.2) and (5.5) we have previously proven for factorials. Stirling's formula is tighter, 
which proves useful for this application. We use the following loose form. 
Lemma 7.3 [Stirling's Formula]: For III > 0, 
Hence, when j > 0, 
(
3j ) 
(3j)! 
j 
-
j!(2j)! 
~ 
J2rr(3j) 
(3j )3i(~)2j(~)i 
4J2rrj /2rr(2j) 
e 
2} 
} 
= 
J3 (27)i 
8;;] 4 
= he:); 
162 

7.2 CLASSIFICATION OF STATES 
for a constant c = v'3/8y0r. Thus, when j > O. 
Also, qo = l. 
qi:> CnGYGr 
:> ~c:n~n~r 
c 
1 
>--
- /J 2)' 
Having established a lower bound for qj, we can now derive a lower bound for q, the 
probability that the process reaches a satisfying assignment in 3n steps when starting 
with a random assignment: 
q 2: L Pr(a random assignment has j mismatches with S) . q) 
)=0 
I 
~ 
(11) ( 1)11 
C 1 
2: 2/1 +f=; j 
:2 /J2) 
:> :nGY tC)GY(I)"-1 
}=o 
=:nGHU 
= :n(~r 
v.here in (7.3) we used L]=o(j)(t))(I)I1-) = (I + ~r. 
(7.3) 
Assuming that a satisfying assignment exists, the number of random assignments 
the process tries before finding a satisfying assignment is a geometric random vari-
jhle with parameter q. The expected number of assignments tried is 1/ q, and for 
;:;.iI:h assignment the algorithm uses at most 3n steps. Thus, the expected number 
~)f "teps until a solution is found is bounded by O(n 3/ 2(4/3)11). As in the case of 2-
SAT (Theorem 7.2), the modified 3-SAT algorithm (Algorithm 7.3) yields a Monte 
Carlo algorithm for the 3-SAT problem. If the expected number of steps until a satis-
f~ 109 solution is found is bounded above by a and if m is set to 2ab. then the prob-
.!hility that no assignment is found when the formula is satisfiable is bounded above 
~~ 2- h . 
7.2. Classification of States 
A fir.;t step in analyzing the long-term behavior of a Markov chain is to classify its 
~(e.;. In the case of a finite Markov chain, this is equivalent to analyzing the connec-
In Hy structure of the directed graph representing the Markov chain. 
163 

MARKOV CHAINS AND RANDOM WALKS 
Definition 7.2: State i is accessible from state j ~f, for some integer n 2: 0, Pt j > 0. 
If two states i and j are accessible from each other, we say that they communicate and 
we write i ++ j. 
In the graph representation of a chain, i ++ j if and only if there are directed paths 
connecting i to j and j to i. 
The communicating relation defines an equivalence relation. That is, the communi-
cating relation is 
1. reflexive - for any state i, i ++ i; 
2. symmetric - if i ++ j then j ++ i; and 
3. transitive - if i ++ j and j ++ k, then i ++ k. 
Proving this is left as Exercise 7.4. Thus, the communication relation partitions the 
states into disjoint equivalence classes, which we refer to as communicating classes. It 
might be possible to move from one class to another, but in that case it is impossible to 
return to the first class. 
Definition 7.3: A Markov chain is irreducible {f all states belong to one communicat-
ing class. 
In other words, a Markov chain is irreducible if, for every pair of states, there is a 
nonzero probability that the first state can reach the second. We thus have the following 
lemma. 
Lemma 7.4: A finite Markov chain is irreducible If and only if its graph representation 
is a strongly connected graph. 
Next we distinguish between transient and recurrent states. Let r/j denote the proba-
bility that, starting at state i, the first transition to state j occurs at time t; that is, 
r/j = Pr(Xt = j and, for 1 :s s :s t -
1, XI -# j I Xo = i). 
Definition 7.4: A state is recurrent if Lt::O:l r/i = 1, and it is transient If Lt::O:l r/i < I. 
A Markov chain is recurrent If every state in the chain is recurrent. 
If state i is recurrent then, once the chain visits that state, it will (with probability 1) 
eventually return to that state. Hence the chain will visit state i over and over again, 
infinitely often. On the other hand, if state i is transient then, starting at i, the chain 
will return to i with some fixed probability p = L t::O: 1 r/ i· In this case, the number of 
times the chain visits i when starting at i is given by a geometric random variable. If 
one state in a communicating class is transient (respectively, recurrent) then all states 
in that class are transient (respectively, recurrent); proving this is left as Exercise 7.5. 
We denote the expected time to return to state i when starting at state i by hi.i = 
l.::r~1 t· r/i· Similarly, for any pair of states i and j, we denote by hi.j = Lt::O:l t· r/j 
164 

7.2 CLASSIFICATION OF STATES 
the expected time to first reach j from state i. It may seem that if a chain is recurrent, 
so that we visit a state i infinitely often, then hi.i should be finite. This is not the case, 
which leads us to the following definition. 
Definition 7.5: A recurrent state i is positive recurrent ~l h i.i < x. Oriu!nrise. it is 
null recurrent. 
To give an example of a Markov chain that has null recurrent states. consider a chain 
whose states are the positive integers. From state i, the probability of going to state 
i + 1 is i/(i + 1). With probability 1/(i + 1), the chain returns to state I. Starting at 
state I, the probability of not having returned to state I within the first r steps is thus 
t 
j 
I 
flj+1 = t+I' 
./=, 
Hence the probability of never returning to state I from state I is O. and state I is recur-
rent. It follows that 
rr, = ---
t(t + I) 
However, the expected number of steps until the first return to state I from state 1 is 
\\hich is unbounded. 
In the foregoing example the Markov chain had an infinite number of states. This is 
necessary for null recurrent states to exist. The proof of the following important lemma 
i" left as Exercise 7.9. 
Lemma 7.5: In a finite Markov chain: 
I. at least one state is recurrent; and 
") all recurrent states are positive recurrent. 
finally. for our later study of limiting distributions of Markov chains we v,'ill need to 
Jetlne what it means for a state to be aperiodic. As an example of periodicity. consider 
.1 random walk whose states are the positive integers. When at state i. with probabil-
It~ 1/ 2 the chain moves to i + I and with probability 1/2 the chain moves to i-I. If 
the chain starts at state 0, then it can be at an even-numbered state only after an even 
number of moves, and it can be at an odd-numbered state only after an odd number of 
;;1\ )\eS. This is an example of periodic behavior. 
Definition 7.6: A state j in a discrete time Markov chain is periodic it" there e.rists an 
:rrrn~er ~ > I such that Pr( XI+ 1 = j I XI = j) = ° 
unless s is divisible by ~. A 
.iI,(rere rime Markov chain is periodic If any state in the chain is periodic. A state or 
. 'I,Wl rhat is not periodic is aperiodic. 
165 

MARKOV CHAINS AND RANDOM WALKS 
In our example, every state in the Markov chain is periodic because, for every state j, 
Pr(Xt+ 1 = j I X t = j) = 0 unless s is divisible by 2. 
We end this section with an important corollary about the behavior of finite Markov 
chains. 
Definition 7.7: An aperiodic, positive recurrent state is an ergodic state. A Markov 
chain is ergodic If all its states are ergodic. 
Corollary 7.6: Any finite, irreducible, and aperiodic Markov chain is an ergodic chain. 
Proof' A finite chain has at least one recurrent state by Lemma 7.5, and if the chain is 
irreducible then all of its states are recurrent. In a finite chain, all recurrent states are 
positive recurrent by Lemma 7.5 and thus all the states of the chain are positive recur-
rent and aperiodic. The chain is therefore ergodic. 
• 
7.2.1. Example: The Gambler's Ruin 
When a Markov chain has more than one class of recurrent states, we are often inter-
ested in the probability that the process will enter and thus be absorbed by a given 
communicating class. 
For example, consider a sequence of independent, fair gambling games between 
two players. In each round a player wins a dollar with probability 1/2 or loses a dollar 
with probability 1/2. The state of the system at time t is the number of dollars won by 
player 1. If player 1 has lost money, this number is negative. The initial state is O. 
It is reasonable to assume that there are numbers f-, and f-2 such that player i can-
not lose more than f-i dollars, and thus the game ends when it reaches one of the two 
states -f-, or f-2. At this point, one of the gamblers is ruined; that is, he has lost all his 
money. To conform with the formalization of a Markov chain, we assume that for each 
of these two end states there is only one transition out and that it goes back to the same 
state. This gives us a Markov chain with two absorbing, recurrent states. 
What is the probability that player 1 wins f-2 dollars before losing f-, dollars? If f-2 = 
f-" then by symmetry this probability must be 1/2. We provide a simple argument for 
the general case using the classification of the states. 
Clearly -f-, and f- 2 are recurrent states. All other states are transient, since there is a 
nonzero probability of moving from each of these states to either state -f-, or state f- 2. 
Let P/ be the probability that, after t steps, the chain is at state i. For -f-, < i < 
t 2 , state i is transient and so limt---+x P/ = O. 
Let q be the probability that the game ends with player I winning £2 dollars, so that 
the chain was absorbed into state f- 2. Then I - q is the probability the chain was ab-
sorbed into state -£ ,. By definition, 
lim P/o = q. 
t---+x 
-
Since each round of the gambling game is fair, the expected gain of player 1 in each 
step is O. Let W t be the gain of player I after t steps. Then E[Wt] = 0 for any t by 
induction. Thus, 
166 

and 
Thus, 
7.3 STATIONARY DISTRIBUTIONS 
L 2 
E [W t] = L i P/ = 0 
i=-tl 
lim E[Wt] = ~2q -
~I(l- q) 
t---+oo 
= O. 
~I 
q=--. 
~I + ~2 
That is, the probability of winning (or losing) is proportional to the amount of money 
a player is willing to lose (or win). 
Another approach that yields the same answer is to let qj represent the probability 
that player 1 wins ~2 dollars before losing ~l dollars when having won j dollars for 
-£1 ::: j ::: ~2. Clearly, q-£I = 0 and q€2 = 1. For -£1 < j < t].. we compute by 
~onsidering the outcome of the first game: 
qj-I 
qj+1 
qj=-2-+2' 
\\"e have ~2 + ~ 1 - 2 linearly independent equations and t]. + { 1 - 2 unknowns. so there 
i-; a unique solution to this set of equations. It is easy to verify that (jj = (t, + j)/ 
It 1 + ~2) satisfies the given equations. 
In Exercise 7.20, we consider the question of what happens if. as is generally the 
I..'ase in real life, one player is at a disadvantage and so is slightly more likely to lose 
than to win any single game. 
7.3. Stationary Distributions 
Recall that if P is the one-step transition probability matrix of a Markov chain and if 
iJ ( r) is the probability distribution of the state of the chain at time t, then 
jJ(t + 1) = jJ(t)P. 
Of particular interest are state probability distributions that do not change after a 
transition. 
Definition 7.8: A stationary distribution (also called an equilibrium distribution) (~f a 
\farkov chain is a probability distribution ir such that 
ir=irP. 
If a chain ever reaches a stationary distribution then it maintains that distribution for 
.ill future time, and thus a stationary distribution represents a steady state or an equi-
lIhrium in the chain's behavior. Stationary distributions playa key role in analyzing 
\L.irkov chains. The fundamental theorem of Markov chains characterizes chains that 
.:,)Il\erge to stationary distributions. 
167 

MARKOV CHAINS AND RANDOM WALKS 
We discuss first the case of finite chains and then extend the results to any discrete 
space chain. Without loss of generality, assume that the finite set of states of the Markov 
chain is {a, 1, ... ,n}. 
Theorem 7.7: Any finite, irreducible, and ergodic Markov chain has the following 
properties: 
1. the chain has a unique stationar.v distribution if = (Jro, Jrl, ... , Jrn ): 
2. for all j and i, the limit lim t ---+ x P/ i exists and it is independent of j: 
3. Jri = lim t --+ x P/ i = l/h i .i . 
Under the conditions of this theorem, the stationary distribution if has two interpre-
tations. First, Jri is the limiting probability that the Markov chain will be in state i 
infinitely far out in the future, and this probability is independent of the initial state. 
In other words, if we run the chain long enough, the initial state of the chain is almost 
forgotten and the probability of being in state i converges to Jri. Second, Jri is the in-
verse of h i.i = L~, t . r/i' the expected number of steps for a chain starting in state 
i to return to i. This stands to reason; if the average time to return to state i from i is 
h i.i , then we expect to be in state i for l/h i.i of the time and thus, in the limit, we must 
have Jri = l/h i.i . 
Proof of Theorem 7.7: We prove the theorem using the following result, which we 
state without proof. 
Lemma 7.8: For an.v irreducible, ergodic Markov chain andfor any state i, the limit 
lim t ---+ x P/' i exists and 
. 
t 
1 
11m PI . I =-
t---+x 
hi.i 
This lemma is a corollary of a basic result in renewal theory. We give an informal jus-
tification for Lemma 7.8: the expected time between visits to i is h i.i , and therefore 
state i is visited l/h i .i of the time. Thus lim t ---+ x P/' i , which represents the probabil-
ity a state chosen far in the future is at state i when the chain starts at state i. must be 
l/hi,i. 
Using the fact that lim t ---+ x P/' i exists, we now show that, for any j and i, 
1 
lim pt. = lim pt. =_; 
t---+x 
.1. 1 
t---+x 
1.1 
hi.i 
that is, these limits exist and are independent of the starting state j. 
Recall that r t. is the probability that starting at j, the chain first visits i at time t . 
.1. 1 
Since the chain is irreducible we have that L~I r/i = L and for any c > ° 
there exists 
(a finite) tl = tiCS) such that L~~I r/i 2: 1 - c. 
For j "# i, we have 
t 
pt. = "rk.pt~k. 
/.1 
~./,II.I 
k=I 
168 

7.3 STATIONARY DISTRIBUTIONS 
For t :::: t], 
tl 
t 
"r.kp.t~k < "r.kpt~k = Pjt.I" 
~ 
j,1 
1.1 
-
~ 
j.1 
1.1 
k=l 
k=] 
Using the facts that lim t ---+ oo P/' i exists and t] is finite, we have 
Similarly, 
tl 
1m p .. > 
1m 
r··· . 
1· 
t 
l' L k p t - k 
t---+oo 
./.1 -
t---+oo 
j.1 
1.1 
k=l 
tl 
L 
k l' 
pt 
= 
r·· 1m 
" 
j.1 t---+oo 
1.1 
k=] 
tl 
= lim pt. " 
rk 
t---+x 
I,I~ j.1 
k=] 
:::: (l - 8) lim P/' i . 
t---+oo 
t 
pt. = "r.kpt~k 
./.1 
~ 
j.1 
1.1 
k=] 
tl 
< 
,..... 
8. 
L "pt-"+ 
-
j.1 
1.1 
k=] 
from which we can deduce that 
lim pt. < lim (~rkpt~" + 8) 
t---+oc 
./.t -
t---+oo ~ 
./.1 
1.1 
k=] 
tl 
= "r.k lim pt~k + 8 
~ 
./.I t ---+ oo 
1.1 
k=] 
< lim pt. + 8. 
-
t---+x 
1.1 
Letting 8 approach 0, we have proven that, for any pair i and j. 
lim p.t. = lim pt. 
t---+oo 
j.1 
t---+oo 
1.1 
h· . 
1.1 
~ow let 
Jri = lim pt. = 
t---+oo 
./.1 
h i . i 
\\'e show that if = (Jro, Jrl, ... ) forms a stationary distribution. 
For every t :::: 0, we have P/' i :::: 0 and thus Jri :::: O. For any t :::: O. L;I=() P/ i 
.mJ thus 
11 
/I 
/I 
lim " 
pt. = " 
lim P'~i = "Jri = 1. 
t---+oo ~ ./.1 
~ 
t---+oc 
j 
~ 
i=() 
i=O 
i=() 
169 

MARKOV CHAINS AND RANDOM WALKS 
and if is a proper distribution. Now, 
Letting t -+ 00, we have 
11 
pt+1 = " 
ptkPU. 
/'{ 
~ .I, 
k=O 
/I 
][i = L][k Pu, 
k=O 
proving that if is a stationary distribution. 
Suppose there were another stationary distribution ¢. Then by the same argument 
we would have 
11 
¢i = L¢kPL, 
k=O 
and taking the limit as t -+ 00 yields 
/I 
n 
Since L~=() ¢k = 1 it follows that ¢i = ][i for all i, or ¢ = if. 
• 
It is worth making a few remarks about Theorem 7.7. First, the requirement that the 
Markov chain be aperiodic is not necessary for the existence of a stationary distribu-
tion. In fact, any finite Markov chain has a stationary distribution; but in the case of a 
periodic state i, the stationary probability ][i is not the limiting probability of being in 
i but instead just the long-term frequency of visiting state i. Second, any finite chain 
has at least one component that is recurrent. Once the chain reaches a recurrent com-
ponent, it cannot leave that component. Thus, the subchain that corresponds to that 
component is irreducible and recurrent, and the limit theorem applies to any aperiodic 
recurrent component of the chain. 
One way to compute the stationary distribution of a finite Markov chain is to solve 
the system of linear equations 
ifP=if. 
This is particularly useful if one is given a specific chain. For example, given the tran-
sition matrix 
[ 
0 
1/2 
P = 
1~4 
1/4 
o 
1/4 
1/2 
o 
1/3 
1/2 
1/4 
3/4] 
1/6 
o ' 
1/4 
we have five equations for the four unknowns ][0, ][1, ][2, and ][3 given by if P = if and 
L~=() ][i = 1. The equations have a unique solution. 
Another useful technique is to study the cut-sets of the Markov chain. For any state 
i of the chain, 
170 

7.3 STATIONARY DISTRIBUTIONS 
I-p 
p 
q 
l-q 
Figure 7.2: A simple Markov chain used to represent bursty behavior. 
/I 
/I 
L Jr} Pj.i = Jri = Jri L Pi,} 
}d) 
j=O 
ur 
}-,Fi 
}-,Fi 
That is, in the stationary distribution the probability that a chain leaves a state equals 
the probability that it enters the state. This observation can be generalized to sets of 
"tates as follows. 
Theorem 7.9: Let S be a set of states of ajinite, irreducible, aperiodic Markm' chain. 
In rhe stationary distribution, the probability that the chain lem'es the set S equals the 
.;1robability that it enters S. 
In other words, if C is a cut-set in the graph representation of the chain. then in the sta-
!innary distribution the probability of crossing the cut-set in one direction is equal to 
the probability of crossing the cut-set in the other direction. 
:\ basic but useful Markov chain that serves as an example of cut-sets is given in 
Figure 7.2. The chain has only two states. From state O. you move to state I with prob-
.1t"lility p and stay at state 0 with probability I -
p. Similarly, from state I you move 
:~, "tate 0 with probability q and remain in state I with probability I - q, This Markov 
,:hain is often used to represent bursty behavior. For example. when bits are corrupted 
:n transmissions they are often corrupted in large blocks, since the errors are often 
.:aused by an external phenomenon of some duration. In this setting. being in state 0 
.1fter r steps represents that the tth bit was sent successfully, while being in state I rep-
re"ents that the bit was corrupted. Blocks of successfully sent bits and corrupted bits 
rx)th have lengths that follow a geometric distribution. When p and q are small. state 
.:hanges are rare, and the bursty behavior is modeled. 
The transition matrix is 
[
1 -
p 
p= 
q 
p 
] 
1 - q . 
S\)hing if P = if corresponds to solving the following system of three equations: 
JroO -
p) + Jrlq = Jro: 
JroP + JrIO -
q) = Jrl; 
JrO+Jr] =1. 
171 

MARKOV CHAINS AND RANDOM WALKS 
The second equation is redundant, and the solution is Jro = q/(p + q) and Jr] = 
p/(p + q). For example, with the natural parameters p = 0.005 and q = 0.1, in the 
stationary distribution more than 95% of the bits are received uncorrupted. 
Using the cut-set formulation, we have that in the stationary distribution the proba-
bility of leaving state 0 must equal the probability of entering state 0, or 
JroP = Jr]q. 
Again, now using Jro + Jr] = 1 yields Jro = q/(p + q) and Jr] = p/(p + q). 
Finally, for some Markov chains the stationary distribution is easy to compute by 
means of the following theorem. 
Theorem 7.10: Consider a finite, irreducible, and ergodic Markov chain with transi-
tion matrix P. ffthere are nonnegative numbers if = (Jro,·.·, Jr/l) such that L;I=O Jri 
1 and If, Jor any pair oJ states i, j, 
then if is the stationary distribution corresponding to P. 
Proof: Consider the jth entry of ifP. Using the assumption of the theorem, we find 
that it equals 
11 
11 
Thus if satisfies if = ifP. Since L;I=O Jri = 1, it follows from Theorem 7.7 that if must 
be the unique stationary distribution of the Markov chain. 
• 
Chains that satisfy the condition 
are called time reversible; Exercise 7.13 helps explain why. You may check that the 
chain of Figure 7.2 is time reversible. 
We turn now to the convergence of Markov chains with countably infinite state 
spaces. Using essentially the same technique as in the proof of Theorem 7.7, one can 
prove the next result. 
Theorem 7.11: Any irreducible aperiodic Markov chain belongs to one oJtheJollmv-
ing m'o categories: 
1. the chain is ergodic - for any pair of states i and j, the limit lim t ---+ cx) P/ i exists 
and is independent of j, and the chain has a unique stationary distribution Jri = 
liml---+x PL > 0; or 
2. flO state is positive recurrent -Jor all i and j, lim t ---+ x P/ i = 0, and the chain has 
flO stationary distribution. 
172 

7.3 STATIONARY DISTRIBUTIONS 
Cut-sets and the property of time reversibility can also be used to find the stationary 
distribution for Markov chains with countably infinite state spaces. 
7.3.1. Example: A Simple Queue 
A queue is a line where customers wait for service. We examine a model for a bounded 
queue where time is divided into steps of equal length. At each time step, exactly one 
of the following occurs. 
• If the queue has fewer than n customers, then with probability A a new customer 
joins the queue. 
• If the queue is not empty, then with probability f1 the head of the line is served and 
leaves the queue. 
• With the remaining probability, the queue is unchanged. 
If Xl is the number of customers in the queue at time t, then under the foregoing 
rules the Xl yield a finite-state Markov chain. Its transition matrix has the following 
nonzero entries: 
Pi,i+] = A if i < n; 
Pi,i-] = f1 
if i > 0; 
P" 
= { 
1 - A 
l-A-f1 
I-f1 
if i = 0, 
if 1 .::: i .::: II -
1. 
if i =ll. 
The Markov chain is irreducible, finite, and aperiodic, so it has a unique stationary 
distribution if. We use if = if P to write 
Jro = (1 -
A)JrO + f1 Jr ], 
Jri = AJri-] + (1 - A -
f1)Jri + f1Jri+h 
1.::: i .::: n - 1, 
Jrn = AJrIl _] + (1 -
f1)Jrn · 
It i~ easy to verify that 
h a solution to the preceding system of equations. Adding the requirement L ;1=0 Jri = 
I. we have 
Jr () = ~ 
II 
(A / ) i . 
L
1=() 
f1 
Fdf all ° .::: i .::: n, 
(A/f1)i 
Jri = -----
L~I=O(A/f1)i . 
(7.4) 
173 

MARKOV CHAINS AND RANDOM WALKS 
Another way to compute the stationary probability in this case is to use cut-sets. For 
any i, the transitions i -+ i + 1 and i + 1 -+ i constitute a cut-set of the graph represent-
ing the Markov chain. Thus, in the stationary distribution, the probability of moving 
from state i to state i + 1 must be equal to the probability of moving from state i + I 
to i, or 
A simple induction now yields 
In the case where there is no upper limit n on the number of customers in a queue. 
the Markov chain is no longer finite. The Markov chain has a countably infinite state 
space. Applying Theorem 7.11, the Markov chain has a stationary distribution if and 
only if the following set of linear equations has a solution with all Jri > 0: 
Jro = (1 -
A)JrO + /1Jr,; 
(7.5) 
It is easy to verify that 
is a solution of the system of equations (7.5). This naturally generalizes the solution to 
the case where there is an upper bound n on the number of the customers in the system 
given in Eqn. (7.4). All of the Jri are greater than 0 if and only if A < /1, which corre-
sponds to the situation when the rate at which customers arrive is lower than the rate 
at which they are served. If A > /1, then the rate at which customers arrive is higher 
than the rate at which they depart. Hence there is no stationary distribution, and the 
queue length will become arbitrarily long. In this case, each state in the Markov chain 
is transient. The case of A = /1 is more subtle. Again, there is no stationary distri-
bution and the queue length will become arbitrarily long, but now the states are null 
recurrent. (See the related Exercise 7.l7.) 
7.4. Random Walks on Undirected Graphs 
A random walk on an undirected graph is a special type of Markov chain that is often 
used in analyzing algorithms. Let G = (V, E) be a finite, undirected, and connected 
graph. 
Definition 7.9: A random walk on G is a Markov chain defined by the sequence (~l 
lluwes (~f a particle between vertices of G. In this process, the place q{ the particle at 
a gi\'en time step is the state (~{the system. If the particle is at \'ertex i and ifi has d(i) 
olltgoin;,? edges, then the probability that the particle fo llmvs the edge (i, j) and moves 
to a neighbor j is l/dU). 
174 

7.4 RANDOM WALKS ON UNDIRECTED GRAPHS 
We have already seen an example of such a walk when we analyzed the randomized 
2-SAT algorithm. 
For a random walk on an undirected graph, we have a simple criterion for aperiod-
icity as follows. 
Lemma 7.12: A random walk on an undirected graph G is aperiodic ~f and only ~f G 
is not bipartite. 
Proof' A graph is bipartite if and only if it does not have cycles with an odd number 
of edges. In an undirected graph, there is always a path of length :2 from a vertex to 
itself. If the graph is bipartite then the random walk is periodic with period d = 2. 
If the graph is not bipartite then it has an odd cycle, and by traversing that cycle we 
have an odd-length path from any vertex to itself. It follows that the Markov chain is 
aperiodic. 
• 
For the remainder of this section we assume that G is not bipartite. A random walk 
on a finite, undirected, connected, and non-bipartite graph G satisfies the conditions 
of Theorem 7.7, and hence the random walk converges to a stationary distribution. We 
~how that this distribution depends only on the degree sequence of the graph. 
Theorem 7.13: A random walk on G converges to a stationary distribution if, vvhere 
d(v) 
Jrv = 21EI' 
Proof' Since L VE v d (v) = 21 E I, it follows that 
"Jrv = "d(v) = 1 
~ 
~21EI 
' 
liE v 
VE V 
.. md iT is a proper distribution over v E V. 
Let P be the transition probability matrix of the Markov chain. Let N (v) represent 
[he neighbors of v. The relation if = if P is equivalent to 
" 
d(u) 
I 
d(v) 
Jr
1' = ~ 21EI d(u) = 21EI' 
IIEN(v) 
. 1llJ the theorem follows. 
• 
Recall that h v. II denotes the expected number of steps to reach u from L'. We have the 
f,)iiowing corollary. 
(~orollary 7.14: For any vertex u in G, 
21EI 
h =-
11.11 
d(u)' 
175 

MARKOV CHAINS AND RANDOM WALKS 
For any pair of vertices u and v, we prove the following simple bound. 
Lemma 7.15: If(u, v) E E, then hv,u < 21EI. 
Proof: Let N(u) be the set of neighbors of vertex u in G. We compute hu,u in two 
different ways: 
21EI 
1 
-- = hll U = -- "(1 + h111 u)· 
d(u) 
, 
d(u) ~ 
. 
WEN(ll) 
Therefore, 
l1'EN(u) 
and we conclude that h v, u < 21 E I. 
• 
Definition 7.10: The cover time of a graph G = (V, E) is the maximum over all ver-
tices v E V of the expected time to visit all of the nodes in the graph by a random walk 
starting from v. 
Lemma 7.16: The cover time of G = (V, E) is bounded above by 41 V I . I E I. 
Proof: Choose a spanning tree of G; that is, choose any subset of the edges that gives 
an acyclic graph connecting all of the vertices of G. There exists a cyclic (Eulerian) 
tour on this spanning tree in which every edge is traversed once in each direction: 
for example, such a tour can be found by considering the sequence of vertices passed 
through when doing a depth-first search. Let Vo, VI, ... , v21V1-2 = Vo be the sequence 
of vertices in the tour, starting from vertex Vo. Clearly the expected time to go through 
the vertices in the tour is an upper bound on the cover time. Hence the cover time is 
bounded above by 
21V1-3 
L hVi,lii+! < (21 VI - 2)(2IEI) < 41 VI· lEI, 
i=O 
where the first inequality comes from Lemma 7.15. 
• 
7.4.1. Application: An s-t Connectivity Algorithm 
Suppose we are given an undirected graph G = (V, E) and two vertices 51 and t in G. 
Let n = I V I and m = I E I. We want to determine if there is a path connecting sand 
t. This is easily done in linear time using a standard breadth-first search or depth-first 
search. Such algorithms, however, require Q (n) space. 
Here we develop a randomized algorithm that works with only D( log n) bits of mem-
ory. This could be even less than the number of bits required to write the path between 
sand t. The algorithm is simple: perform a random walk on G for enough steps so that 
a path from s to t is likely to be found. We use the cover time result (Lemma 7.16) to 
bound the number of steps that the random walk has to run. For convenience, assume 
176 

7.S PARRONDO'S PARADOX 
s-t Connectivity Algorithm: 
1. Start a random walk from s. 
2. If the walk reaches t within 4n 3 steps, return that there is a path. Otherwise. 
return that there is no path. 
Algorithm 7.4: s-t Connectivity algorithm. 
that the graph G has no bipartite connected components, so that the results of Theo-
rem 7.l3 apply to any connected component of G. (The results can be made to apply 
to bipartite graphs with some additional technical work.) 
Theorem 7.17: The s-t connectivi(v algorithm (Algorithm 7.4) returns the correct 
Ulls}t'er with probability 1/2, and it only errs by returning that there is no path from s 
[() t when there is such a path. 
Proof: If there is no path then the algorithm returns the correct answer. If there is a 
path, the algorithm errs if it does not find the path within 411 3 steps of the walk. The 
expected time to reach t from s (if there is a path) is bounded from above by the cover 
time of their shared component, which by Lemma 7.16 is at most 411111 < 2n 3. By 
\larkov's inequality, the probability that a walk takes more than 411'~ steps to reach s 
from t is at most 1/2. 
• 
The algorithm must keep track of its current position. which takes D( log 11) bits, as 
\\ ell as the number of steps taken in the random walk, which also takes only D( log 11) 
hit...; (since we count up to only 4n 3 ). As long as there is some mechanism for choosing 
.1 random neighbor from each vertex, this is all the memory required. 
7.5. Parrondo's Paradox 
Parrondo's paradox provides an interesting example of the analysis of Markov chains 
\\ hile also demonstrating a subtlety in dealing with probabilities. The paradox appears 
~l) contradict the old saying that two wrongs don't make a right, showing that two los-
:n~ games can be combined to make a winning game. Because Parrondo's paradox can 
~ analyzed in many different ways, we will go over several approaches to the problem. 
First, consider game A, in which we repeatedly flip a biased coin (call it coin a) that 
,:\)mes up heads with probability Pu < 1/2 and tails with probability I - PU' You win a 
j\ )llar if the coin comes up heads and lose a dollar if it comes up tails. Clearly. this is a 
:\ hing game for you. For example, if Po = 0.49, then your expected loss is two cents 
0'r game. 
In game B, we also repeatedly flip coins, but the coin that is flipped depends on how 
: \)u have been doing so far in the game. Let w be the number of your wins so far and 
! the number of your losses. Each round we bet one dollar, so w - t represents your 
177 

MARKOV CHAINS AND RANDOM WALKS 
winnings; if it is negative, you have lost money. Game B uses two biased coins, coin b 
and coin c. If your winnings in dollars are a multiple of 3, then you flip coin b, which 
comes up heads with probability Ph and tails with probability 1 -
Ph. Otherwise, you 
flip coin c, which comes up heads with probability Pc and tails with probability 1 -
Pc. 
Again, you win a dollar if the coin comes up heads and lose a dollar if it comes up tails. 
This game is more complicated, so let us consider a specific example. Suppose coin 
b comes up heads with probability Ph = 0.09 and tails with probability 0.91 and that 
coin c comes up heads with probability Pc = 0.74 and tails with probability 0.26. At 
first glance, it might seem that game B is in your favor. If we use coin b for the 1/3 
of the time that your winnings are a multiple of 3 and use coin c the other 2/3 of the 
time, then your probability w of winning is 
1 9 
2 74 
157 
1 
w = 3 100 + 3 100 = 300 > 2:' 
The problem with this line of reasoning is that coin b is not necessarily used 1/3 of 
the time! To see this intuitively, consider what happens when you first start the game. 
when your winnings are O. You use coin b and most likely lose, after which you use 
coin c and most likely win. You may spend a great deal of time going back and forth 
between having lost one dollar and breaking even before either winning one dollar or 
losing two dollars, so you may use coin b more than 1/3 of the time. 
In fact, the specific example for game B is a losing game for you. One way to show 
this is to suppose that we start playing game B when your winnings are 0, continuing 
until you either lose three dollars or win three dollars. If you are more likely to lose 
than win in this case, by symmetry you are more likely to lose three dollars than win 
three dollars whenever your winnings are a mUltiple of 3. On average, then, you would 
obviously lose money on the game. 
One way to determine if you are more likely to lose than win is to analyze the ab-
sorbing states. Consider the Markov chain on the state space consisting of the integers 
{-3, ... , 3}, where the states represent your winnings. We want to know, when you 
start at 0, whether or not you are more likely to reach -3 before reaching 3. We can 
determine this by setting up a system of equations. Let Zi represent the probability you 
will end up having lost three dollars before having won three dollars when your cur-
rent winnings are i dollars. We calculate all the probabilities Z-3, Z-2, Z-I, Zo, ZI, Z2, 
and Z3, although what we are really interested in is ZOo If Zo > 1/2, then we are more 
likely to lose three dollars than win three dollars starting from O. Here Z-3 = 1 and 
23 = 0; these are boundary conditions. We also have the following equations: 
Z-2 = (1 -
Pc)Z-3 + PcZ-I, 
Z-I = (1 -
Pc)Z-2 + PcZO, 
Zo = (1 -
PIJZ-I + PhZI, 
ZI = (1 -
Pc)ZO + Pc Z2, 
Z2 = (1 -
Pc)ZI + Pc Z3· 
This is a system of five equations with five unknowns, and hence it can be solved easily. 
The general solution for Z 0 is 
178 

7.5 PARRONDO'S PARADOX 
20 = (1 -
Ph)(l -
Pc)2 + PhfJ,: . 
For the specific example here, the solution yields Zo = 15.379 27.700 :::::: 0.555, show-
ing that one is much more likely to lose than win playing this game O\er the long run. 
Instead of solving these equations directly, there is a simpler \\ay of determining 
the relative probability of reaching - 3 or 3 first. Consider any sequence of moves that 
starts at 0 and ends at 3 before reaching -3. For example, a possible sequence is 
s = 0, 1. 2, 1. 2, 1,0, -1. - 2, - L 0, 1, 2, 1. 2, 3. 
We create a one-to-one and onto mapping of such sequences with the sequences that 
start at 0 and end at - 3 before reaching 3 by negating every number starting from the 
last 0 in the sequence. In this example, s maps to f(s), where 
f(s) = 0, 1,2, 1,2, 1,0, -1. -2, -1.0, -1. -2, -1. -2, -3. 
It is simple to check that this is a one-to-one mapping of the relevant sequences. 
The following lemma provides a useful relationship between sand f(s). 
Lemma 7.18: For any sequence s o.f moves that starts at 0 and ends at 3 before reach-
ing -3, vve have 
Pres occurs) 
Pr(f(s) occurs) 
Proof' For any given sequence s satisfying the properties of the lemma. let t, be the 
number of transitions from 0 to 1; t2, the number of transitions from 0 to -1: t3. the 
"um of the number of transitions from - 2 to -1. -1 to 0, 1 to 2, and 2 to 3; and t 4, 
[he sum of the number of transitions from 2 to 1, 1 to 0, -1 to - 2, and - 2 to - 3. Then 
[he probability that the sequence s occurs is p~l(l- PIJ t2PP(l- pJt4 . 
~ow consider what happens when we transform s to f(s). We change one transi-
[inn from 0 to 1 into a transition from 0 to -1. After this point, in s the total number of 
[[:lnsitions that move up 1 is two more than the number of transitions that move down 
1. since the sequence ends at 3. In f(s), then, the total number of transitions that move 
Jl)\\'n 1 is two more than the number of transitions that move up 1. It follows that the 
rwbability that the sequence f(s) occurs is p~l-'(l -
PIJ t2+'pp-2(l -
Pc)14+2. The 
iL'mma follows. 
• 
B: letting 5 be the set of all sequences of moves that start at 0 and end at 3 before 
i\?:lching -3, it immediately follows that 
Pr(3 is reached before - 3) 
Pre - 3 is reached before 3) 
L.IES Pres occurs) 
L.I'ES Pr(f(s) occurs) 
I:' [his ratio is less than 1, then you are more likely to lose than win. In our specific 
:=\:lmple, this ratio is 12,321/15,379 < 1. 
Finally, yet another way to analyze the problem is to use the stationary distribu-
:]\)n. Consider the Markov chain on the states {O, 1, 2}, where here the states represent 
179 

MARKOV CHAINS AND RANDOM WALKS 
the remainder when our winnings are divided by 3. (That is, the state keeps track of 
w - £ mod 3.) Let Jr; be the stationary probability of this chain. The probability that 
we win a dollar in the stationary distribution, which is the limiting probability that we 
win a dollar if we play long enough, is then 
Pb Jro + Pc Jr! + Pc Jr2 = Pb Jro + PcO -
Jro) 
= Pc - (Pc - Pb)JrO. 
Again, we want to know if this is greater than or less than 1/2. 
The equations for the stationary distribution are easy to write: 
Jro + Jr! + Jr2 = 1, 
PbJrO + (1 -
pJJr2 = Jr!, 
Pc Jr! + (1 -
Pb)JrO = Jr2, 
IJc Jr2 + (1 - Pc)Jr! = Jro· 
Indeed, since there are four equations and only three unknowns, one of these equations 
is actually redundant. The system is easily solved to find 
1 - Pc + P; 
Jro = 
) , 
3 - 2pc - Pb + 2PbPc + p(-: 
PbPc - Pc + 1 
Jr! = 
2' 
3 - 2pc - Pb + 2PbPc + Pc 
PbPc - Pb + 1 
Jr" = -----------
~ 
3 - 2pc - Pb + 2PbPc + p? 
Recall that you lose if the probability of winning in the stationary distribution is less 
than 1/2 or. equivalently. if p( - (Pc -
Ph)JrO < 1/2. In our specific example, Jro = 
673/1759 :::::: 0.3826 .... and 
86,421 
Pc - (Pc -
Ph)JrO = 175,900 < -. 
2 
Again, we find that game B is a losing game in the long run. 
We have now completely analyzed game A and game B. Next let us consider what 
happens when we try to combine these two games. In game C, we repeatedly perform 
the following bet. We start by flipping a fair coin, call it coin d. If coin d is heads, we 
proceed as in game A: we flip coin a, and if the coin is heads, you win. If coin d is 
tails, we then proceed to game B: if your current winnings are a multiple of 3, we flip 
coin b: otherwise, we flip coin c, and if the coin is heads then you win. It would seem 
that this must be a losing game for you. After all, game A and game B are both losing 
games, and this game just flips a coin to decide which of the two games to play. 
In fact, game C is exactly like game B, except the probabilities are slightly differ-
ent. If your winnings are a multiple of 3, then the probability that you win is p~ = 
! 
! 
0 h 
. 
h 
b b'l' 
h 
.. *! 
! 
U . 
* 
:2 Pa + 2 Pb· 
t erwIse, t e pro a I Ity t at you WIn IS Pc = 
:2 Pa + :2 Pc· 
SIng Pb 
180 

7.5 PARRONDO'S PARADOX 
and P; in place of Ph and Pc' we can repeat any of the foregoing analyses we used for 
game B. 
For example: if the ratio 
pZ( p;):: 
< 1, 
(1- pZHI - pn 2 
then the game is a losing game for you; if the ratio is larger than 1, it is a winning game. 
In our specific example the ratio is 438,741/420,959 > 1, so game C appears to be a 
winning game. 
This seems somewhat odd, so let us recheck by using our other approach of consid-
ering the stationary distribution. The game is a losing game if P; - (p; -
pZ )Jro < 
1/2 and a winning game if P; - (p; -
pZ )Jro > 1/2, where Jro is now the station-
ary distribution for the chain corresponding to game C. In our specific example, Jro = 
30,529/88,597, and 
* 
* 
* 
4,456,523 
Pc - (pc - Ph )Jro = 8,859,700 > 2' 
so game C again appears to be a winning game. 
How can randomly combining two losing games yield a winning game? The key 
is that game B was a losing game because it had a very specific structure. You were 
likely to lose the next round in game B if your winnings were divisible by 3, but if 
you managed to get over that initial barrier you were likely to win the next few games 
as well. The strength of that barrier made game B a losing game. By combining the 
games that barrier was weakened, because now when your winnings are divisible by 3 
you sometimes get to play game A, which is close to a fair game. Although game A 
is biased against you, the bias is small, so it becomes easier to overcome that initial 
barrier. The combined game no longer has the specific structure required to make it a 
losing game. 
You may be concerned that this seems to violate the law of linearity of expectations. 
If the winnings from a round of game A, B, and Care X,4, X B , and Xc (respectively), 
then it seems that 
so ifE[X,4] and E[X B ] are negative then E[XcJ should also be negative. The problem 
is that this equation does not make sense, because we cannot talk about the expected 
winnings of a round of games Band C without reference to the current winnings. We 
have described a Markov chain on the states {O, 1, 2} for games Band C. Let s repre-
sent the current state. We have 
E[Xc Is] = E[~(XA + XB) Is] 
= 1E [XA Is] + ~E[XB Is]. 
Linearity of expectations holds for any given step, but we must condition on the cur-
rent state. By combining the games we have changed how often the chain spends in 
each state, allowing the two losing games to become a winning game. 
181 

MARKOV CHAINS AND RANDOM WALKS 
7.6. Exercises 
Exercise 7.1: Consider a Markov chain with state space {O, 1,2,3} and a transition 
matrix 
[ 
0 
1/10 
P = 
1/10 
9/10 
3/10 
1/10 
7/10 
1/10 
1/10 
7/10 
1/10 
o 
so PO.3 = 3/5 is the probability of moving from state 0 to state 3. 
(a) Find the stationary distribution of the Markov chain. 
(b) Find the probability of being in state 3 after 32 steps if the chain begins at state O. 
(c) Find the probability of being in state 3 after 128 steps if the chain begins at a state 
chosen uniformly at random from the four states. 
(d) Suppose that the chain begins in state O. What is the smallest value of t for which 
maxI I p(~.s -
Jr.I·1 .:::; 0.01? Here iT is the stationary distribution. What is the smallest 
value of t for which maxs I P6.s -
Jrs I .:::; 0.001? 
Exercise 7.2: Consider the two-state Markov chain with the following transition matrix. 
p-[ P 
I-pPJ. 
-
1 - p 
Find a simple expression for P6.o' 
Exercise 7.3: Consider a process X O, Xl, X 2 , ... with two states, 0 and 1. The process 
is governed by two matrices, P and Q. If k is even, the values Pi,} give the proba-
bility of going from state i to state j on the step from X k to X k+ l . Likewise, if k is 
odd then the values Qi.} give the probability of going from state i to state j on the 
step from X k to X k+l . Explain why this process does not satisfy Definition 7.1 of a 
(time-homogeneous) Markov chain. Then give a process with a larger state space that 
is equivalent to this process and satisfies Definition 7.1. 
Exercise 7.4: Prove that the communicating relation defines an equivalence relation. 
Exercise 7.5: Prove that if one state in a communicating class is transient (respectively, 
recurrent) then all states in that class are transient (respectively, recurrent). 
Exercise 7.6: In studying the 2-SAT algorithm, we considered a I-dimensional ran-
dom walk with a completely reflecting boundary at O. That is, whenever position 0 
is reached, with probability 1 the walk moves to position 1 at the next step. Consider 
now a random walk with a partially reflecting boundary at O. Whenever position 0 is 
reached, with probability 1/2 the walk moves to position 1 and with probability 1/2 
the walk stays at O. Everywhere else the random walk moves either up or down 1, each 
with probability 1/2. Find the expected number of moves to reach n, starting from po-
sition i and using a random walk with a partially reflecting boundary. 
182 

7.6 EXERCISES 
Exercise 7.7: Suppose that the 2-SAT Algorithm 7.1 starts with an assignment chosen 
uniformly at random. How does this affect the expected time until a satisfying assign-
ment is found? 
Exercise 7.8: Generalize the randomized algorithm for 3-SAT to k-SAT. What is the 
expected time of the algorithm as a function of k? 
Exercise 7.9: In the analysis of the randomized algorithm for 3-SAT, we made the pes-
simistic assumption that the current assignment A I and the truth assignment 5 differ on 
just one variable in the clause chosen at each step. Suppose instead that, independently 
at each step, the two assignments disagree on one \'ariable in the clause with probabil-
ity p and at least two variables with probability 1 - p. What is the largest value of p 
for which you can prove that the expected number of steps before Algorithm 7.2 termi-
nates is polynomial in p? Give a proof for this value of p and give an upper bound on 
the expected number of steps in this case, 
Exercise 7.10: A coloring of a graph is an assignment of a color to each of its vertices. 
A graph is k-colorable if there is a coloring of the graph with k colors such that no two 
adjacent vertices have the same color. Let G be a 3-colorable graph. 
(a) Show that there exists a coloring of the graph with two colors such that no triangle 
is monochromatic. (A triangle of a graph G is a subgraph of G \vith three \ertices. 
which are all adjacent to each other.) 
(b) Consider the following algorithm for coloring the vertices of G with two colors 
so that no triangle is monochromatic. The algorithm begins with an arbitrary 2-
coloring of G. While there are any monochromatic triangles in G, the algorithm 
chooses one such triangle and changes the color of a randomly chosen vertex of 
that triangle. Derive an upper bound on the expected number of such recoloring 
steps before the algorithm finds a 2-coloring with the desired property. 
Exercise 7.11: An n x n matrix P with entries Pi, j is called stochastic if all entries are 
nonnegative and if the sum of the entries in each row is 1. It is called doubly stochas-
tic if, additionally, the sum of the entries in each column is 1. Show that the uniform 
distribution is a stationary distribution for any Markov chain represented by a doubly 
stochastic matrix. 
Exercise 7.12: Let X I1 be the sum of n independent rolls of a fair die. Show that, for 
any k ~ 2, 
1 
lim Pre Xn is divisible by k) = -. 
11---+ X 
k 
Exercise 7.13: Consider a finite Markov chain on n states with stationary distribution 
.7 and transition probabilities PI . j . Imagine starting the chain at time 0 and running 
it for m steps, obtaining the sequence of states X o, Xl, ... , Xiii' Consider the states in 
re\'erse order, X m , X m - l , ••• , Xo. 
183 

MARKOV CHAINS AND RANDOM WALKS 
(a) Argue that given X k+], the state X k is independent of X k+2 , X k+3 , ... , X m . Thus 
the reverse sequence is Markovian. 
(b) Argue that for the reverse sequence, the transition probabilities Qi.j are given by 
Jr.p .. 
Q .. -~ 
{,J -
Jri 
. 
(c) Prove that if the original Markov chain is time reversible, so that JriPi,j = JrjPj,i' 
then Q i, j = Pi, j. That is, the states follow the same transition probabilities whether 
viewed in forward order or reverse order. 
Exercise 7.14: Prove that the Markov chain corresponding to a random walk on an 
undirected, non-bipartite graph that consists of one component is time reversible. 
Exercise 7.15: Let P/' i be the probability that a Markov chain returns to state i when 
started in state i after t steps. Prove that 
Xi 
'" p,t, 
L 
{,I 
t=] 
is unbounded if and only if state i is recurrent. 
Exercise 7.16: Prove Lemma 7.5. 
Exercise 7.17: Consider the following Markov chain, which is similar to the I-dimen-
sional random walk with a completely reflecting boundary at O. Whenever position 0 
is reached, with probability 1 the walk moves to position 1 at the next step. Otherwise, 
the walk moves from i to i + 1 with probability p and from i to i-I with probability 
1 -
p. Prove that: 
(a) if p < 1/2, each state is positive recurrent; 
(b) if p = 1/2, each state is null recurrent; 
(c) if p > 1/2, each state is transient. 
Exercise 7.18: (a) Consider a random walk on the 2-dimensional integer lattice, where 
each point has four neighbors (up, down, left, and right). Is each state transient, null 
recurrent, or positive recurrent? Give an argument. 
(b) Answer the problem in (a) for the 3-dimensional integer lattice. 
Exercise 7.19: Consider the gambler's ruin problem, where a player plays until they 
lose .£] dollars or win .£2 dollars. Prove that the expected number of games played is 
.£ ].£2. 
Exercise 7.20: We have considered the gambler's ruin problem in the case where the 
game is fair. Consider the case where the game is not fair; instead, the probability of 
losing a dollar each game is 2/3 and the probability of winning a dollar each game is 
184 

7.6 EXERCISES 
1/3. Suppose that you start with i dollars and finish either when you reach n or lose it 
all. Let Wt be the amount you have gained after t rounds of play. 
(a) Show that E[2Wr+l] = E[2Wr]. 
(b) Use part (a) to determine the probability of finishing with 0 dollars and the proba-
bility of finishing with n dollars when starting at position i. 
(c) Generalize the preceding argument to the case where the probability of losing is 
p > 1/2. (Hint: Try considering E[c Wr ] for some constant c.) 
Exercise 7.21: Consider a Markov chain on the states {O. 1 .... , 11 }, where for i < n we 
have Pu +! = 1/2 and p i ,() = 1/2. Also, P'1.11 = 1/2 and PII . O = 1/2. This process can 
be viewed as a random walk on a directed graph wi th vertices {O. 1 .... , n }, where each 
vertex has two directed edges: one that returns to 0 and one that moves to the vertex 
with the next higher number (with a self-loop at vertex II). Find the stationary distribu-
tion of this chain. (This example shows that random walks on directed graphs are very 
different than random walks on undirected graphs.) 
Exercise 7.22: A cat and a mouse each independently take a random walk on a con-
nected, undirected, non-bipartite graph G. They start at the same time on different 
nodes, and each makes one transition at each time step. The cat eats the mouse if they 
are ever at the same node at some time step. Let nand m denote. respectively. the num-
ber of vertices and edges of G. Show an upper bound of O(m2 n) on the expected time 
before the cat eats the mouse. (Hint: Consider a Markov chain whose states are the or-
dered pairs (a, h), where a is the position of the cat and h is the position of the mouse.) 
Exercise 7.23: One way of spreading information on a network uses a rumor-spreading 
paradigm. Suppose that there are n hosts currently on the network. Initially, one host 
begins with a message. Each round, every host that has the message contacts another 
host chosen independently and uniformly at random from the other n - 1 hosts, and 
sends that host the message. We would like to know how many rounds are necessary 
before all hosts have received the message with probability 0.99. 
(a) Explain how this problem can be viewed in terms of Markov chains. 
(b) Determine a method for computing the probability that j hosts have received the 
message after round k given that i hosts have received the message after round k -1. 
(Hint: There are various ways of doing this. One approach is to let P(i, j. c) be 
the probability that j hosts have the message after the first c of the i hosts have 
made their choices in a round; then find a recurrence for P.) 
(c) As a computational exercise. write a program to determine the number of rounds 
required for a message starting at one host to reach all other hosts with probability 
0.9999 when n = 128. 
Exercise 7.24: The lollipop graph on n vertices is a clique on n/2 vertices connected 
to a path on 11/2 vertices, as shown in Figure 7.3. The node u is a part of both the clique 
and the path. Let v denote the other end of the path. 
185 

MARKOV CHAINS AND RANDOM WALKS 
Figure 7.3: Lollipop graph. 
(a) Show that the expected covering time of a random walk starting at v is 8(n 2). 
(b) Show that the expected covering time for a random walk starting at u is 8(n 3). 
Exercise 7.25: The following is a variation of a simple children's board game. A player 
starts at position O. On a player's turn, she rolls a standard six-sided die. If her old po-
sition was the positive integer x and her roll is y, then her new position is x + y, except 
in two cases: 
• if x + y is divisible by 6 and less than 36, her new position is x + y - 6; 
• if x + y is greater than 36, the player remains at x. 
The game ends when a player reaches the goal position, 36. 
(a) Let Xi be a random variable representing the number of rolls needed to get to 36 
from position i for 0 :s i :s 35. Give a set of equations that characterize E[Xi ]. 
(b) Using a program that can solve systems of linear equations, find E[XiJ for 0 :s 
i :s 35. 
Exercise 7.26: Let n equidistant points be marked on a circle. Without loss of gener-
ality, we think of the points as being labeled clockwise from 0 to n - 1. Initially, a wolf 
begins at 0 and there is one sheep at each of the remaining n - 1 points. The wolf takes 
a random walk on the circle. For each step, it moves with probability 1/2 to one neigh-
boring point and with probability 1/2 to the other neighboring point. At the first visit 
to a point, the wolf eats a sheep if there is still one there. Which sheep is most likely 
to be the last eaten? 
Exercise 7.27: Suppose that we are given n records, RI, R2 , ... , Rn. The records are 
kept in some order. The cost of accessing the jth record in the order is j. Thus, if we 
had four records ordered as R2 , R4, R3, RI, then the cost of accessing R4 would be 2 
and the cost of accessing R I would be 4. 
Suppose further that, at each step, record Ri is accessed with probability Pi' with 
each step being independent of other steps. If we knew the values of the Pi in advance, 
we would keep the R j in decreasing order with respect to Pi. But if we don't know the 
Pi in advance, we might use the "move to front" heuristic: at each step, put the record 
that was accessed at the front of the list. We assume that moving the record can be 
186 

7.6 EXERCISES 
j\me with no cost and that all other records remain in the same order. For example, if 
[he order was R2 , R 4 , R3, RI before R3 was accessed, then the order at the next step 
\1, l)llid be R3, R2 , R4 , R I. 
In this setting, the order of the records can be thought of as the state of a \larkoy 
-:h;Jin. Give the stationary distribution of this chain. Also. let X k be the cost for ac-
-:e ... sing the kth requested record. Determine an expression for limk~ ~ E [X z]. '{our 
npression should be easily computable in time that is polynomial in II. gi\t?ll the I)" 
Exercise 7.28: Consider the following variation of the discrete time queue. Time is 
JI\ided into fixed-length steps. At the beginning of each time step, a customer arriYes 
",I, ith probability A. At the end of each time step. if the queue is nonempty then the cus-
t, )mer at the front of the line completes service with probability f.1. 
I a I Explain how the number of customers in the queue at the beginning of each time step 
forms a Markov chain, and determine the corresponding transition probabilities. 
I b I Explain under what conditions you would expect a stationary distribution ir to 
exist. 
leI If a stationary distribution exists, then what should be the value of lTo, the proba-
bility that no customers are in the queue at the beginning of the time step? (Hint: 
Consider that, in the long run, the rate at which customers enter the queue and the 
rate at which customers leave the queue must be equal.) 
Id I Determine the stationary distribution and explain how it corresponds to your con-
ditions from part (b). 
leI ~ow consider the variation where we change the order of incoming arrivals and 
service. That is: at the beginning of each time step, if the queue is nonempty then 
a customer is served with probability f.1: and at the end of a time step a customer 
arrives with probability A. How does this change your answers to parts (a)-(d)? 
187 

CHAPTER EIGHT 
Continuous Distributions 
and the Poisson Process 
This chapter introduces the general concept of continuous random variables, focusing 
on two examples of continuous distributions: the uniform distribution and the expo-
nential distribution. We then proceed to study the Poisson process, a continuous time 
counting process that is related to both the uniform and exponential distributions. We 
conclude this chapter with basic applications of the Poisson process in queueing theory. 
8.1. Continuous Random Variables 
8.1.1. Probability Distributions in R 
The continuous roulette wheel in Figure 8.1 has circumference l. We spin the wheel, 
and when it stops. the outcome is the clockwise distance X (computed with infinite 
precision) from the "0" mark to the arrow. 
The sample space Q of this experiment consists of all real numbers in the range 
[0,1). Assume that any point on the circumference of the disk is equally likely to face 
the arrow when the disk stops. What is the probability p of a given outcome x? 
To answer this question, we recall that in Chapter 1 we defined a probability func-
tion to be any function that satisfies the following three requirements: 
1. Pr(Q) = 1; 
2. for any event E, 
° 
.::s Pr(E) .::s 1; 
3. for any (finite or enumerable) collection B of disjoint events, 
Let S(k) be a set of k distinct points in the range [0,1), and let p be the probabil-
ity that any given point in [0, 1) is the outcome of the roulette experiment. Since the 
probability of any event is bounded by 1, 
188 

8.1 CONTINUOUS RANDOM VARIABLES 
Figure 8.1: A continuous roulette wheel. 
Pr (x E S ( k)) = k p ::s 1. 
\\"e can choose any number k of distinct points in the range [0, I), so we must have 
~/) ::s I for any integer k, which implies that p = 0. Thus, we observe that in an infinite 
,ample space there may be possible events that have probability 0. Taking the comple-
ment of such an event, we observe that in an infinite sample space there can be events 
\\ ith probability I that do not correspond to all possible experimental outcomes, and 
thus there can be events with probability 1 that are, in some sense. not certain ~ 
If the probability of each possible outcome of our experiment is O. how do we define 
the probability of larger events with nonzero prohahility': For prohahility distrihutions 
1.)\ er l{, probabilities are assigned to intelyals rather than to indi\idual \~tlue-;.I 
The probability distribution of a random yariahle X i-; gi\en hy it-; disnihllli(lntllllc-
:11111 F(x), where for any x E lR we define 
F ( x) = PI"( X ::s x ) . 
\Ve say that a random variable X is continuous if its distribution fUllction F( x) is 
.1 I.:ontinuous function of x. We will assume that our random \ariable-; are continuous 
:hmughout this chapter. In this case, we must have that Pr( X = x) = () for any spe-
':ltic \'alue x. This further implies that Pr(X ::s x) = Pr(X < x). a fact \\e make use 
"t freely throughout this chapter. 
If there is a function f(x) such that, for all -x < a < x. 
J
{/ 
F(a) = 
-:x; l(t) dt, 
then f(x) is called the densityfunction of F(x), and 
I(x) = F'(x) 
.~ here the derivative is well-defined. 
\ ;, )rmal treatment of nondenumerably infinite probability spaces relies on measure theory and is beyond the 
'-0. 're' of thi~ book. We just note here that the probability function needs to be measurable on the set of events. 
T:l' (armot hold in general for the family of all subsets of the sample space. but it does always hold for the 
H, 'rd ,et of intervals. 
189 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Because 
Pr(x < X .:s x + dx) = F(x + dx) -
F(x) ~ f(x) dx, 
we can informally think of f(x) dx as the "probability" of the infinitesimal interval 
[x,x + dx). Carrying this analogy forward, in discrete spaces the probability of an 
event E is the sum of the probabilities of the simple events included in E. The paral-
lel concept in the case of events in ffi. is the integral of the probability density function 
over the basic events in E. 
For example, the probability of the interval [a, b) is given by the integral 
Pr(a .:s X < b) = fb f(x) dx, 
({ 
and the expectation and higher moments of a random variable X with density function 
f (x) are defined by the integrals 
E[Xi] = 1-: xif(x) dx. 
More generally, for any function g, 
E[ g( X)] = 1-: g(x )f(x) dx. 
when this integral exists. The variance of X is given by 
Var[X] = E[(X - E[X])2] = 1-: (x - E[X])'f(x)dx = E[X2]- (E[XJ)2 
The following lemma gives the continuous analog to Lemma 2.9. 
Lemma 8.1: Let X be a continuous random variable that takes on only nonnegative 
values. Then 
E[X] = 1'" Pr(X :-: x) dx. 
Proof: Let f(x) be the density function of X. Then 
1:, Pr(X:-: x)dx = 1: l:f (Y)dydx 
= l:Lo f(y)dxdy 
= 1: yf(y)dy 
= E[X]. 
The interchange of the order of the integrals is justified because the expression being 
integrated is nonnegative. 
• 
190 

8.1 CONTINUOUS RANDOM VARIABLES 
8.1.2. loint Distributions and Conditional Probability 
The notion of a distribution function for a real-\'alued random variable easily general-
izes to mUltiple random variables. 
Definition 8.1: The joint distribution function or X alld Y is 
F (x, y) = Pr (X ~ x. }' ~ y). 
The variables X and Y have joint densit)'fullctioll f ~r ./(J!' all x. y. 
j \' j'\ 
F(x, y) = 
~:X: 
~:X: .lUt. t') du dt' . 
. ..\gain, we denote 
a2 
f(x, y) = o.r oy F(x. y) 
\\hen the derivative exists. These definitions are generalized to jL)i nt distribution func-
tions over more than two variables in the obvious way. 
Given a joint distribution function F(x, y) over X and Y. one Illay cunsider the lIlar-
'.filial distribution functions 
F x (x) = Pr (X ~ x), 
F y ( y) = Pr (}' ~ .' I . 
.. 1I1d the corresponding marginal density functions Ix (x) and f> ( -' I. 
Definition 8.2: The random variables X and Yare independent it: t(i!' all x ([lid y. 
Pr ( (X ~ x) n (Y ~ y)) = Pr (X ~ .r) Pr (Y ::= y). 
from the definition, two random variables are independent if and only if their joint 
Jhtribution function is the product of their marginal distribution functions: 
F(x,y) = Fx(x)Fy(y). 
It follows from taking the derivatives with respect to x and y that. if X and Yare inde-
0ndent, then 
f(x,y) = fx(x)fy(y) . 
.1nO this condition is sufficient as well. 
As an example, let a and b be positive constants, and consider the joint distribution 
:unction for two random variables X and Y given by 
F(x, y) = I -
e~ax -
e~by + e~(ux-+-h\1 
,)\ cr the range x, y 2: O. We can compute that 
Fx(x) = F(x, 00) = I -
e~(/X, 
.1110 similarly Fy Cv) = I -
e~h\'. Alternatively, we could compute 
191 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
f(x, y) = abe-(ax+b\'), 
from which it follows that 
Fx(z) = j;: jX! abe-(ax+b\') dy dx = j;: _ae-ax = I - e-a;:. 
x=O 
\,=0 
X=O 
We obtain 
F(x, y) = 1 - e-ax - e-b\' + e-(ax+b\') = (1 - e-aX)(l -
e-lH') = Fx(x)Fy(Y), 
so X and Yare independent. Alternatively, working with the density functions we ver-
ify their independence by 
fx(x) = ae-ax, 
fy(y) = be-b\', 
f(x, y) = fx(x)fy(y)· 
Conditional probability for continuous random variables introduces a nontrivial sub-
tlety. The natural definition, 
Pr(E I F) = Pr(E n F) 
Pr(F) 
, 
is suitable when Pr(F) -=J- O. For example, 
Pr((X < 3) n (Y < 6)) 
Pre X < 3 I Y < 6) = 
-
-
-
-
Prey :::=; 6) 
when Prey :::=; 6) is not zero. 
In the discrete case, if Pr(F) = 0 then Pr(E I F) was simply not well-defined. In 
the continuous case, there are well-defined expressions that condition on events that 
occur with probability O. For example, for the joint distribution function F(x, y) 
I - e-ax - e-b\' + e-(ax+b\,) examined previously, it seems reasonable to consider 
Pr (X :::=; 3 I Y = 4), 
but since Prey = 4) is an event with probability 0, the definition is not applicable. 
If we did apply the definition, it would yield 
Pr ( (X < 3) n (Y = 4)) 
Pr (X < 3 I Y = 4) = 
-
. 
-
Prey = 4) 
Both the numerator and denominator are zero, suggesting that we should be taking a 
limit as they both approach zero. The natural choice is 
Pr (X :::=; 3 I Y = 4) = lim Pr (X :::=; 3 I 4 :::=; Y :::=; 4 + ()). 
0-+0 
This choice leads us to the following definition: 
Pr (X :::=; x I Y = y) = 
--' 
~- d U. 
l' 
feu v) 
l/=-C;G fy(y) 
To see informally why this is a reasonable choice, consider 
192 

8.2 THE UNIFORM DISTRIBUTION 
lim Pre X -:=; x I v -:=; Y -:=; V + 6) 
0---+0 
-
• 
1
. 
Pr ( (X -:=; x) n (y -:=; Y -:=; y + 6)) 
= Im------------------------
(S ---+ 0 
Pr (y -:=; Y -:=; y + 6) 
. 
F(x, \' + 6) -
F(x, v) 
= hm· 
. 
0---+0 Fy(y + 6) - Fy(y) 
3F(u, \' + 6)/3x - 3F(u, v)/ax 
-
-
dl{ 
Fy(y + 6) -
Fy(y) 
1
x 
= lim 
0---+0 
ll=-X 
l' . (3F(u, y + 6)/3x - 3F(u, y)/3X)/6 
= 
hm 
du 
l/=-X 0---+0 
(Fy(y + 6) -
Fy(Y))/6 
l' f(u,y) 
= 
-----duo 
I/=-X fyev) 
Here we have assumed that we can interchange the limit \vith the integration and that 
tr(y) #- O. 
The value 
h also called a conditional densityfllllctioll. \Ve may simi larly use 
. 
(( r. \") 
I) \ (.\. \") = --. --'-. 
. 
t \ (.\ ) 
Our definition yields the natural interpretation that. in order to compute Pre X -:=; .r 
}" = y), we integrate the corresponding conditional density function over the appropri-
Jte range. You can check that this definition yields the standard definition for Pre X -:=; x I 
}" -:=; y) through appropriate integration. Similarly, we may compute the conditional 
cxpectation 
E[X [ y = y] = l~x x/xlY(x,y)dx 
;J,ing the conditional density function. 
For our example, when F(x, y) = 1 - e-ax - e- br + e-(ax+hr). it follows that 
1
3 abe-ax+4b 
0 
Pr(X-:=;3IY=4)= 
du=l-e-·'(/. 
l/=O 
he-4b 
.1 result we could also have achieved directly using independence. 
8.2. The Uniform Distribution 
\\"hen a random variable X assumes values in the interval [a, b] such that all subinter-
\ .11" of equal length have equal probability, we say that X has the uniform distribution 
\.)\ er the interval [a, b] or alternatively that it is uniform over the interval [a, b]. We de-
:":\)te such a random variable by U[a,bJ. We may also talk about uniform distributions 
193 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
I 
-------------------~-------
b-a 
---------~i--------~ 
a 
b 
a 
b 
(a) f(x) = "~,,, (j :s x :s b. 
(b) F(x) = 
, a :s x :s b. 
Figure 8.2: The uniform distribution. 
over the interval [a,b), (a,b], or (a, b). Indeed, since the probability of taking on any 
specific value is ° 
when b > a, the distributions are essentially the same. 
The probability distribution function of such an X is 
F(x) = 
I 
~-([ 
b-([ 
and its density function is 
{(xl = I ~~" 
ifx:su, 
if u :s x :s b, 
if x ~ b, 
if x < a, 
if a :s x :s b, 
if x > b. 
These are shown in Figure 8.2. 
The expectation of X is 
I
b 
x 
b 2 -a 2 
E[X] = 
-- dx = ---
([ b - a 
2(b - a) 
and the second moment is 
') 
Ib x 2 
b 3 - a 3 
E[X~] = 
-- dx = ---
([ 
b - a 
3(b - a) 
The variance is computed by 
b+a 
2 
b 2 +ab+a 2 
Var[X] = E[X2] - (E[X])2 = ----
3 
(b + a)2 
4 
(b -
a)2 
12 
In our continuous roulette example, the outcome X of the experiment has a uniform 
distribution over [0, 1). Thus, the expectation of X is 1/2 and the variance of X is 1/12. 
8.2.1. Additional Properties of the Uniform Distribution 
Suppose you have a random variable X chosen from a uniform distribution, say over 
[0, 1], and it is revealed that X is less than or equal to 1/2. With this information, the 
conditional distribution of X remains uniform over the smaller interval [0,1/2]. 
194 

8.2 THE UNIFORM DISTRIBUTION 
Lemma 8.2: Let X be a un(lorm random variable 011 [a. b]. Then, for c :s d. 
c-a 
Pr(X < c I X < d) = --. 
-
-
d-a 
TI/{/t is, conditioned on the fact that X :s d. X is un(lorm on [a. d]. 
Proof: 
Pr ( (X < c) n (X < d)) 
Pr (X < c I X < d) = 
-
-
-
-
Pr(X :s d) 
Pre X :s c) 
Pr(X :s d) 
c-{/ 
d-a 
It follows that X. conditioned on being less than or equal to d. has a distribution func-
t i on that is exactly that of a uniform random variable on [a. d]. 
• 
Of course, a similar statement holds if we consider Pre X :s c I X 2: d): conditioned 
\)ll X 2: d, the resulting distribution is uniform over [d,b]. 
Another fact about the uniform distribution stems from the intuition that. if II points 
.1r~ uniformly distributed over an intervaL we expect them to be roughly ~qually spaced. 
\\'~ can codify this idea as follows. 
Lemma 8.3: Let XI. X.2 ..... XII be independent lIni/emn wndolli \'oriohles (i\'e/' [0. Il, 
Let YI. Y.2, ... , YII he the same rallies (IS XI. X.2 . .... X" in increosing sorted order. Then 
E[ Y;J = k/(n + 1). 
Proof' Let us first prove the result for YI with an explicit calculation. By d~tinition. 
r = min(XI ,X.2 .... ,XII ). Now 
Pr(YI 2: y) = Pr(min(XI , X.2, .... XII) 2: y) 
= Pr ( ( X I 2: y) n (X.2 2: y) n ... n (X II 2: y)) 
11 
= fl Pr ( Xi 2: y) 
i=1 
I: fl)llows from Lemma 8.1 that 
E[YIJ=jl (l-y)lIdJ,=_l_. 
y=() 
n + 1 
-\lternatively. one could use F(y) = I - (l -
y)1I so that the density function of YI is 
• I \) = 11 (1 - y) II-I, and hence using integration by parts yields 
195 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
P4 
0 
I 
Po 
Ps 
P2 
PI 
P4 
P3 P6 
Figure 8.3: A correspondence between random points on a circle and random points on a line. 
This analysis can be extended to find E[Ykl with some computation, which we leave 
as Exercise 8.5. A simpler approach, however, makes use of symmetry. Consider the 
circle of circumference 1, and place n + 1 points Po, PI, ... , PI! independently and uni-
formly at random on the circle. This is equivalent to choosing each point by a spin of 
the continuous roulette wheel of Section 8.l.1. Label the point Po as 0, and let Xi be 
the distance traveling clockwise from Po to Pi. The Xi are then independent, uniform 
random variables from [0,1]. The value Yk is just the distance to the kth point reached 
traveling clockwise from Po. See Figure 8.3. 
The distance between Yk and Yk+1 is the length of the arc between the two corre-
sponding adjacent points. By symmetry, however, all of the arcs between adjacent 
points must have the same expected length. The expected length of each arc is there-
fore 1/(n + 1), since there are n + 1 arcs created by the n points and since their total 
length is 1. By the linearity of expectations, E [Yk ] is the sum of the expected lengths 
of the first k arcs, and hence E[Ykl = kl(n + 1). 
• 
This proof makes use of an interesting one-to-one correspondence between choosing 
n points independently and uniformly at random from [0, 1] and choosing n + 1 points 
independently and uniformly at random from the boundary of the circle with circumfer-
ence 1. Such relationships, when they are available, can often greatly simplify an other-
wise lengthy analysis. We develop other similar relationships throughout this chapter. 
8.3. The Exponential Distribution 
Another important continuous distribution is the exponential distribution. 
Definition 8.3: An exponential distribution with parameter e is given by thefollowing 
probability distribution function: 
{
I - e-8x 
for x 2: 0, 
F(x) = ° 
otherwise. 
196 

8.3 THE EXPONENTIAL DISTRIBUTION 
! 1j- - - - - - - - - - - - - - - - - - - - -=--=-=-=--= 
I 
/ 
----------~~--~~~~=====--
(a) f(x) = 8e-lIl , t :::: O. 
(b) F(x) = 1 - e- 11t , t :::: O. 
Figure 8.4: The exponential distribution. 
The density function of the exponential distribution is 
j(x) = ee-fix 
for x 2: o. 
See Figure 8.4. 
lts first and second moments are 
E[X] = 
tee-fit dt = -, 
l
x 
I 
() 
(j 
1
'"'-
1 
, 
, 
-
H 
-
E[X-l = 
t-(je-
r cit = ---:;-. 
() 
H-
Hence, 
, 
, 
1 
Var[X] = E[X-] - (E[XJ)- = ---:;-. 
H-
8.3.1. Additional Properties of the Exponential Distribution 
Perhaps the most important property of the exponential distribution is that, like the dis-
.. :rete geometric distribution, it is memoryless. 
Lemma 8.4: For an exponential random variable with parameter e, 
Proof' 
Pr (X > s + t I X > t) = Pr (X > s). 
Pr(X > s + t) 
Pr(X > s + t I X > t) = -----
Pr(X > t) 
1 - Pr (X :s s + t) 
1 - Pre X :s t) 
e-fi(l+tl 
e- fit 
= e-&s 
= Pr(X > s). 
197 
• 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
The exponential distribution is the only continuous memory less distribution. It can 
be viewed as the continuous version of the discrete geometric distribution, which is 
the only discrete memoryless distribution. The geometric distribution models the time 
until first success in a sequence of independent identical Bernoulli trials, whereas the 
exponential distribution models the time until the first event in a memory less continu-
ous time stochastic process. 
The minimum of several exponential random variables also exhibits some interest-
ing properties. 
Lemma 8.5: If XI, X2, ... , Xn are independent exponentially distributed random vari-
ables with parameters e l, e2, ... , en, respectively, then mine XI, X2, ... , Xn) is exponen-
tially distributed with parameter L;·I=I ei and 
Proof' It suffices to prove the statement for two exponential random variables; the gen-
eral case then follows by induction. Let XI and X2 be independent exponential random 
variables with parameters e I and e2 . Then 
Pr(min(XI, X2) > x) = Pr((XI > x) n (X2 > x)) 
= Pr ( X I > x) Pr ( X 2 > x) 
Hence the minimum has an exponential distribution with parameter e I + e2 . 
Moreover, let f(xI,x2) be the joint distribution of (XI, X2). Since the variables are 
independent, we have f(xI,x2) = ele-&IXle2e-112X2. Hence 
Pr( XI < X2) = 1':0 1,~o f(XI,X,) dXI dx, 
e2 
=1----
el + e2 
el 
---
el + e2 
• 
For example, suppose that an airline ticket counter has n service agents, where the time 
that agent i takes per customer has an exponential distribution with parameter ei . You 
198 

8.3 THE EXPONENTIAL DISTRIBUTION 
stand at the head of the line at time To, and all of the 11 agents are busy. What is the 
average time you wait for an agent? 
Because service times are exponentially distributed, it does not matter for how long 
each agent has been helping another customer before time To; the remaining time for 
each customer is still exponentially distributed. This is a feature of the memoryless 
property of the exponential distribution. Lemma 8.5 therefore applies. The time un-
til the first agent becomes free is exponentially distributed with parameter L;I=1 fJi , so 
the expected waiting time is I/L;I=1 fJi . Indeed, you can even determine the proba-
bility that each agent is the first to become free: the jth agent is first with probability 
fJj /L;I=1 fJi · 
8.3.2.* Example: Balls and Bins with Feedback 
As an application of the exponential distribution, we consider an interesting variation 
of our standard balls-and-bins model. In this problem we have only two bins, and balls 
arrive one by one. Initially both bins have at least one ball. Suppose that. if bin 1 has 
x balls and bin 2 has y balls, then the probability that bin 1 obtains the next ball is 
x/(x + y) while the probability that bin :2 obtains the next ball is y/Cr + y). This sys-
tem has feedback: the more balls a bin has, the more balls it is likely to obtain in the 
future. An equivalent problem is given in Exercise 1.6. You may \vish to check (by in-
duction) that, if both bins start with one ball and there are II total balls. then the number 
of balls in bin 1 is uniformly distributed in the range [1. II -
1]. 
Suppose instead that we strengthen the feedback in the following way. If hin 1 has 
x balls and bin 2 has y balls, then the probability that bin 1 obtains the next ball is 
xP/(x P + yP) and the probability that bin :2 obtains the next ball is y'/(x Ji + yP) for 
some p > 1. For example, when p = :2, if bin 1 has three balls and bin 2 has four balls, 
then the probability that the next ball goes into bin I is only 9/25 < 3/7. Setting p > 1 
strengthens the advantage of the bin with more balls. 
This model has been suggested to describe economic situations that result in mo-
nopoly. For example, suppose there are two operating systems, Lindows and Winux. 
Users will tend to purchase machines with the same operating system that other users 
have in order to maintain compatibility. This effect might be nonlinear in the number 
of users of each system; this is modeled by the parameter p. 
We now show a remarkable result: as long as p > 1, there is some point at which 
one bin obtains all the rest of the balls thrown. In the economic setting, this is a very 
strong form of monopoly; the other competitor simply stops obtaining new customers. 
Theorem 8.6: Under any starting conditions, ~f p > 1 then with probability 1 there 
exists a number c such that one (~f the f}vo bins gets no more than c balls. 
~ote the careful wording of the theorem. We are not saying that there is some fixed c 
(perhaps dependent on the initial conditions) such that one bin gets no more than c balls. 
(If we meant this, we would say that there exists a number c such that, with probabil-
ity 1, one bin gets no more than c balls.) Instead, we are saying that. with probability 1, 
at some point (which we do not know ahead of time) one bin stops receiving balls. 
199 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Bin 1 
Bin 2 
Bins 1 and 2 
I I I 
II 
II 
I 
:: :: : 
I I I 
II 
II 
I 
II 
II 
I 
II 
II 
I 
Figure 8.5: In the setup where the time between ball arrivals is exponentially distributed, each bin 
can be considered separately; an outcome of the original process is obtained by simply combining 
the timelines of the two bins. 
Proof' For convenience, assume that both bins start with one ball; this does not affect 
the result. 
We start by considering a very closely related process. Consider two bins that start 
with one ball at time O. Balls arrive at each of the bins. If bin 1 obtains its zth ball at 
time t then it obtains its next ball at a time t + T~, where T~ is a random variable expo-
nentially distributed with parameter ;,/). Similarly, if bin 2 obtains its zth ball at time t 
then it obtains its next ball at a time t + V~, where V~ is also a random variable expo-
nentially distributed with parameter ;,/). All values of T~ and V~ are independent. Each 
bin can be considered independently in this setup; what happens at one bin does not 
affect the other. 
Although this process may not seem related to the original problem, we now claim 
that it mimics it exactly. Consider the point at which a ball arrives, leaving x balls in 
bin 1 and y balls in bin 2. By the memory less nature of the exponential distribution, 
it does not matter which bin the most recently arrived ball has landed in; the time for 
the next ball to land in bin 1 is exponentially distributed with mean x- P and the time 
for the next ball to land in bin 2 is exponentially distributed with mean y-p. Moreover, 
by Lemma 8.5, the next ball lands in bin 1 with probability xP/(x P + yP) and in bin 2 
with probability yP/(x P + yP). Therefore, this setup mimics exactly what happens in 
the original problem. See Figure 8.5. 
Let us define the saturation time F] for bin 1 by F] = L~] Tj , and similarly F2 = 
L~] Vj . The saturation time represents the first time in which the total number of balls 
received by a bin is unbounded. It is not clear that saturation times are well-defined 
random variables: What if the sum does not converge, and thus its value is infinity? It 
is here that we make use of the fact that p > 1. We have 
[X] xx 
1 
E[FJJ = E L 
1j = LE[1j] = L~' 
j=] 
j=] 
j=] } 
Here we used linearity of expectations for a countably infinite summation of random 
variables, which holds if L~] E rl1j I] converges. (Chapter 2 discusses the applicabil-
ity of the linearity of expectations to countably infinite summations; see in particular 
Exercise 2.29.) It suffices to show that L~] 1 /j P converges to a finite number when-
ever p > 1. This follows from bounding the summation by the appropriate integral: 
200 

8.4 THE POISSON PROCESS 
X) 
1 
lCX 1 
1 
,,- < 1 + 
-
du = 1 + --. 
L 
J'P -
_ u l) 
P -
1 
j=1 
II-I 
Indeed, all of the integral moments converge to a finite number. It follows that both Fl 
and F2 are, with probability 1, finite and hence well-defined. 
Furthermore, FI and F2 are distinct with probability 1. To see this, suppose that the 
\alues for all of the random variables T~ and V:: are given except for T I • Then, for Fl 
to equal F2 , it must be the case that 
TI = L Vj - L Tj . 
j=1 
j=2 
But the probability that TI takes on any specific value is 0, just as the probability that 
our roulette wheel takes on any specific value is 0. Hence, FI -=I=- F2 with probability 1. 
Suppose that FI < F2 . Then we must have for some n that 
11 
n+1 
L Vj < FI < L Vj . 
j=1 
j=1 
This implies that, for any sufficiently large number 111. 
1/ 
11/ 
1/-1 
1=1 
1=1 
/=1 
which means that bin 1 has obtained III balls before bin .2 ha..; obtained it..; (II -
l)th 
ball. Since our new process corresponds exactly to the original ball..;-and-bilb proce..;..;. 
this is also what happens in the original process. But this means that. once bin .2 ha..; II 
balls, it does not receive any others; they all go to bin I. The argument is the same if 
F~ < Fl. Hence, with probability 1, there exists some II such that one bin obtain..; no 
more than n balls. 
• 
\\'hen p is close to 1 or when the bins start with a large and nearly equal number of 
balls. it can take a long time before one bin dominates enough to obtain such a mo-
nopoly. On the other hand, monopoly happens quickly when p is much greater than I 
,..;uch as p = 2) and the bins start with just one ball each. You are asked to simulate 
this process in Exercise 8.24. 
8 ..... The Poisson Process 
The Poisson process is an important counting process that is related to both the uni-
!t)rm and the exponential distribution. Consider a sequence of random events. such as 
.1rri\'als of customers to a queue or emissions of alpha particles from a radioactive ma-
terial. Let N(t) denote the number of events in the interval [0, tl. The process {N(t), 
: .::: O} is a stochastic counting process. 
201 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Definition 8.4: A Poisson process vvith parameter (or rate) A is a stochastic counting 
process {N(t), t :::. o} such that the following statements hold. 
1. N(O) = 0. 
2. The process has independent and stationary increments. That is, for any t, s > 0, 
the distribution ofN(t + s) - N(s) is identical to the distribution of N(t), andfor 
any two disjoint intervals [t1,t2] and [t3,t4L the distribution of N(t2) -
N(tl) is 
independent of the distribution of N(t4) -
N(t3). 
3. limf-+oPr(N(t) = I)/t = A. That is, the probability (~fa single event in a short 
interval t tends to At. 
4. limf-+o Pr(N(t) :::. 2)/t = 0. That is, the probability of more than one event in a 
short interval t tends to zero. 
The surprising fact is that this set of broad, relatively natural conditions defines a unique 
process. In particular, the number of events in a given time interval follows the Poisson 
distribution defined in Section 5.3. 
Theorem 8.7: Let {N(t) It:::' O} he a Poisson process with parameter A. For any 
t, s :::. ° 
and any integer n :::. 0, 
, (At) 11 
PIl(t) = Pr(N(t + s) -
N(s) = n) = e-Af __ . 
n! 
Proof: We first observe that PIl(t) is well-defined since, by the second property of Def-
inition 8.4, the distribution of N(t + s) -
N(s) depends only on t and is independent 
of s. 
To compute Po (t), we note that the number of events in the intervals [0, t] and 
(t, t + h] are independent random variables and therefore 
Po(t + h) = Po(t)Po(h). 
We now write 
Po (t + h) -
Po ( t) 
Po ( h) - I 
------- = Po(t) ----
h 
h 
I - Pr (N ( h) = I) - Pr (N ( h) :::. 2) -
I 
= poet) 
h 
-Pr(N(h) = I) - Pr(N(h) :::. 2) 
= poet) 
h 
. 
Taking the limit as h ---+ ° 
and applying properties 2-4 of Definition 8.4, we obtain 
To solve 
I 
• 
Po (t + h) -
Po (t ) 
poet) = 11m -------
h-+O 
h 
. 
- Pr (N (h) = I) - Pr (N (h) :::. 2) 
= 11m Po(t)------------
/z-+o 
h 
= -APo(t)· 
p(;(t) = -APo(t), 
202 

8.4 THE POISSON PROCESS 
we rewrite it as 
Integrating with respect to t gives 
In poet) = -At + C, 
or 
poet) = e-At+C . 
Since PoCO) = 1, we conclude that 
For n ~ 1, we write 
n 
Pn(t + h) = L Pn-k(t)Pk(h) 
k=O 
n 
= Pn(t)Po(h) + Pn-] (t)p] (h) + L Pn- k (t) Pr(N(h) = k). 
k=2 
Computing the first derivative of Pn(t) yields 
I 
• 
Pn(t+h)-Pn(t) 
Pn(t) = lIm ------
h----+O 
h 
where we use the facts that 
I by properties 2 and 3) and 
1 
11 
Pre N (/1) > .2) 
o ~ lim - " 
Pn-k(t) Pr(N(h) = k) ~ lim 
-
= 0 
h----+O h L 
h---->() 
Iz 
I by property 4), so 
To solve 
\\c write 
\\hich gives 
k=2 
1 
n 
lim - L Pn-k(t) Pr(N(h) = k) = O. 
h----+oh k=2 
203 
(8.1) 
(8.2) 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Using (8.1) then yields 
implying 
Since PI (0) = 0, we conclude that 
We continue by induction on n to prove that, for all n ~ 0, 
(At)1I 
Pn(t) = e- Af __ • 
n! 
Using Eqn. (8.2) and the induction hypothesis, we have 
d 
' 
, 
Allt ll - I 
_(eAfPII(t)) = AeAfPII_I(t) = ---
dt 
(n-I)! 
Integrating and using the fact that PII(O) = 0 gives the result. 
(8.3) 
• 
The parameter A is also called the rate of the Poisson process, since (as we have proved) 
the number of events during any time period of length t is a Poisson random variable 
with expectation At. 
The reverse is also true. That is, we could equivalently have defined the Poisson 
process as a process with Poisson arrivals, as follows. 
Theorem 8.8: Let {N(t) I t ~ O} be a stochastic process such that: 
1. N(O) = 0: 
2. the process has independent increments (i.e., the number (~f events in disjoint tirne 
intervals are independent et'ents): and 
3. the numberolet'ents in an inten'ul (~f'length t has a Poisson distribution with mean At. 
Then {N (t) I t ~ O} is a Poisson process lvith rate A. 
Proof: The process clearly satisfies conditions I and 2 of Definition 8.4. To prove con-
dition 3, we have 
. 
Pr(N(t) = I) 
e-),fAt 
11m 
=lim--=A. 
f~() 
t 
f---->() 
t 
Condition 4 follows from 
. 
Pr(N(t) ~ 2) 
e- icf(At)2 
11m 
= 
= n. 
• 
f~() 
t 
2t 
8.4.1. Interarrival Distribution 
Let XI be the time of the first event of the Poisson process, and let Xn be the inter-
val of time between the (n -
I)th and the nth event. The XII are generally referred to 
204 

8.4 THE POISSON PROCESS 
as interarrival times, since they represent the time between arrivals of events. Here, 
we show that all of the XII have the same distribution and that this distribution is 
exponential. 
We begin by deriving the distribution of XI, 
Theorem 8.9: XI has an exponential distribution \\'ith parameter A. 
Proof: 
Pr ( X I > t) = Pr ( N (t) = 0) = e -I: , 
Thus, 
• 
Csing the fact that the Poisson process has independent and "tatilmary increments. we 
can prove the following stronger result. 
Theorem 8.10: The random variables Xi, i = 1, 2, .... are indt,/Jt'ndenr, idenrica/IY 
tlistributed, exponential random variables ),vith parameter i., 
Proof: The distribution of Xi is given by 
Pr(Xi > ti I (Xo = to) n (XI = t]) n··· n (X 1 -
1 = {_'II 
= e- Al;. 
Thus, the distribution of Xi is exponential with parameter i". and it i" independent of 
l)ther interarrival values. 
• 
Theorem 8.10 states that, if we have a Poisson arrival process, then the interarri\al times 
.ife identically distributed exponential random variables. In fact. it is ea,,~ to check that 
the reverse is also true (this is left as Exercise 8.17). 
Theorem 8.11: Let {N(t) I t ~ O} be a stochastic process such (/w{: 
1 . . Y(O) = 0; and 
2. the interarrival times are independent, identically distributed, nponentia/ random 
\'ariables with parameter A. 
Then {N(t) I t ~ O} is a Poisson process with rate A. 
SA.2. Combining and Splitting Poisson Processes 
The correspondence between Poisson processes and exponentially distributed inter-
JITi\al times is quite useful in proving facts about the behavior of Poisson processes. 
205 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
One immediate fact is that Poisson processes combine in a natural way. We say that 
two Poisson processes N](t) and N2(t) are independent ifand only if the values N](t) 
and N2(u) are independent for any t and u. Let N](t) + N2(t) denote the process that 
counts the number of events corresponding to both of the processes N] (t) and N2 (t)' 
We show that, if N] (t) and N2 (t) are independent Poisson processes, then they com-
bine to form a Poisson process N] (t) + N 2 (t). 
Theorem 8.12: Let N] (t) and N2 (t) be independent Poisson processes with parame-
ters A] and A2, respective!.v. Then N] (t) + N2 (t) is a Poisson process with parameter 
A] + A2, and each event of the process N](t) + N2(t) arisesfrom the process N](t) 
with probability A]/(A] + A2)' 
Proof: Clearly N] (0) + N2 (0) = 0, and since the two processes are independent and 
each has independent increments, the sum of the two processes also has independent 
increments. The number of arrivals N] (t) + N 2 (t) is a sum of two independent Pois-
son random variables, which (as we saw in Lemma 5.2) has a Poisson distribution with 
parameter A] + A2. Thus, by Theorem 8.8, N] (t) + N2(t) is a Poisson process with rate 
A] + A2. 
By Theorem 8.9, the interarrival time for N] (t) + N2 (t) is exponentially distributed 
with parameter A] + A2, and by Lemma 8.5 an event in N](t) + N2(t) comes from the 
process N](t) with probability A]/(A] + A2)' 
• 
The theorem extends to more than two processes by induction. 
It is interesting to note that Poisson processes can be split as well as combined. If 
we split a Poisson process with rate A by labeling each event as being either type I 
with probability p or type 2 with probability (1 -
p), then it seems that we should get 
two Poisson processes with rates AP and A(l - p). In fact, we can say something even 
stronger: the two processes will be independent. 
Theorem 8.13: Suppose that we have a Poisson process N(t) with rate A. Each event 
is independently labeled as being type I with probability p or type 2 with probabil-
ity 1- p. Then the type-l eventsform a Poisson process N](t) of rate AP, the type-2 
eventsform a Poisson process N2(t) of rate A(l - p), and the m'o Poisson processes 
are independent. 
Proof: We first show that the type-l events in fact form a Poisson process. Clearly 
N] (t) = 0, and since the process N (t) has independent increments, so does the process 
N] (t). Next we show that N] (t) has a Poisson distribution: 
Pr (N] (t) = k) = L Pr (N] (t) = kiN (t) = j) Pr (N (t) = j) 
j=k 
206 

8.4 THE POISSON PROCESS 
e -),IJt pc pt) ~ 
k! 
Thus, by Theorem 8.8, N1(t) is a Poisson process with rate i.p. 
To show independence, we need to show that ,\'1 (r ) and X~ (I{) are independent for 
any t and u. In fact, it suffices to show that NI (r) and .\'2 ( r) are independent for any t: 
we can then show that N1(t) and N 2(u) are independent for any rand [{ by taking ad-
\antage of the fact that Poisson processes have independent and stationary increments 
(see Exercise 8.18). We have: 
Pr ( (N1 (t) = m) n (N 2 (t) = n)) = Pr ( (N (t) = 111 -
II) -
I.\'~ ([) = 11)) 
e-i.f(At)"I-n (111 - II) " 
I 
= 
11 II -
f» 
I 
(111 + II) ~ 
, 
II 
e-t,f (A t)1I/-1i 
'I' 
' 
-----/1 '(I -
III 
m!n! 
e-)·fP(Atp)1I/ e-i"I-" li,lll -
f>11' 
m! 
= Pr(N1(t) = 111) Pr( .\'2 ( II = 1/ I. 
SA.3. Conditional Arrival Time Distribution 
• 
\\'e have used the fact that a Poisson process has independent increment" tl) "hl)\\ that 
the distribution of the interarrival times is exponential. Another applicatil)n ()f thi" ~h­
'lImption is the following: Ifwe condition on exactly one e\'ent occllrrin~ in an inten al. 
then the actual time at which that event occurs is uniformly distributed (ncr that inter-
\al. To see this, consider a Poisson process where N(t) = 1. and cl)n"iuer the time XI 
,,1' the single event that falls in the interval (0, t]: 
Pr((X1 < s) n (N(t) = I)) 
Pr(X1 < s I N(t) = 1) = ---------
Pr(N(t) = I) 
Pr((Ne\') = I) n (N(t) -
S(\) = 0) I 
Pr(N(t) = I) 
(Ase- AS )e-f.(f-S) 
Ate-At 
s 
Here we have used the independence of N (s) and N (t) -
N (s ). 
To generalize this to the case of N(t) = n, we use the concept of order statistics. Let 
\' ..... XII be n independent observations of a random variable. The order statistics of 
\' ..... XII consists of the n observations in (increasing) sorted order. For example, if 
\' . X 2. X:I, X 4 are independent random variables generated by taking a number chosen 
207 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
uniformly on [0,1] and rounding to two decimal places, we might have X] = 0.47, 
X 2 = 0.33, X3 = 0.93, and X 4 = 0.26. The corresponding order statistics, where Y(il 
is used to refer to the ith smallest, would be YO) = 0.26, Y(2) = 0.33, Y(3) = 0.47, and 
Y(4) = 0.93. 
Theorem 8.14: Given that N(t) = n, the n arrival times have the same distribution as 
the order statistics of n independent random variables with uniform distribution over 
[0, t]. 
Proof: We first compute the distribution of the order statistics of n independent ob-
servations X], X2, ... , XII drawn from a uniform distribution in [0, t]. Let Y(l),"" Y(lll 
denote the order statistics. 
We want an expression for 
Let £ be the event that 
For any permutation iI, i2, ... , ill of the numbers from 1 to n, let £il.i2 ..... i" be the event 
that 
The events £il.i~ .. ".i" are disjoint, except for the cases where Xii = Xii+1 for some j. 
Since two uniform random variables are equal with probability 0, the total probabil-
ity of such events is ° 
and can be ignored. By symmetry, all events £il.i2, ... )" have the 
same probability. Also, 
where the union is over all permutations. It follows that 
Pr (Y(!) ::: s], Y(:::) ::: s:::, ... , Y( 11) ::: s 11 ) 
= L Pre Xii ::: 5], Xii ::: X i2 ::: s2, "', Xi,,_1 ::: Xi" ::: Sll) 
= n! Pr ( X] ::: 5] , X] ::: X 2 ::: s 2, ... , X n -] ::: XII::: s 11 ) , 
where the sum in the second line is over all n! permutations. If we now think of Ui as 
representing the value taken on by Xi, then 
Pr ( X] ::: s], X] ::: X 2 ::: 52, ... , X 11-] ::: XII::: Sll) 
= L~() L:", ... L~",j ~ )" du" .. duJ, 
where we use the fact that the density function of a uniform random variable on [0, t] 
is f(t) = lit. This gives 
1
,1'11 
ll,,-Li,,_1 
208 

8.4 THE POISSON PROCESS 
We now consider the distribution of the arrival times for a Poisson process, condi-
tioned on N(t) = n. Let 51, ... ,511+1 be the first n + I arrival times. Also. let TI = 
51 and Tf = 5f -
5f - 1 be the length of the interarrival intervals. By Theorem 8.10. we 
know that (a) without the condition N(t) = n, the distributions of the random vari-
ables TI , ••• , Tn are independent, and (b) for each i, Tf has an exponential distribution 
with parameter A. Recalling that the density function of the exponential distribution is 
ice-At, we have 
Integrating with respect to tn+1 then yields 
An+ e- A Li=1 t, dt 
= _Xl e- A Li=1 t, 
, 
I 
,(",11+1.) 
[ 
"",,,+1] x 
11+1 
rl1~l=r- ~'=I r 
Thus, 
1
.1'11 
... 
dUll'" dUI, 
1I,,=1l,,_1 
\I. here the last equation is obtained by substituting Uf = 2::)=1 ti. 
Since 
(At)11 
Pr(N(t) = n) = e- At __ 
n! 
J.nd because the number of events in an interval of length t has a Poisson distribution 
\\ ith parameter At, the conditional probability computation gives 
Pr ( 5 I ::: S I, 52 ::: S 2, ..., 5 n ::: S 11 
I N ( t) = n) 
Pr ( 5 I ::: S I, 52 ::: S 2, ..., 511 ::: S 11, N ( t) = 11) 
Pr(N(t) = n) 
This is exactly the distribution function of the order statistics, proving the theorem. 
• 
209 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
8.5. Continuous Time Markov Processes 
In Chapter 7 we studied discrete time and discrete space Markov chains. With the 
introduction of continuous random variables, we can now study the continuous time 
analogue of Markov chains, where the process spends a random interval of time in a 
state before moving to the next one. To distinguish between the discrete and continu-
ous processes, when dealing with continuous time we speak of Markov processes. 
Definition 8.5: A continuous time random process {Xt It:::: O} is Markovian (or is 
called a Markov process) if: for all s, t :::: 0: 
Pr ( X (s + t) = x I X (u ), 0 ~ u ~ t) = Pr ( X (s + t) = x I X ( t ) ), 
and this probabili(v is independent qfthe time t.? 
The definition says that distribution of the state of the system at time Xes + t), condi-
tioned on the history up to time t, depends only on the state XU) and is independent of 
the particular history that led the process to state X (t). 
Restricting our discussion to discrete space, continuous time Markov processes, 
there is another equivalent way of formulating such processes that is more convenient 
for analysis. Recall that a discrete time Markov chain is determined by a transition ma-
trix P = (Pi.)), where Pi.) is the probability of a transition from state i to state j in 
one step. A continuous time Markov process can be expressed as a combination of two 
random processes as follows. 
1. A transition matrix P = (Pi.)), where Pi,) is the probability that the next state 
is j given that the current state is i. (We use lowercase letters here for the transi-
tion probabilities in order to distinguish them from the transition probabilities for 
corresponding discrete time processes.) The matrix P is the transition matrix for 
what is called the embedded or skeleton Markov chain of the corresponding Markov 
process. 
2. A vector of parameters (8 1, 8?, ... ) such that the distribution of time that the process 
spends in state i before moving to the next step is exponential with parameter 8i . 
The distribution of time spent at a given state must be exponential in order to satisfy 
the memoryless requirement of the Markov process. 
A formal treatment of continuous time Markov processes is more involved than their 
discrete counterparts, and a full discussion is beyond the scope of this book. We limit 
our discussion to the question of computing the stationary distribution (also called 
equilibrium distribution) for discrete space, continuous time processes, assuming that 
a stationary distribution exists. As for the discrete time case, the value Jri in a stationary 
distribution if gives the limiting probability that the Markov process will be in state i 
infinitely far out in the future, regardless of the initial state. That is, if we let Pj.i(t) be 
the probability of being in state i at time t when starting from state j at time 0, then 
-' Technically, as with the discrete time Markov chains, this is a time-homogeneous Markov process: this will be 
the only type we study in this book. 
210 

8.5 CONTINUOUS TIME MARKOV PROCESSES 
lim Pj.i(t) = Jrj. 
[---->x 
Similarly, Jri gives the long-term proportion of the time the process IS In state i. 
Furthermore, if the initial state j is chosen from the stationary distribution. then the 
probability of being in state i at time t is Jri for all t. 
To determine the stationary distribution, consider the derivative P;/t): 
P ' ( ) -
l' 
Pj.i(t + h) -
Pj,i(t) 
.. t -
1m -"--------"---
j,1 
/z---->O 
h 
1
. 
Lk Pj,dt)Pk.i(h) -
Pj.i(t) 
= Im-----------
/z---->() 
h 
( 
P" ·(h) 
1 -
p. ·(h) 
) 
= lim ,,_·.I_ p . k(t) _ 
1,1 
p. ·(t) . 
/z---->() L 
h 
j. 
h 
j.1 
k¥-i 
Since the distribution of time spent at state k is exponential with parameter fh. we 
can use the properties of the Poisson process to observe that, as h tends to zero. the 
limiting probability of a transition out of state k in an interval of length Iz is Izfh. and 
the limiting probability of more than one transition is O. Thus. 
. 
Pk.i (/1 ) 
hm ----- = H"Pk,' 
h---->() 
Iz 
Similarly, 1 - Pi.i(h) is the probability that a transition occurs O\cr the intcnal of time 
h. and the transition is not from state i back to itsclf. Thus. 
We now assume that we can interchange the limit and the summation: we empha-
,ize that this interchange is not always justified for countably infinite spaces. Subject 
tlJ this assumption, 
lim (" Pk.i(h) p. k(t) _ 1 -
Pu(h) p .. (t)\ 
/z---->O L 
h 
j. 
h 
j.1 
') 
I.;¥-i 
= L fhpk.iPj.k(t) -
Pj.i(t)(f)i -
f)iPi.i) 
"¥-i 
= L f)kPk.iPj.d t ) - f)iPj.i(t). 
k 
Taking the limit as t ---+ 00, we have 
If the process has a stationary distribution, it must be that 
lim P/~i(t) = O. 
{---->x' 
211 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Otherwise, Pj,j(t) would not converge to a stationary value, Hence, in the stationary 
distribution if we have the following rate equations: 
Jr/i j = L Jr" 8" Pu· 
" 
(8,4 ) 
This set of equations has a nice interpretation, The expression on the left, Jr j 8j , is the 
rate at which transitions occur out of state i. The expression on the right, L" Jr"8,, IJ;.;,j, 
is the rate at which transitions occur into state i. (A transition that goes from state i 
back to state i is counted both as a transition into and as a transition out of state i.) 
At the stationary distribution, these rates must be equal, so that the long-term rates of 
transitions into and out of the state are equal. This equalization of rates into and out 
of every state provides a simple, intuitive way to find stationary distributions for con-
tinuous Markovian processes, This observation can be generalized to sets of states, 
showing that a result similar to the cut-set equations of Theorem 7,9 for discrete time 
Markov chains can be formulated for continuous time Markov processes. 
If the exponential distributions governing the time spent in all of the states have the 
same parameter, so that all the ()j are equal, then Eqn, (8.4) becomes 
This corresponds to 
Jrj = LJr"Pk,j, 
" 
if = ifP, 
where P is the transition matrix of the embedded Markov chain. We can conclude that 
the stationary distribution of the continuous time process is the same as the stationary 
distribution of the embedded Markov chain in this case. 
8.6. Example: Markovian Queues 
Queues appear in many basic applications in computer science. In operating systems, 
schedulers can hold tasks in a queue until the processor or other required resources are 
available, In parallel or distributed programming, threads can queue for a critical sec-
tion that allows access to only one thread at a time, In networks, packets are queued 
while waiting to be forwarded by a router. Even before computer systems were preva-
lent, queues were widely studied to understand the performance of telephone networks, 
where similar scheduling issues arise. In this section we analyze some of the most basic 
queueing models, which use Poisson processes to model the stochastic process of cus-
tomers arriving at a queue and exponentially distributed random variables to model the 
time required for service. 
In what follows, we refer to queue models using the standard notation Y/ Z /n, where 
Y represents the distribution of the incoming stream of customers, Z represents the ser-
vice time distribution, and n represents the number of servers. The standard notation 
for a Markovian or memoryless distribution is M. Thus, M/ M/n stands for a queue 
model with customers arriving according to a Poisson process and served by n servers 
212 

8.6 EXAMPLE: MARKOVIAN QUEUES 
having identical and independent exponentially di stributed service times. Other queue-
ing models include the M 1M 100 model, where there are an infinite number of servers, 
and the MIG/l model, where the G indicates that the sen'ice time can be any arbitrary 
general distribution. 
A queue must also have a rule for determining the order in which customers are 
"erved. Unless otherwise specified, we assume that a ljueue follows the First In First 
Out (FIFO) rule, where customers are served in order of their arri\al. 
8.6.1. M / M /1 Queue in Equilibrium 
,-\ssume that customers arrive to a queue according to a P()i""on pn)l'ess with parame-
ter A, and assume they are served by one server. The senil'e time" for the cllstomers 
Jre independent and exponentially distributed with parameter Ii, 
Let M(t) be the number of customers in the queue at time I, SinLe both the arrival 
process and the service time have memoryless distributions. the proLe"" (,\III) 
I ~ O} 
defines a continuous time Markov process. We consider the "tatil)nar~ di"tribution for 
this process. 
Let 
Pdt) = Pr(M(t) = k) 
denote the probability that the queue has k customers at time I. \\'c u"e thc faLt that. 
In the limit as h approaches 0, the probability of an arrival (respeLti\cl~. a dcparture) 
\1\er a time interval is Ah (respectively, /-lh). Thus, 
.1nd for k ~ 1, 
dPo(t) 
. 
p()(t + h) -
poet) 
----- = lIm --------------
dt 
h-'>() 
h 
. 
Po(t)(l -
Ah) + P1(t)/-lh -
Pili!) 
= lIm --------------------------
h~O 
h 
= -APO(t) + /-lP1(t), 
dPdt) 
. 
Pk(t + h) -
Pk(t) 
----- = lIm --------------
dt 
h-'>O 
h 
. 
Pk(t)(l- Ah -
/-lh) + Pk-1(t)Ah + Pk- I([ )/(/z -
P. ({ I 
= lIm --------------------------------------------
In equilibrium, 
h-'>() 
h 
dPdt) 
-- = ° for k = 0, 1, 2, .... 
dt 
If the system converges to a stationary distribution 3 fr. then applying Eqn. (8.5) 
~ relds 
\;,lin, the proof that the system indeed converges relies on renewal theory and i~ beyond the scope of this book, 
213 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
This equation has a simple interpretation in terms of rates. In equilibrium, the rate into 
the state where there are no customers in the queue is /-IJr]: the rate out is AJro. These 
two rates must be equal. If we write this as Jr] = JrO(A//-I), then (8.6) and a simple 
induction give 
Since Lk>O Jrk = L we must have 
-
JT 0 L (~)k = 1. 
k?:O 
/-I 
(8.7) 
Assuming that A < /-I, it follows that 
JT 0 = 1 - ~ and 
JT, = ( 1 - ~) (~ r 
If A > /-I, then the summation in Eqn. (8.7) does not converge and, in fact, the system 
does not reach a stationary distribution. This is intuitively clear; if the rate of arrival 
of new customers is larger than the rate of service completions, then the system cannot 
reach a stationary distribution. If A = /-I, the system also cannot reach an equilibrium 
distribution, as discussed in Exercise 8.22. 
To compute the expected number of customers in the system in equilibrium, which 
we denote by L, we write 
A 
::x:: 
( 
A) (A )k-] 
=-Lk 1--
-
/-I k=] 
/-I 
/-I 
/-II - A//-I 
A 
/-I- A' 
where in the third equation we used the fact that the sum is the expectation of a geo-
metric random variable with parameter 1 - A//-I. 
It is interesting that we have nowhere used the fact that the service rule was to serve 
the customer that had been waiting the longest. Indeed, since all service times are 
exponentially distributed and since the exponential distribution is memoryless, all cus-
tomers appear equivalent to the queue in terms of the distribution of the service time 
required until they leave, regardless of how long they have already been served. Thus, 
our equations for the equilibrium distribution and the expected number of customers in 
the system hold for any service rule that serves some customer whenever at least one 
customer is in the queue. 
Next we compute the expected time a customer spends in the system when the sys-
tem is in equilibrium, denoted by W, assuming a FIFO queue. Let L(k) denote the 
event that a new customer finds k customers in the queue. We can write 
214 

8.6 EXAMPLE: MARKOVIAN QUEUES 
W = L E [W I L ( k)] Pr ( L ( k ) ) . 
k=O 
Since the service times are independent, memoryless. and have expectation 1/ j.1, it fol-
lows that 
I 
E[W I L(k)] = (k + 1)-. 
Ii 
To compute Pr(L(k)), we observe that if the system is in equilibrium then the rate 
of transitions out of state k is Jrk fh, where 80 = ), and HI. = i. + fJ. for k ~ l. Applying 
Lemma 8.5, the probability that the next transition from state k is caused by the arrival 
of a new customer is A/8k . Therefore, the rate at which customers an"ive and find k 
customers already in the queue is 
A 
Jrk8k- = Jrk A. 
8k 
Since the total rate of new arrivals to the system is A. we cone lude that the probability 
that a new arrival finds k customers in the system is 
Jrk A 
Pr(L(k)) = -
= Jrk. 
A 
This is an example of the PASTA principle, which states that Poi""on .-\.rri\als See 
Time Averages. That is, if a Markov process with Poisson arri\ab hi.b a ..,tationary dis-
tribution and if the fraction of time the system is in state k is :T •. then :T; i" al"o the 
proportion of arrivals that find the system in state k when they arri\e. The PAST.-\. 
principle, which is due to the independence and memory less propertie" of the Poi..,son 
process, is a useful tool that often simplifies analysis. A proof of the PAST-\. principle 
for more general situations is beyond the scope of this book. 
We can now compute 
W = L E[W I L(k)] Pr(L(k)) 
k=O 
= HI + I:knk) 
k=O 
I 
= -(1+L) 
j.1 
= ~(1 + _A_) 
j.1 
j.1-A 
I 
j.1-A 
L 
A 
215 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
The relationship L = A W is known as Little's result, and it holds not only for MIMI 1 
queues but for any stable queueing system. 
The proof of this fundamental result is 
beyond the scope of this book. 
Although the MIMI I queue represents a very simple process, it can be useful for 
studying more complicated processes. For example, suppose that we have several types 
of customers entering a queue, with each type arriving according to a Poisson process, 
and that all customers have exponentially distributed service times of mean fl. Since 
Poisson processes combine, the arrival process to the queue is Poisson, and this can be 
modeled as an MIMI I queue. Similarly, suppose that we have a single Poisson arrival 
process, and we establish a separate queue for each type of customer. If each arriving 
customer is of type i with some fixed probability Pi, then the Poisson process splits 
into independent Poisson processes for each type of customer, and hence the queue for 
each type is an M 1M 11 queue. This type of splitting might occur, for example, if we 
use separate processors for different types of jobs in a computer network. 
8.6.2. M / M /1/ K Queue in Equilibrium 
An MIMI 11K queue is an MIMI I queue with bounded queue size. If a customer ar-
rives while the queue already has K customers, then this customer leaves the system 
instead of joining the queue. Models with bounded queue size are useful for applica-
tions such as network routers, where packets that arrive once the packet buffer is full 
must be dropped. 
The system is entirely similar to the previous example. In equilibrium we have 
and 
for k :'S K, 
for k > K, 
These equations define a proper probability distribution for any A, fl > 0, and we no 
longer require that A < fl. 
8.6.3. The Number of Customers in an M / M /00 Queue 
Suppose new users join a peer-to-peer network according to a Poisson process with 
rate A. The length of time a user stays connected to the network has exponential dis-
tribution with parameter fl. Assume that, at time 0, no users were connected to the 
network. Let M (t) be the number of connected users at time t. What is the distribution 
of M(t)? 
We can view this process as a Markovian queue with an unlimited number of servers. 
A customer starts being served the moment she joins the system and leaves when she is 
done. We demonstrate two ways of analyzing this process. We first use the rate equa-
tions (8.4) to compute the stationary distribution for the process. The second approach 
216 

8.6 EXAMPLE: MARKOVIAN QUEUES 
is more complex, but it yields more information: we explicitly compute the distribu-
tion of the number of customers in the system at time t and then consider the limit as t 
goes to infinity. 
To write the rate equations of the process. we observe that if (at a given time) there 
are k :::: 0 customers in the system, then the next event can be either termination of ser-
\'ice of one of the k current customers or the arrival of a new customer. Thus, the time 
to the first event is the minimum of k + I independent exponentially distributed random 
\'ariables; k of these variables have parameter fl. and one has parameter A. Applying 
Lemma 8.5 shows that, when there are k customers in the sy"tem. the time to first event 
has an exponential distribution with parameter f)i = k;l -
i" Furthermore, the lemma 
implies that, given that an event occurs, the probability that the e\ent is an arrival of a 
new customer is 
A 
Pk.k+1 = A + kfl . 
and when k :::: I the probability that the event is the departure of a l.'lhtomer is 
kfl 
Pk,k-I = A + kfl . 
Plugging these values into (8.4), we have that the stationary di"tributil))1 :7 '<.lti"tle" 
and, for k :::: 1, 
\Ve rewrite (8.8) as 
Jrk+l(k + l)fl = JrdA + kfl) -
Jrk-I)~ 
= Jrk A + Jrkkfl -
Jrk-I/" 
.-\ simple induction yields that 
and therefore 
A 
Jr 
-
Jr 
k+1 -
fl(k + 1) 
k· 
~ow, again a simple induction yields 
.md therefore 
\\'e conclude that Jro = e-A/f-l. and, more generally, 
217 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
e- A/ J1 (A/ fl )k 
Jrk = 
k! 
so that the equilibrium distribution is the discrete Poisson distribution with parameter 
A/fl· 
We now proceed with our second approach: computing the distribution of the num-
ber of customers in the system at time t, denoted by M(t), and then considering the 
limit of M (t) as t goes to infinity. Let N (t) be the total number of users that have 
joined the network in the interval [0, t]. Since N(t) has a Poisson distribution, we can 
condition on this value and write 
OC 
, (At)1I 
Pr (M (t) = j) = "Pr (M (t) = j 
I N (t) = n) e - A t -- . 
(8.9) 
L 
n! 
11=0 
If a user joins the network at time x, then the probability that she is still connected 
at time t is e-rl(t-X). From Section 8.4.3, we know that the arrival time of an arbitrary 
user is uniform on [0, t]. Thus, the probability that an arbitrary user is still connected 
at time t is given by 
p = 
e-Il(t-x) _ 
= _(I _ e-Ilf ). 
i
f 
dx 
I 
() 
t 
flt 
Because the events for different users are independent, for j .:s n we have 
Pr ( M (t) = j I N (t) = 11) = C) p j (I - p)" - J 
Plugging this value into Eqn. (8.9), we find that 
~ (n) . 
. ,(At)'1 
Pr(M(t) = j) = L 
j 
pJ(l -
p)lI- j e-u---;;! 
11=.1 
= e-i,f (Atp)) ~ 
(At(l -
p))'I- j 
j! 
L 
(n-j)! 
II=J 
= e- Xf (Atp)) ~ 
(At(l -
p))117 
., 
L 
1 
J. 
111=0 
m. 
( ' t ) j 
-At A p 
ict(l-IJ) 
= e 
---e 
j! 
= e- Afp (Atp )i . 
j! 
Thus, the number of users at time t has a Poisson distribution with parameter Atp. 
Since 
I 
A 
lim Atp = lim At-(l - e- rU ) = -, 
f -'> 'X) 
f -'>OC 
fl t 
fl 
it follows that, in the limit, the number of customers has a Poisson distribution with 
parameter A/ fl, matching our previous calculation. 
218 

8.7 EXERCISES 
8.7. Exercises 
Exercise 8.1: Let X and Y be independent. uniform random variables on [0,1]. Find 
the density function and distribution function for X --'- r. 
Exercise 8.2: Let X and Y be independent, expon~nti<.llly distributed random variables 
with parameter 1. Find the density function and distribution function for X + y. 
Exercise 8.3: Let X be a uniform random variable on [0. I], Dctcrmin~ Pre X ~ 1/2 I 
1/4 ~ X ~ 3/4) and Pr (X ~ 1/4 I (X ~ 1/3) u (X :::: 2 
_~ I ) . 
Exercise 8.4: We agree to try to meet between 12 and I for lunl.'h at our fa\orite sand-
wich shop. Because of our busy schedules, neither of u~ i~ ~urc \\ hcn \\ c'll arri\'e: we 
assume that, for each of us, our arrival time is uniformly di~tributcd lHcr thc hour. So 
that neither of us has to wait too long, we agree that we \\ill c<.ll.'h \\ ait cxal.,tl: 15 min-
utes for the other to arrive, and then leave. What is the probabilit: \\ c aL'tual h mcd 
~ach other for lunch? 
Exercise 8.5: In Lemma 8.3, we found the expectation of thc ~mal Ie,t l)f 1/ indcpcn-
dent uniform random variables over [0, 1] by directly computing thc prl1babilit: that it 
\vas larger than y for ° 
~ y ~ 1. Perform a similar calculation w rind the prl1babilit: 
that the kth smallest of the n random variables is larger than y. and u~c thi~ tl1 ,hlm 
that its expected value is k/(n + 1). 
Exercise 8.6: Let Xl, X 2., ... , XII be independent exponential randl)m \ ariable, \\ ith 
parameter 1. Find the expected value of the kth largest of the n random \ ariabk" 
Exercise 8.7: Consider a complete graph on n vertices. Each edgc i~ a~,igncd a \\cight 
.:hosen independently and uniformly at random from the real int~n~t1 [(). I I, Sh(m that 
the expected weight of the minimum spanning tree of this graph is at ka~t 1- I (1- ( ~)), 
Find a similar bound when each edge is independently assigned a \\~ight from an cx-
ponential distribution with parameter 1. 
Exercise 8.8: Consider a complete graph on n vertices. Each edg~ i:-- a~~ign~J a \\cight 
,:hosen independently and uniformly at random from the real inknal lO. II. \\'~ pro-
~)se the following greedy method for finding a small-weight Hamiltonian cyck in the 
~raph. At each step, there is a head vertex. Initially the head is \·~rt~.\. I, At each st~p. 
\\ ~ find the edge of least weight between the current head vertex and a ncw \'~rtex that 
'a~ never been the head. We add this edge to the cycle and set the hcad vcrtex to the 
~cw vertex. After n -1 steps, we have a Hamiltonian path, which wc complete to make 
.J. Hamiltonian cycle by adding the edge from the last head vertex back to vertex 1. 
\\'hat is the expected weight of the Hamiltonian cycle found by this greedy approach? 
-\bo. find the expectation when each edge is independently assigned a weight from an 
exponential distribution with parameter 1. 
219 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Exercise 8.9: You would like to write a simulation that uses exponentially distributed 
random variables. Your system has a random number generator that produces indepen-
dent, uniformly distributed numbers from the real interval (0, I). Give a procedure that 
transforms a uniform random number as given to an exponentially distributed random 
variable with parameter A. 
Exercise 8.10: Let n points be placed uniformly at random on the boundary of a circle 
of circumference l. These n points divide the circle into n arcs. Let Z/ for 1 ~ Z/ ~ n 
be the length of these arcs in some arbitrary order. 
(a) Prove that all Z/ are at most clnn/(n - 1) with probability at least 1 - l/ne-I. 
(b) Prove that, for sufficiently large n, there exists a constant c' such that at least one 
Z/ is at least c'ln n with probability at least 1/2. (Hint: Use the second moment 
method.) 
(c) Prove that all Z/ are at least 1/2n2 with probability at least 1/2. 
(d) Prove that, for sufficiently large n, there exists a constant c' such that at least one 
Zi is at most c,/n 2 with probability at least 1/2. (Hint: Use the second moment 
method.) 
(e) Explain how these results relate to the following problem: XI, X 2 , ... , Xn- I are 
values chosen independently and uniformly at random from the real interval [0, 1]. 
We let YI , Y2 , ••• , Y17 - 1 represent these values in increasing sorted order, and we 
also define Yo = ° 
and Y17 = 1. The points }j break the unit interval into n seg-
ments. What can we say about the shortest and longest of these segments? 
Exercise 8.11: Bucket sort is a simple sorting algorithm discussed in Section 5.2.2. 
(a) Explain how to implement Bucket sort so that its expected running time is O(n) 
when the n elements to be sorted are independent, uniform random numbers that 
are chosen from [0, IJ. 
(b) We now consider how to implement Bucket sort when the elements to be sorted 
are not necessarily uniform over an interval. Specifically, suppose the elements to 
be sorted are numbers of the form X + Y, where (for each element) X and Yare 
independent, uniform random numbers chosen from [0,1]. How can you modify 
the buckets so that Bucket sort still has expected running time O(n)? What if the 
elements to be sorted were numbers of the form max( X, Y) instead of X + Y? 
Exercise 8.12: Let n points be placed uniformly at random on the boundary of a circle 
of circumference l. These n points divide the circle into n arcs. Let Z/ for 1 ~ Z/ ~ n 
be the length of these arcs in some arbitrary order, and let X be the number of Zi that 
are at least lin. Find E[XJ and Var[X]. 
Exercise 8.13: A digital camera needs two batteries. You buy a pack of n batteries, 
labeled 1 to n. Initially, you install batteries 1 and 2. Whenever a battery is drained, 
you immediately replace the drained battery with the lowest numbered unused battery. 
Assume that each battery lasts for an amount of time that is exponentially distributed 
220 

8.7 EXERCISES 
with mean f1 before being drained, independent of all other batteries. Eventually, all 
the batteries but one will be drained. 
(a) Find the probability that the battery numbered i is the one that is not eventually 
drained. 
(b) Find the expected time your camera will be able to run with this pack of batteries. 
Exercise 8.14: Let Xl, X 2, .•• be exponential random variables with parameter 1. 
(a) Argue that Xl + X 2 is not an exponential random variable. 
(b) Let N be a geometric random variable with parameter p. Prove that Ll~l Xi IS 
exponentially distributed with parameter p. 
Exercise 8.15: (a) Let Xl, X 2 , ... be a sequence of independent exponential random 
\Oariables, each with mean 1. Given a positive real number k. let X be defined by 
N = minl": tx, > kl· 
1=1 
That is, N is the smallest number for \vhich the sum of the first.\" of the Xi is larger 
than k. Determine E[N]. 
(b) Let Xl, X 2 , ... be a sequence of independent uniform random \ariables on the 
interval (0, l). Given a positive real number k with 0 < k < I. let S be defined by 
N = minl": fI Xi < kl· 
1=1 
That is, N is the smallest number for which the product of the first N of the Xi is 
,maIler than k. Determine E[N]. (Hint: You may find Exercise 8.9 helpful.) 
Exercise 8.16: There are n tasks that are given to n processors. Each task has two 
phases, and the time for each phase is given by an exponentially distributed random 
\ ariable with parameter 1. The times for all phases and for all tasks are independent. 
\\Oe say that a task is half-done if it has finished one of its two phases. 
~ a) Derive an expression for the probability that there are k tasks that are half-done at 
the instant when exactly one task becomes completely done. 
I b) Derive an expression for the expected time until exactly one task becomes com-
pletely done. 
Ie) Explain how this problem is related to the birthday paradox. 
Exercise 8.17: Prove Theorem 8.11. 
Exercise 8.18: Complete the proof of Theorem 8.13 by showing formally that, if Nl (t) 
.md N2 (t) are independent, then so are Nl (t) and N2 (u) for any t, u > O. 
221 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
Exercise 8.19: You are waiting at a bus stop to catch a bus across town. There are ac-
tually n different bus lines you can take, each following a different route. Which bus 
you decide to take will depend on which bus gets to the bus stop first. As long as you 
are waiting, the time you have to wait for a bus on the ith line is exponentially distrib-
uted with mean /1i minutes. Once you get on a bus on the ith line, it will take you ti 
minutes to get across town. 
Design an algorithm for deciding - when a bus arrives - whether or not you should 
get on the bus, assuming your goal is to minimize the expected time to cross town. 
(Hint: You want to determine the set of buses that you want to take as soon as they ar-
rive. There are 211 possible sets, which is far too large for an efficient algorithm. Argue 
that you need only consider a small number of these sets.) 
Exercise 8.20: Given a discrete space, continuous time Markov process X(t), we can 
derive a discrete time Markov chain Z (t) by considering the states the process visits. 
That is, let Z (0) = X(O), let Z (1) be the state that process X (t) first moves to after 
time t = 0, let Z (2) be the next state process X (t) moves to, and so on. (If the Markov 
process X(t) makes a transition from state i to state i, which can occur when Pi.i #- 0 
in the associated transition matrix, then the Markov chain Z (t) should also make a 
transition from state i to state i.) 
(a) Suppose that, in the process X(t), the time spent in state i is exponentially distrib-
uted with parameter 8i = () (which is the same for all i). Further suppose that the 
process X (t) has a stationary distribution. Show that the Markov chain Z (t) has 
the same stationary distribution. 
(b) Give an example showing that, if the 8i are not all equal, then the stationary dis-
tributions for X(t) and Z(t) may differ. 
Exercise 8.21: The Ehrenfest model is a basic model used in physics. There are n par-
ticles moving randomly in a container. We consider the number of particles in the left 
and right halves of the container. A particle in one half of the container moves to the 
other half after an amount of time that is exponentially distributed with parameter 1, 
independently of all other particles. See Figure 8.6. 
(a) Find the stationary distribution of this process. 
(b) What state has the highest probability in the stationary distribution? Can you sug-
gest an explanation for this'? 
Exercise 8.22: We can obtain a discrete time Markov chain from the M/ M/ 1 queue-
ing process in the manner described in Exercise 8.20. The discrete time chain tracks 
the number of customers in the queue. It is useful to allow departure events to occur 
with rate A at the queue even when it is empty; this does not affect the queue behavior. 
but it gives transitions from state 0 to state 0 in the corresponding Markov chain. 
(a) Describe the possible transitions of this discrete-time chain and give their proba-
bilities. 
222 

8.7 EXERCISES 
• 
I. 
• • •• • • • I 
• •• 
• ••• I 
• 
•• • 
•• I 
•• •• • • • .1 • 
•• 
I 
• 
• • • •• I 
• 
• • I 
Figure 8.6: The Ehrenfest I11nJeJ. 
~ b) Show that the stationary distribution of this chain \\h~n /, < ! l I" th~ "am~ a" for 
the M/ M/ 1 process. 
~C) Show that, in the case A = /1, there is no valid stationar: di"tributilJn flJr th~ .\1ar-
kov chain. 
Exercise 8.23: In a tandem queue, customers arrive to an .\1 .\1 1 yU~Ue L1L"L",Jrding 
to a Poisson process of rate A with service times independ~nt and ~\.pl)n~ntidll: di,,-
tributed with parameter /11. After completing service at thi:-, hr"t qUeue. the L"lbtl)m~r" 
proceed immediately to a second queue, also being sen'ed by a "ingk "en ~r. \\ h~r~ 
,~rvice times are independent and exponentially distributed \\ith param~ter ;1. =. Find 
th~ stationary distribution of this system. (Hint: Try to general il~ the fl)fI11 l)f th~ "td-
tionary distribution for a single queue.) 
Exercise 8.24: Write a program to simulate the model of balls and bin" \\ ith fcedbdL'k. 
la) Start your simulation with 51 balls in bin I and 49 balls in bin 2. lbing p = 2. Run 
your program 100 times, having it stop each time one bin ha" 6()c( of th~ balk On 
average, how many balls are in the bins when the program stop"~ Ho\\ l)ften dne" 
bin 1 have the majority? 
I b) Perform the same experiment as in part (a) but start with 52 ball" in bin I dnd -+~ 
balls in bin 2. How much does this change your answers": 
~C) Perform the same experiment as in part (a) but start with 102 balI... in bin I dnd l)~ 
balls in bin 2. How much does this change your answers '! 
1 d) Perform the same experiment as in part (a), but now use p = 1..5. Hl)\\ mu-:h doc" 
this change your answers? 
Exercise 8.25: We consider here one approach for studying a FIFO qu~ue \\ ith a L'on-
,tant service time of duration 1 and Poisson arrivals with parametn i. < I. \\'~ replace 
th~ constant service time by k exponentially distributed service stage". ~ach of m~an 
juration 1/ k. A customer must pass through all k stages before Iea\ing th~ lju~u~. and 
lmce one customer begins going through the k stages, no other custom~r can receive 
..en·ice until that customer finishes. 
1 a) Derive Chernoff bounds for the probability that the total time taken in k exponen-
tially distributed stages, each of mean 1/ k, deviates significantly from 1. 
I b) Derive a set of equations that define the stationary distribution for this situation. 
(Hint: Try letting Jrj be the limiting probability of having j stages of service left 
223 

CONTINUOUS DISTRIBUTIONS AND THE POISSON PROCESS 
to be served the queue. Each waiting customer requires k stages; the one being 
served requires between I and k stages.) You should not try to solve these equa-
tions to give a closed form for Jrj. 
(c) Use these equations to numerically determine the average number of customers 
in the queue in equilibrium, say for A = 0.8 and for k = 10, 20, 30, 40, and 50. 
Discuss whether your results seem to be converging as k increases, and compare 
the expected number of customers to an Mj Mj I queue with arrival rate A < I and 
expected service time f1 = 1. 
Exercise 8.26: Write a simulation for a bank of n Mj Mj I FIFO queues, each with 
Poisson arrivals of rate A < I per second and each with service times exponentially dis-
tributed with mean I second. Your simulation should run for t seconds and return the 
average amount of time spent in the system per customer who completed service. You 
should present results for your simulations for n = 100 and for t = 10,000 seconds 
with A = 0.5, 0.8, 0.9. and 0.99. 
A natural way to write the simulation that we now describe is to keep a priority 
queue of events. Such a queue stores the times of all pending events, such as the next 
time a customer will arrive or the next time a customer will finish service at a queue. 
A priority queue can answer queries of the form, "What is the next event?" Priority 
queues are often implemented as heaps, for example. 
When a customer bound for queue k arrives, the arrival time for the next customer to 
queue k must then be calculated and entered in the priority queue. If queue k is empty, 
the time that the arriving customer wiIl complete service should be put in the priority 
queue. If queue k is not empty, the customer is put at the tail of the queue. If a queue 
is not empty after completing service for a customer, then the time that the next cus-
tomer (at the head of the queue) will complete service should be calculated and put in 
the priority queue. You will have to track each customer's arrival time and completion 
time. 
You may find ways to simplify this general scheme. For example, instead of con-
sidering a separate arrival process for each queue, you can combine them into a single 
arrival process based on what we know from Section 8.4.2. Explain whatever simpli-
fications you use. 
You may wish to use Exercise 8.9 to help construct exponentially distributed random 
variables for your simulation. 
Modify your simulation so that, instead of service times being exponentially dis-
tributed with mean 1 second, they are always exactly I second. Again present results 
for your simulation for n = 100 and for t = 10,000 seconds with A = 0.5, 0.8, 0.9, 
and 0.99. Do customers complete more quickly with exponentially distributed service 
times or constant service times? 
224 

CHAPTER NINE 
Entropy, Randomness, 
and Information 
Suppose that we have two biased coins. One comes up head" \\ith probability 3/4, and 
the other comes up heads with probability 7/8. \\'hich coin produces more random-
ness per flip? In this chapter, we introduce the el/lropy function as a uni\ersal measure 
of randomness. In particular, we show that the number of independent unbiased ran-
dom bits that can be extracted from a sequence of biased coin flip" corresponds to the 
entropy of the coin. Entropy also plays a fundamental role in information and commu-
nication. To demonstrate this role. we examine some ba"ic re"ulh in compre""ion and 
coding and see how they relate to entropy. The main result \\e prO\e i" Shannon'" cod-
ing theorem for the binary symmetric channel. one of the fundamental resulh of the 
field of information theory. Our proof of Shannon's theorem uses se\'eral ideas that we 
have developed in previous chapters, including Chernoff bounds. Markm''s inequality. 
and the probabilistic method. 
9.1. The Entropy Function 
The entropy of a random variable is a function of its distribution that. as we shall see. 
gives a measure of the randomness of the distribution. 
Definition 9.1: 
1. The entropy in bits of a discrete random variable X is given by 
H(X) = - L Pr(X = x) log2 Pr(X = x). 
}vhere the summation is over all values x in the range of X. Equivalently, we may 
}vrite 
H(X) = E[IOg') _1_]. 
~ Pr(X) 
225 

ENTROPY, RANDOMNESS, AND INFORMATION 
0.8 
0.6 
~ 
::r: 
0.4 
0.2 
0 
0 
0.2 
0.4 
0.6 
0.8 
p 
Figure 9.1: The binary entropy function. 
2. The binary entropy function H(p) for a random variable that assumes only two 
possible outcomes, one o.fwhich occurs with probability p, is 
H(p) = -p log2 P -
(1 -
p) log2(l - p). 
We define H(O) = H(l) = 0, so the binary entropy function is continuous in the inter-
val [0, 1]. The function is drawn in Figure 9.l. 
For our two biased coins, the entropy of the coin that comes up heads with proba-
bility 3/4 is 
(
3) 
3 
3 
1 
1 
3 
H -
= --log? - - -log) - = 2 - -log) 3 ~ 0.8113 
4 
4 
-4 
4 
-4 
4 
-
, 
while the entropy of the coin that comes up heads with probability 7/8 is 
(
7) 
7 
7 
I 
1 
7 
H -
= - - log? - -
- log) - = 3 - - log) 7 ~ 0.5436. 
8 
8 
~8 
8 
~8 
8 
~ 
Hence the coin that comes up heads with probability 3/4 has a larger entropy. 
Taking the derivative of H(p), 
dH(p) 
1 -
p 
-d- = -log;:: P + log;::(l -
p) = log;:: --, 
p 
p 
we see that H(p) is maximized when p = 1/2 and that H(l/2) = 1 bit. One way of 
interpreting this statement is to say: each time we flip a two-sided coin, we get out at 
most 1 bit worth of randomness, and we obtain exactly 1 bit of randomness when the 
coin is fair. Although this seems quite clear, it is not yet clear in what sense H(3/4) = 
2 -
~ log;:: 3 means that we obtain H(3/4) random bits each time we flip a coin that 
lands heads with probability 3/4. We clarify this later in the chapter. 
226 

9.1 THE ENTROPY FUNCTION 
As another example, the entropy of a standard six-sided die that comes up on each 
side with probability 1/6 has entropy log.:! 6. In general, a random variable that has n 
~qually likely outcomes has entropy 
11 
I 
I 
-L -log.:! -
= log.:! II. 
n 
n 
i=1 
The entropy of an eight-sided die is therefore 3 hil',. This result should seem quite 
natural; if the faces of the die were numbered from 0 to 7 written in binary, then the 
outcome of the die roll would give a sequ~nc~ of 3 hits uniform over the set {a, 1 }3, 
which is equivalent to 3 bits generated independ~ntly and uniformly at random. 
It is worth emphasizing that the entropy of a random yariahk X depends not on the 
\, .. dues that X can take but only on the probability di"trihution of X over those values. 
The entropy of an eight-sided die does not d~p~nd OIl \\l1at numb~rs are on the faces 
of the die; it only matters that all eight sides ar~ eljuall~ lik~ly to com~ up. This prop-
~rty does not hold for the expectation or \ariance of X. hut it doe" makes sense for a 
measure of randomness. To measure the randomne"" in a die. \\ ~ "l1ould not care about 
\\hat numbers are on the faces but only ahout hO\\ oft~n the die L'ome" up on ~ach side. 
Often in this chapter we consider the ~ntropy of a "~ljuence l)f independent random 
yariables, such as the entropy of a seljuence of independent coin Hip", For "uch situa-
tions, the following lemma allows us to consid~r the ~ntropy of ~ach random \ariahk 
to find the entropy of the sequence. 
Lemma 9.1: Let XI and X.:! be illdependel/l random rariahles, and let }" = I XI. X ~ I. 
Then 
H(Y) = H( XI) + H( X.:!). 
Of course, the lemma is trivially extended by induction to the case wher~ Y is any tinit~ 
"~quence of independent random variables. 
Proof· In what follows, the summations are to be taken over all possibk \'alu~s that 
Lan be assumed by XI and X.:!. The result follows by using the independence of XI and 
X.:! to simplify the expression: 
.\1, .\2 
= - L L Pr ( X.:! = .r 2) Pr ( X I = X I ) log 2 Pr ( X I = X I ) 
Xl 
X2 
- LLPr(XI =xl)Pr(X.:! =x.:!)log.:!Pr(X2 = x.:!) = 
227 

ENTROPY, RANDOMNESS, AND INFORMATION 
= - (I: Pr( X, = X,) log, Pr(X, = X,)) (I: Pr(X, = X2)) 
rl 
x::> 
-(I: Pr( X, = X,) log, Pr(X, = X,)) (I: Pr(X, = X,)) 
r2 
Xl 
= H(X) + H(Y). 
• 
9.2. Entropy and Binomial Coefficients 
As a prelude to showing the various applications of entropy, we first demonstrate how 
it naturally arises in a purely combinatorial context. 
Lemma 9.2: Suppose that nq is an integer in the range [0, n]. Then 
__ .::::; 
n 
.::::; 2IJH (q). 
2
I1H
(q) 
( 
) 
n + I 
nq 
Proof: The statement is trivial if q = 0 or q = I, so assume that 0 < q < I. To prove 
the upper bound, notice that by the binomial theorem we have 
Cnq )qq"(l - q )o-q}" '" t G 
)qk(l - q ),,-k '" (q + (I - q))" = l. 
k=O 
Hence, 
(nn
q
) '" q-q"(l- q)-O-q}n = 2-W"oglq2-(l-q},,'og,(1-q} = 2nH(q) 
For the lower bound, we know that (,;~)q qllo - q )(I-c/)11 is one term of the expression 
L~=()(Z)qk(l - q)ll-k. We show that it is the largest term. Consider the difference 
between two consecutive terms as follows: 
G)qk(l - q)n-k - C : I )qk+ '(I - q),,-k-' 
= (n) qkO_q)ll-k(l_ ~-q 
). 
k 
k+ll-q 
This difference is nonnegative whenever 
n - k 
q 
1----->0 
k+ll-q-
or (equivalently, after some algebra) whenever 
k ~ qn - I + q. 
The terms are therefore increasing up to k = q n and decreasing after that point. Thus 
k = qn gives the largest term in the summation. 
228 

9.2 ENTROPY AND BINOMIAL COEFFICIENTS 
Since the summation has n + I terms and since (';:JqCjll(l -
q)(l-q)1l is the largest 
term, we have 
or 
• 
We often use the following slightly more specific corollary. 
Corollary 9.3: When 0 :'S q :'S 1/2, 
(9.1) 
similarly, when 1/2 ::s q ::s I, 
(9.2) 
For 1/2 :'S q ::s I, 
(9.3) 
similarL.v, }vhen 0 :'S q :'S 1/2. 
Proof: We first prove Eqn. (9.1); the proof of (9.2) is entirely similar. When 0 ::s q ::s 
1/2, 
from which we can proceed exactly as in Lemma 9.2. 
Equation (9.3) holds because, when q ~ 1/2, Lemma 9.2 gives 
( 
) 
2I1H(lIlCfJ/n) 
2 nH (q) 
L n~ J ~ -n-+-I- ~ -n-+-I ; 
Equation (9.4) is derived similarly. 
• 
Although these bounds are loose, they are sufficient for our purposes. The relation be-
tween the combinatorial coefficients and the entropy function arises repeatedly in the 
proofs of this chapter when we consider a sequence of biased coin tosses, where the 
coin lands heads with probability p > 1/2. Applying the Chernoff bound, we know 
that, for sufficiently large n, the number of heads will almost always be close to np. 
Thus the sequence will almost always be one of roughly (,;;)) ~ 2
11 H(jJ) sequences, 
where the approximation follows from Lemma 9.2. Moreover, each such sequence oc-
curs with probability roughly 
229 

ENTROPY, RANDOMNESS, AND INFORMATION 
Hence, when we consider the outcome of n flips with a biased coin, we can essen-
tially restrict ourselves to the roughly 211H( p) outcomes that occur with roughly equal 
probabil i ty. 
9.3. Entropy: A Measure of Randomness 
One way of interpreting the entropy of a random variable is as a measure of how many 
unbiased, independent bits can be extracted, on average, from one instantiation of the 
random variable. We consider this question in the context of a biased coin, showing 
that, for sufficiently large n, the expected number of bits that can be extracted from n 
flips of a coin that comes up heads with probability p > 1/2 is essentially nH(p). In 
other words, on average, one can generate approximately H (p) independent bits from 
each flip of a coin with entropy H(p). This result can be generalized to other random 
variables, but we focus on the specific case of biased coins here (and throughout this 
chapter) to keep the arguments more transparent. 
We begin with a definition that clarifies what we mean by extracting random bits. 
Definition 9.2: Let Iy I be the number of bits in a sequence of bits y. An extraction 
function Ext takes as input the value of a random variable X and outputs a sequence 
of bits y such that 
Pr(Ext(X) = y I Iyl = k) = I/2k 
whenever Pr(I)'1 = k) > O. 
In the case of a biased coin, the input X is the outcome of n flips of our biased coin. 
The number of bits in the output is not fixed but instead can depend on the input. If 
the extraction function outputs k bits, we can think of these bits as having been gener-
ated independently and uniformly at random, since each sequence of k bits is equally 
likely to appear. Also, there is nothing in the definition that requires that the extraction 
function be efficient to compute. We do not concern ourselves with efficiency here, 
although we do consider an efficient extraction algorithm in Exercise 9.12. 
As a first step toward proving our results about extracting unbiased bits from biased 
coins, we consider the problem of extracting random bits from a uniformly distributed 
integer random variable. For example, let X be an integer chosen uniformly at random 
from {O, ... , 7}, and let Y be the sequence of 3 bits obtained when we write X as a bi-
nary number. If X = 0 then Y = 000, and if X = 7 then Y = Ill. It is easy to check 
that every sequence of 3 bits is equally likely to arise, so we have a trivial extraction 
function Ext by associating any input X with the corresponding output Y. 
Things are slightly harder when X is uniform over {O, ... , II}. If X .:s 7, then we can 
again let Y be the sequence of 3 bits obtained when we write X in binary. This leaves 
the case X E {8, 9,10, II}. We can associate each of these four possibilities with a dis-
tinct sequence of 2 bits, for example, by letting Y be the sequence of 2 bits obtained 
230 

9.3 ENTROPY: A MEASURE OF RANDOMNESS 
I 
1 
Input 
0 
1 
2 
:3 
-! 
.J 
G 
i 
I 
I 
i 
Output 000 
001 
010 
011 I 100 
! 101 i 110 
111 ! 
Input 
0 
1 
2 
3 
4 
I 
Ci 
-
'" 
9 ' 10 111 I 
;) 
i 
I 
, 
Output 000 om 
010 
011 
100 
101 1110 
111 
1111 
01 
! WI 11 i 
Figure 9.2: Extraction functions for numbers that are chosen unifurml: ,11 r~ilhj(lll1 frlll11 (0 ..... 7) 
and {D, ... , 11). 
from writing X - 8 as a binary number. Thus, if X = 8 th~n }" = 00. ami if X = 
II then Y = II. The entire extraction function is shown in Figur~ Y.~. E\~ry .3-bit s~­
quence arises with the same probability 1/12, and every 2-bit "~lju~nL~ ari"~,, \\ith th~ 
same probability 1/12, so Definition 9.2 is satisfied. 
We generalize from these examples to the following th~or~Il1. 
Theorem 9.4: Suppose that the value of a random variahle X i\ lh(l\t'll l/llirimllh at 
mndomfrom the integers {a, ... ,111- I}, so that H(X) = log.:: 171. Thel/ there i\ (/1/ ex-
tmctionfunctionfor X that outputs on average at least LIog.:: Ill_ -
I = _HI.\ 1_ -
I 
iI/dependent and unbiased bits. 
Proof' If 111 > I is a power of 2, then the extraction function can simpl~ output th~ bit 
r~presentation of the input X using log2 m bits. (If III = 1. th~n it output" nothing or. 
~quivalently, an empty sequence.) All output sequences hav~ log.:: III hit". ~lI1d all ,,~­
ljuences of IOg2 171 bits are equally likely to appear, so this satisti~" Dd1nitiun Y.2. If III 
is not a power of 2, then matters become more complicated. \\-'~ (k"crih~ th~ ~:\traction 
function recursively. (A nonrecursive description is given in E:\~rci,,~ Y.X.I L~t U' = 
JOg2 mj. If X .:'S 20' - I, then the function outputs the a-bit binar~ r~pr~"entation of 
X: all sequences of a bits are equally likely to be output in thi" ca,,~, If X ::::: 2", th~n 
X - 20' is uniformly distributed in the set {O, ... , m - 20' - I}. which i" "malkr than th~ 
"et {a, ... ,m}. The extraction function can then recursively produce the output from 
the extraction function for the variable X - 20'. 
The recursive extraction function maintains the property that. for ~\~ry k. ~ach of th~ 
2{ sequences of k bits is output with the same probability. We claim b~ induction that 
the expected number of unbiased, independent bits produced by thi" ~:\traction func-
tion is at least Llog 2 m J - 1. The cases where m is a power of 2 ar~ tri\ial. Otherwise, 
by induction, the number of bits Y in the output satisfies 
20' 
m - 20' 
E[YJ ~ -a + 
(LIog2(m - 20')J - I) 
m 
m 
m -20' 
= a + 
(Llog2(m - 2(Y)J - a -
I). 
m 
231 

ENTROPY, RANDOMNESS, AND INFORMATION 
Suppose Llog2(m - 2(Y)J = f3, where 0 ~ f3 :s 0: -
1. Then (m - 2(Y)/m is minimized 
when m = 20' + 2fl. Hence 
completing the induction. 
2fl 
E[Y] > 0: + 
(f3 -
0: -
1) 
-
2(Y +2/J 
1 
~ 0: -
--(0: -
f3 + 1) 
2(Y-/J 
~ 0: -1, 
We use Theorem 9.4 in our proof of the main result of this section. 
• 
Theorem 9.5: Consider a coin that comes up heads with probability p > 1/2. For 
any constant 8 > 0 andfor n sufficientf.v large: 
1. there exists an extraction function Ext that outputs, on an input sequence of n inde-
pendent flips, an average (~fat least (1- 8)nH(p) independent random bits; and 
2. the average number of bits output by any extraction function Ext on an input se-
quence of n independent flips is at most n H (p). 
Proof: We begin by describing an extraction function that generates, on average, at 
least (1 - 8)nH(p) random bits from n flips of the biased coin. We saw before that, 
in the case of a biased coin, the outcome of n flips is most likely be one of roughly 
2 1l H(p) sequences, each occurring with probability roughly 2- IlH(p). If we actually had 
a uniform distribution of this type, we could use the extraction function that we have 
just described for numbers chosen uniformly at random to obtain on average almost 
nH(p) uniform random bits. In what follows, we handle the technical details that arise 
because the distribution is not exactly uniform. 
There are C) possible sequences with exactly j heads, and each of them has the 
same probability of occurring, p j (1 -
p) 1/ - j. For each value of j, 0 :s j :s n, we map 
each of the cn sequences with j heads to a unique integer in the set {O, ... , C) - I}. 
When j heads come up, we map the sequence to the corresponding number. Condi-
tioned on there being j heads, this number is uniform on the integers {O, ... , C) - I}, 
and hence we can apply the extraction function of Theorem 9.4 designed for this case. 
Let Z be a random variable representing the number of heads flipped, and let B be 
the random variable representing the number of bits our extraction function produces. 
Then 
n 
E[B] = L Pr(Z = k)E[B I Z = k] 
k=O 
and, by Theorem 9.4, 
Er8 I Z = k] ::> llog2G) J - I. 
Let £ < P - 1/2 represent a constant to be determined. We compute a lower bound 
for E[B] by considering only values of k with n(p -
£) ~ k ~ n(p + c). For every 
such k, 
232 

9.3 ENTROPY: A MEASURE OF RANDOMNESS 
(n) > ( 
11 
) > 2
11H
( 1'+1') 
k 
-
Ln(p + E)J 
-
11 + 1 ' 
\\here the last inequality follows from Corollary 9.3. Hence 
III ( p+l'll 
E[B] ~ L 
Pr(Z = k)E[B I Z = k] 
k=Ln(p-E.)J 
III(p+Ell 
:> L 
Pr(Z = k)(lIO&,(n J - I) 
k= LII( p-E)J 
Pr( Z = k) 
~ (n H (p + E) - log:: (11 + I ) -
I) Pr ( 
I Z -
11 pi :s En). 
:'-iow E[Z] = np, and Pr(1 Z - np I > Ell) can be bounded by using the Chernoff bound 
of Eqn. (4.6), giving 
Pr(IZ - IIpl > Ell) :s 2e-I:--~;'" 
Hence 
We conclude that, for any constant 8 > O. we can ha\"e 
E[B] ~ (1 - 8)IIH(p) 
by choosing E sufficiently small and n sufficiently large. For example. for sufficiently 
small E, 
nH(p + E) ~ (1- 8/3)nH(p), 
and when 11 > (3p/E2) In(6/8) we have 
1 - 2e-1l1'2j3p ~ 1 - 8/3. 
Hence, with these choices, 
E[B] ~ ((1 - 8/3)nH(p) - log2(n + 1) -
1)(1 - 8/3). 
As long as we now also choose n sufficiently large that (8/3)nH( p) is greater than 
log2(n + 1) + 1, we have 
E[B] ~ ((1- 28/3)nH(p)(l- 8/3) ~ (1- 8)nH(p). 
proving there exists an extraction function that can extract (1 -
Ii) 11 H (p) independent 
and uniform bits on average from n flips of the biased coin. 
We now show that no extraction function can obtain more than 11 H (p) bits on av-
erage. The proof relies on the following basic fact: If an input sequence x occurs 
with probability q, then the corresponding output sequence Ext(x) can have at most 
IExt(x)1 :s log2(l/q) bits. This is because all sequences with IExt(x)1 bits would have 
probability at least q, so 
233 

ENTROPY, RANDOMNESS, AND INFORMATION 
giving the desired boundon Ext(x). Given any extraction function, if B is a random vari-
able representing the number of bi ts our extraction function produces on input X, then 
E[B] = L Pr(X = x)IExt(x)1 
1 
< L Pr(X = x) log') ----
-
X 
~ Pr(X = x) 
= E[IOg2 _1_] 
Pr(X) 
= H(X). 
• 
Another natural question to ask is how we can generate biased bits from an unbiased 
coin. This question is partially answered in Exercise 9.11. 
9.4. Compression 
A second way of interpreting the entropy value comes from compression. Again sup-
pose we have a coin that comes up heads with probability p > 1/2 and that we flip 
it n times, keeping track of which flips are heads and which flips are tails. We could 
represent every outcome by using one bit per flip, with 0 representing heads and 1 rep-
resenting tails, and use a total of n bits. If we take advantage of the fact that the coin is 
biased, we can do better on average. For example. suppose that p = 3/4. For a pair of 
consecutive flips, we use 0 to represent that both flips were heads, 10 to represent that 
the first flip was heads and the second tails, 110 to represent that the first flip was tails 
and the second heads, and III to represent that both flips were tails. Then the average 
number of bits we use per pair of flips is 
9 
3 
3 
I 
27 
1 . - + 2 . - + 3 . - + 3 . -
= -
< 2. 
16 
16 
16 
16 
16 
Hence, on average, we can use less than the 1 bit per flip of the standard scheme by 
breaking a sequence of n flips into pairs and representing each pair in the manner shown. 
This is an example of compression. 
It is worth emphasizing that the representation that we used here has a special prop-
erty: if we write the representation of a sequence of flips, it can be uniquely decoded 
simply by parsing it from left to right. For example, the sequence 
011110 
corresponds to two heads, followed by two tails, followed by a heads and a tails. There 
is no ambiguity, because no other sequence of flips could produce this output. Our rep-
resentation has this property because no bit sequence we use to represent a pair of flips 
is the prefix of another bit sequence used in the representation. Representations with 
this property are called prefix codes, which are discussed further in Exercise 9.15. 
234 

9.4 COMPRESSION 
Compression continues to be a subject of considerable study. When storing or trans-
mitting information, saving bits usually corresponds to saving resources, so finding 
ways to reduce the number of used bits by taking advantage of the data's structure is 
often worthwhile. 
We consider here the special case of compressing the outcome of a sequence of 
biased coin flips. For a biased coin with entropy H( p), we show (a) that the outcome 
of n flips of the coin can be represented by approximately n H (p) bits on average and 
( b) that approximately n H (p) bits on average are necessary. In particular, any repre-
sentation of the outcome of n flips of a fair coin essentially requires n bits. The entropy 
is therefore a measure of the average number of bits generated by each coin flip after 
compression. This argument can be generalized to any discrete random variable X, so 
that n independent, identically distributed random \ariables XI. X 2 ••.• ,XII with the 
same distribution X can be represented using approximately n H ( X) bits on average. 
In the setting of compression, entropy can be \"je\\ed as measuring the amount of in-
formation in the input sequence. The larger the entropy of the sequence. the more bits 
are needed in order to represent it. 
We begin with a definition that clarifies \\"hat \\e mean by compression in this context. 
Definition 9.3: A compression function Cum rakes ([S inplIr ([ s(,(/llence orn coin flips, 
f.{iven as an elernent (~f' {H. T} II. ([nd ollrplIrs ([ se(/llence (lr hirs \[/c/z rl/([r e([c/z input 
sequence ~f n flips yields ([ distinCf olltplIr s(,(/llence. 
Definition 9.3 is rather weak. but it \\ill pro\"e sufficient for our purposes. L sually. com-
pression functions must satisfy stronger requirements: for example. \\e may require a 
prefix code to simplify decoding. Using this weaker definition makes our lower-bound 
proof stronger. Also, though we are not concerned here with the efficiency of com-
pressing and decompressing procedures. there are very efficient compression schemes 
that perform nearly optimally in many situations. We will consider an efficient com-
pression scheme in Exercise 9.17. 
The following theorem formalizes the relationship between the entropy of a biased 
coin and compression. 
Theorem 9.6: Consider a coin that comes up heads vvith probability p > 1/2. For 
([ny constant 8 > 0, when n is sufficient!.\' large: 
1. there exists a compression fllnction Com such that the expected nllmher of' hits 
output by Com on an inpllt sequence (~f n independent coin flips is at most 
(1 + 8)nH(p): and 
2. the expected nllmber (~l bits outpllt by any compression function on ([n inpllt se-
quence qfn independent coin flips is at least (1 - 8)nH(p). 
Theorem 9.6 is quite similar to Theorem 9.5. The lower bound on the expected num-
ber of bits output by any compression function is slightly weaker. In fact, we could 
raise this lower bound to n H (p) if we insisted that the code be a prefix code - so that 
no output is the prefix of any other - but we do not prove this here. The compression 
235 

ENTROPY, RANDOMNESS, AND INFORMATION 
function we design to prove an upper bound on the expected number of output bits does 
yield a prefix code. Our construction of this compression function follows roughly the 
same intuition as Theorem 9.5. We know that, with high probability, the outcome from 
the n flips will be one of roughly 211H( p) sequences with roughly np heads. We can use 
about n H (p) bits to represent each one of these sequences, yielding the existence of 
an appropriate compression function. 
Proof of Theorem 9.6: We first show that there exists a compression function as guar-
anteed by the theorem. Let £ > 0 be a suitably small constant with p -
£ > 1/2. Let 
X be the number of heads in n flips of the coin. The first bit output by the compression 
function we use as a flag. We set it to 0 if there are at least n (p -
£) heads in the se-
quence and to 1 otherwise. When the first bit is a 1, the compression function uses the 
expensive default scheme, using 1 bit for each of the n flips. This requires that n + 1 
total bits be output; however, by the Chernoff bound (4.5), the probability that this case 
happens is bounded by 
Now let us consider the case where there are at least n (p -
£) heads. The number 
of coin-flip sequences of this form is 
II 
() 
II 
( 
) 
L 
n 
L 
n 
n 
nH(p-E:) 
. 
< 
< -2 
. 
} 
-
in(p-£)l 
-
2 
j= 111 ( p-fll 
j= III ( p-Ell 
The first inequality arises because the binomial terms are decreasing as long as j > n / 2, 
and the second is a consequence of Corollary 9.3. For each such sequence of coin flips, 
the compression function can assign a unique sequence of exactly LnH( p -£) + log2 nJ 
bits to represent it, since 
2LIIH(p-f)+log2I1J > ~2nH(p-E). 
- 2 
Including the flag bit, it therefore takes at most nH( p - £) + log2 n + 1 bits to represent 
the sequences of coin flips with this many heads. 
Totaling these results, we find that the expected number of bits required by the com-
pression function is at most 
e-IIE2j2p(n + I) + (1 - e-IIE2/2p)(nH(p - £) + log2 n + 2) :s (1 + 8)nH(p), 
where the inequality holds by first taking £ sufficiently small and then taking n suffi-
ciently large in a manner similar to that of Theorem 9.5. 
We now show the lower bound. To begin, recall that the probability that a specific 
sequence with k heads is flipped is pk(l- p)lI-k. Because p > 1/2, if sequence 51 has 
more heads than another sequence 5'2, then 51 is more likely to appear than 52. Also, 
we have the following lemma. 
Lemma 9.7: Ifsequence 51 is more likely than 52, then the compression function that 
minimizes the expected number of bits in the output assigns a bit sequence to 52 that is 
at least as long as 51. 
236 

9.5* CODING: SHANNON'S THEOREM 
Proof' Suppose that a compression function assigns a bit sequence to 52 that is shorter 
than the bit sequence it assigns to 51. We can improve the expected number of bits out-
put by the compression function by switching the output sequences associated with 51 
and 52, and therefore this compression function is not optimal. 
0 
Hence sequences with more heads should get shorter strings from an optimal compres-
sion function. 
We also make use of the following simple fact. If the compression function assigns 
distinct sequences of bits to represent each of s coin-flip sequences, then one of the 
output bit sequences for the s input sequences must ha\'e length at least log2 s - 1 bits. 
This is because there are at most 1 + 2 + 4 + .. , --i- 2h = 2h~1 - 1 distinct bit sequences 
with up to b bits, so if each of s sequences of coin fl ips is assigned a bit sequence of at 
most b bits, then we must have 2h+1 > s and hence h > log:: s -
1. 
Fix a suitably small £ > 0 and count the number of input sequences that have 
L(p + £)nJ heads. There are (l(p~r)lIj) sequences \\ith _( P --i- f)IIJ heads and, by 
Corollary 9.3, 
( 
) 
')IIHI r)--
L(P~£)I1J ~ -II~I 
Hence any compression function must output at least log::(2 ,:fI ;;--
(11 -+- I)) - 1 = 
11 H( P + £) - log2 (n + I) -
I bits on at least one of the .... equence .... of coin flips with 
L (p + £ )nJ heads. The compression function that minimizes the expected output length 
must therefore use at least this many bits to represent any .... equence \\ith fe\\er heads. 
by Lemma 9.7. 
By the Chernoff bound (4.2), the number of heads X satisfies 
Pr(X ~ Ln(p + £)J) ::'S Pr(X ~ n(p + £ -
1/11)) ::'S e-I/l'-I I:I:~:' ::'S e-' - 1-
as long as n is sufficiently large (specifically, n > 2/£). We thus obtain. \\ith proba-
bility at least 1 - e-n I'2/ 12/), an input sequence with fewer than LII ( P ~ E ) _ head ..... and 
by our previous reasoning the compression function that minimizes the expected out-
put length must still output at least nH(p + £) - log2(n + I) - I bits in thi" ca .... e, The 
expected number of output bits is therefore at least 
(1 - e- nI'2/ 12 p )(nH(p + £) -
log2(n + I) -
I). 
This can be made to be at least (1 - (j)nH(p) by first taking £ to be sufficiently small 
and then taking n to be sufficiently large. 
• 
9.5. * Coding: Shannon's Theorem 
\Ve have seen how compression can reduce the expected number of bits required to 
represent data by changing the representation of the data. Coding also changes the 
representation of the data. Instead of reducing the number of bits required to repre-
sent the data, however, coding adds redundancy in order to protect the data against loss 
l)r errors. 
237 

ENTROPY, RANDOMNESS, AND INFORMATION 
In coding theory, we model the information being passed from a sender to a re-
ceiver through a channel. The channel may introduce noise, distorting the value of 
some of the bits during the transmission. The channel can be a wired connection, a 
wireless connection, or a storage network. For example, if I store data on a recordable 
medium and later try to read it back, then I am both the sender and the receiver, and 
the storage medium acts as the channel. In this section, we focus on one specific type 
of channel. 
Definition 9.4: The input to a binary symmetric channel with parameter p is a se-
quence of bits x I, X2, ... , and the output is a sequence of bits Y I, Y2, ... , such that 
Pr(xi = Yi) = 1 -
p independently for each i. Informal!.v, each bit sent is flipped to 
the wrong value independently with probability p. 
To get useful information out of the channel, we may introduce redundancy to help 
protect against the introduction of errors. As an extreme example, suppose the sender 
wants to send the receiver a single bit of information over a binary symmetric chan-
nel. To protect against the possibility of error, the sender and receiver agree to repeat 
the bit n times. If p < 1/2, a natural decoding scheme for the receiver is to look at 
the n bits received and decide that the value that was received more frequently is the 
bit value the sender intended. The larger n is, the more likely the receiver determines 
the correct bit; by repeating the bit enough times, the probability of error can be made 
arbitrarily small. This example is considered more extensively in Exercise 9.18. 
Coding theory studies the trade-off between the amount of redundancy required and 
the probability of a decoding error over various types of channels. For the binary sym-
metric channel, simply repeating bits may not be the best use of redundancy. Instead 
we consider more general encoding functions. 
Definition 9.5: A (k,n) encoding function Enc: {0,l}A: ---+ {0,1}1I takes as input 
a sequence q( k bits and outputs a sequence of n hits. A (k, n) decoding function 
Dec: {O. l}1I ---+ {O. 1}A: takes as input a sequence of n bits and outputs a sequence of 
k bits. 
With coding, the sender takes a k-bit message and encodes it into a block of n ~ k bits 
via the encoding function. These bits are then sent over the channel. The receiver ex-
amines the n bits received and attempts to determine the original k-bit message using 
the decoding function. 
Given a binary channel with parameter p and a target encoding length of n, we wish 
to determine the largest value of k so that there exist (k, n) encoding and decoding 
functions with the property that, for any input sequence of k bits, with suitably large 
probability the receiver decodes the correct input from the corresponding n-bit encod-
ing sequence after it has been distorted by the channel. 
Let m E {O, I} A: be the message to be sent and Enc (m) the sequence of bi ts sent over 
the channel. Let the random variable X denote the sequence of received bits. We re-
quire that Dec( X) = m with probability at least 1 -
y for all possible messages m 
and a pre-chosen constant y. If there were no noise, then we could send the original k 
bits over the channel. The noise reduces the information that the receiver can extract 
238 

9.5* CODING: SHANNON'S THEOREM 
from each bit sent, and so the sender can reliably send messages of only about k = 
nIl - H( p)) bits within each block of n bits. This result is known as Shannon's theo-
rem. which we prove in the following form. 
Theorem 9.8: For a binwJ symmetric c/zannel 1\'iT/z parameter p < 1/2 and for any 
("(Instants 8, y > 0, when n is sufficiently large: 
I. lor any k:::: n(l- H(p) - 8), there exisT (A:.n) encoding and decoding functions 
such that the probability the receirerfails Tu ()/Jrain The correct message is at most 
y for every possible k-bit input message: and 
., there are no (k, n) encoding and decoding till/cfiuns I\'ith k ~ n (1 -
H( p) + 8) 
such that the probability of decoding correcTly is ([{ leasT y if'" ({ k-bit input message 
chosen un~formly at random. 
Proof' We first prove the existence of suitable I k. n ) encoding and decoding functions 
\\hen k :::: n(l- H(p) - 8) by using the probahili .... tk method. In the end. we want our 
encoding and decoding functions to ha\e error probability at n1O .... t / on C\'e/~\, possi-
hie input. We begin with a weaker result. "hm\ ing that there exi .... t appropriate coding 
functions when the input is chosen uniformly at randum frum all k-hit inputs. 
The encoding function assigns to each of the .2' .... tring .... an II -hit codell'ord chosen 
mdependently and uniformly at random from the .... pal.·e of all II-hit .... e4uenl.·e". Label 
these codewords Xo. XI ..... X::; -I' The encoding function .... imrl~ outputs the code-
\\ord assigned to the k-bit message using a large lookup tahle l.·unLlining an entr~ for 
each k-bit string. (You may be concerned that t\\o l.'ode\\ord .... ma~ turn uut tl) he the 
.... ame; the probability of this is very small and is handled in the anal: .... i .... that f(lll(l\\ ..... ) 
To describe the decoding function. we prm'ide a decoding algllrithm h~bed (1l1 the 
Illokup table for the encoding function, which we may assume the rel.'ei\ cr pll ........ e ........ e ..... 
The decoding algorithm makes use of the fact that the recei\er expects the dlannel tll 
make roughly pn errors. The receiver therefore looks for a code\\ord that differ .... frum 
the 11 bits received in between (p -
£)n and (p + £)n places for some .... uitabl: .... mall 
.:onstant £ > O. If just one codeword has this property. then the recei\er will a ........ ume 
that this was the codeword sent and will recover the message accordingl:. If mure than 
\me codeword has this property, the decoding algorithm fails. The decoding algurithm 
described here requires exponential time and space. As in the rest of thi .... chapter. \\e 
,ire not now concerned with efficiency issues. 
The corresponding (k, n) decoding function can be obtained from the algorithm b~ 
.... imply running through all possible n-bit sequences. Whene\'er a se4uence decodes 
properly with the foregoing algorithm, the output of the decoding function for that se-
yuence is set to the k-bit sequence associated with the corresponding code\\ord. When-
e\"er the algorithm fails, the output for the sequence can be any arbitrary sequence of 
I..: bits. For the decoding function to fail, at least one of the two follmving events must 
\)ccur: 
• the channel does not make between (p -
£) nand (p + £) 11 errors: or 
• when a codeword is sent, the received sequence differs from some other codeword 
in between (p -
£)n and (p + £)17 places. 
239 

ENTROPY, RANDOMNESS, AND INFORMATION 
The path of the proof is now clear. A Chernoff bound can be used to show that, with 
high probability, the channel does not make too few or too many errors. Conditioning 
on the number of errors being neither too few nor too many, the question becomes how 
large k can be while ensuring that, with the required probability, the received sequence 
does not differ from mUltiple codewords in between (p - £)n and (p + £)n places. 
Now that we have described the encoding and decoding functions, we establish the 
notation to be used in the analysis. Let R be the received sequence of bits. For se-
quences SI and 52 of n bits, we write ~CS'I, 52) for the number of positions where these 
sequences differ. This value ~(SI, S2) is referred to as the Hamming distance between 
the two strings. We say that the pair (Sl, S2) has weight 
w (SI, S2) = p~(.'I.S2)(1 -
P )n-~('\I.S2). 
The weight corresponds to the probability that S2 is received when SI is sent over the 
channel. We introduce random variables 50,51, ••• , 52k-1 and Wo, WI, ... , W2k-1 de-
fined as follows. The set 5i is the set of all received sequences that decode to Xi' The 
value Wi is given by 
Wi = L W(Xi , r). 
rtj:Sj 
The 5i and Wi are random variables that depend only on the random choices of 
Xo, XI, ... , X 2,,-1. The variable Wi represents the probability that, when Xi is sent, 
the received sequence R does not lie in 5i and hence is decoded incorrectly. It is also 
helpful to express Wi in the following way: letting Ii,s be an indicator random variable 
that is 1 if S tf- 5i and 0 otherwise, we can write 
Wi = Lhrw(Xi,r). 
We start by bounding E[Wi ]. By symmetry, E[Wi ] is the same for all i, so we bound 
E[Wo]. Now 
E[Wol = E[ ~ 
Io.,W(Xo,rl] 
= LE[w(Xo,r)/o.r]' 
We split the sum into two parts. Let TI = {s : 1~(Xo,s) -
pnl > en} and T2 = {s : 
I ~(Xo, s) - pn I ::'S £n}, where £ > 0 is some constant to be determined. Then 
LE[w(Xo,r)Io.r ] = L 
E[w(Xo,r)/o.r] + L 
E[w(Xo,r)/O,r]' 
and we bound each term. 
We first bound 
L 
E[w(Xo, r)/o.r] ::'S L 
w(Xo, r) 
rE TI 
L 
p~(Xu.r)(l -
p)Il-~(Xo.r) 
r:IMXo.r)-pnl>EIl 
= Pr ( I ~ 
( X 0, R) - n pi> £ n ) . 
240 

9.5* CODING: SHANNON'S THEOREM 
That is, to bound the first term, we simply bound the probability that the receiver fails 
to decode correctly and the number of errors is not in the range [( p -
£ ) n, (p + £ ) n] by 
the probability that the number of errors is not in this range. Equivalently, we obtain 
our bound by assuming that, whenever there are too many or too few errors introduced 
by the channel, we fail to decode correctly. This probability is very small, as we can 
see by using the Chernoff bound (4.6): 
For any £ > 0, we can choose n sufficiently large so that this probability, and hence 
LrET\ E[w(Xo, r)/O,r], is less than y/2. 
We now find an upper bound for L"ET~ E[u'( XII. r)/II,]. For every r E T2 , the de-
coding algorithm will be successful when r is recci\'ed unless r differs from some other 
codeword Xi in between (p -
£)11 and (p + E)II places. Hence lo.r will be 1 only if 
such an Xi exists, and thus for any values of Xo and r E T-:, we have 
E[w(Xo, r)/o.r] 
= w(Xo,r) Pr(for some Xi with 1 ~ i ~ 2' -l. ~(XIJ) -
pili ~ En). 
It follows that if we obtain an upper bound 
Pr(forsome Xi with 1 ~ i ~ 2" - L 1~(Xl.r) -
pll ~ Ell) ~;,; I 
for any values of Xo and rET:" then 
To obtain this upper bound, we recall that the other codewords XI. X -:, ..... X -:,' 
are 
chosen independently and uniformly at random. The probability that any other . ..,pecitlc 
codeword Xi, i > 0, differs from any given string r of length II in between (p -
E)II 
and (p + £) n places is therefore at most 
ll1(p+E)j 
(11) 
(11) 
L 
} 
ll1(P+f)j 
-<n 
. 
211 -
2" 
j=rl1( p-sn 
Here we have bounded the summation by n times its largest term: C) is largest when 
j = Ln(p + £)J over the range of j in the summation, as long as E is chosen so that 
p + £ < 1/2. 
Using Corollary 9.3, 
(ll1(/~l+f)j) 
2H (P+S)11 
------- < ------
211 
211 
= 2- 11 (I-H(p+c)). 
Hence the probability that any specific Xi matches a string r on a number of bits 
so as to cause a decoding failure is at most n2- 11 (I-H(p+f)). By a union bound, the 
241 

ENTROPY, RANDOMNESS, AND INFORMATION 
probability that any of the 2k - 1 other codewords cause a decoding failure when Xo is 
sent is at most 
where we have used the fact that k ::: nO - H(p) - 8). By choosing E small enough 
so that H(p + E) -
H(p) - 8 is negative and then choosing n sufficiently large, we 
can make this term as small as desired, and in particular we can make it less than y /2. 
By summing the bounds over the two sets TI and T2 , which correspond to the two 
types of error in the decoding algorithm, we find that E [Wo] ::: y. 
We can bootstrap this result to show that there exists a specific code such that, if 
the k-bit message to be sent is chosen uniformly at random, then the code fails with 
probability y. We use the linearity of expectations and the probabilistic method. We 
have that 
2'_1 
2'_1 
L E[J,I)l = E[ L Wj] '" 2'y, 
j=() 
j=() 
where again the expectation is over the random choices of the codewords Xo, XI, ... , 
X 2,,- \. By the probabilistic method, there must exist a specific set of codewords 
Xo, XI, ... , X2' _I such that 
2'-1 
L W; ::: 2ky. 
j=O 
When a k-bit message to be sent is chosen uniformly at random, the probability of 
error is 
')' -\ 
1 -
- '" W· < Y 
2k L 
J-
j=O 
for this set of codewords, proving the claim. 
We now prove the stronger statement in the theorem: we can choose the codewords 
so that the probability of failure for each individual codeword is bounded above by y. 
Notice that this is not implied by the previous analysis, which simply shows that the 
average probability of failure over the codewords is bounded above by y. 
We have shown that there exists a set of codewords X (), X I, ... , X 2' _I for which 
2'-1 
L Wj ::: 2ky. 
j=() 
Without loss of generality, let us assume that the Xi are sorted in increasing order of 
Wi. Suppose that we remove the half of the codewords that have the largest values W/< 
that is, we remove the codewords that have the highest probability of yielding an error 
when being sent. We claim that each Xi, i < 2k - l, must satisfy Wi ::: 2y. Otherwise 
we would have 
242 

9.5* CODING: SHANNON'S THEOREM 
2"-1 
L Wj >2 k - I(2y)=2 k y, 
j=2 4- 1 
a contradiction. (We used similar reasoning in the proof of Markov's inequality in Sec-
tion 3.l.) 
We can set up new encoding and decoding functions on all (k -
I)-bit strings using 
just these 2 k- 1 codewords, and now the error probability for every codeword is simul-
taneously at most 2y. Hence we have shown that. when k -
1 ::s n (1 -
H (p) -
8). 
there exists a code such that the probability that the receiver fails to obtain the correct 
message is at most 2y for any message that is sent. Since 8 and y were arbitrary con-
stants, let y' = y/2 and 8' = 8/2. Then we have k -
1 ::s nO - H(p) -
28'). which 
implies that k ::s nO - H(p) - 8'), and so the probability that the decoding fails for 
any individual codeword is bounded by y'. This is exactly the statement of the first half 
of the theorem, with y' and 8' in place of y and 8, and hence this part of the theorem 
has been proven. 
Having completed the first half of the theorem, we now move to the _"econd half: for 
any constants 8, y > 0 and for n sufficiently large. there do not exist (k.ll) encoding 
and decoding functions with k 2: n (1 - H( p) + 8) such that the probabilit: of decoding 
correctly is at least y for a k-bit input message chosen uniformly at random. 
Before giving the proof. let us first consider some hel pful intuition. \\'e know that 
the number of errors introduced by the channel i". with high probability. between 
i(p-E)nl and L(p+E')nJ forasuitableconstantf > O. Supro"e that \\c try to set up 
the decoding function so that each codeword i" decoded properly \\ hene\er the num-
ber of errors is between (p -
E)II and (p + f )n. Then e'.!L'h codeword is associated 
with 
bit sequences by the decoding function: the last inequality follows from Corollary 9.3. 
But there are 2k different codeword". and 
')IIHII'I 
,)IIH(!)j 
2k_-__ > 21111--HI!JI-hl_ 
.... __ > 211 
11+1 -
n+l 
when n is sufficiently large. Since there are only 211 possible bit sequences that can be 
received, we cannot create a decoding function that always decodes properly whenever 
the number of errors is between ( p -
E) II and (p + E) n. 
We now need to extend the argument for any encoding and decoding functions. This 
argument is more complex. since we cannot assume that the decoding function neces-
sarily tries to decode properly whenever the number of errors is between (p -
E)n and 
( P + E) n, even though this would seem to be the best strategy to pursue. 
Given any fixed encoding function with codewords X().XI, .. ,.X24_1 and any fixed 
decoding function. let z be the probability of successful decoding. Define Si to be the 
set of all received sequences that decode to Xi. Then 
243 

ENTROPY, RANDOMNESS, AND INFORMATION 
2'-1 
Z = L L Pr((xi is sent) n (R = s)) 
i=() SES; 
2"-1 
= L L Pr(xi is sent) Pr(R = S I Xi is sent) 
i=() SES, 
The second line follows from the definition of conditional probability. The third line 
uses the fact that the message sent and hence the codeword sent is chosen uniformly at 
random from all codewords. The fourth line is just the definition of the weight func-
tion. 
To bound this last line, we again split the summation L;~~I LSES, W(Xi, s) into two 
parts. LetSi.1 = {SES;: 1~(Xi,s)-pnl > .sn}andSi.2 = {SESi: 1~(Xi,s)-pnl ~ 
En}, where again E > 0 is some constant to be determined. Then 
L W(Xi'S) = L W(Xi,S) + L W(Xi'S). 
Now 
which can be bounded using Chernoff bounds. The summation on the right is sim-
ply the probability that the number of errors introduced by the channel is not between 
(p - E)n and (p + E)n, which we know from previous arguments is at most 2e-
p"n!3p. 
This bound is equivalent to assuming that decoding is successful even if there are too 
many or too few errors introduced by the channel; since the probability of too many or 
too few errors is small, this assumption still yields a good bound. 
To bound L.IES'.2 W(Xi,S), we note that W(Xi'S) is decreasing in ~(Xi'S). Hence, 
for S E Si.2, 
Therefore, 
244 

9.6 EXERCISES 
We continue with 
2k _I 
= 2
1
, ~(L w(x"sJ+ L Ii'(Xj,S») 
1-0 
SE 51. 1 
.1,= 5, . .2 
< 2e- E2n /3p + ~2-H(P)II( 1 - P )"11211. 
-
2" 
p 
In this last line, we have used the important fact that the sets of bit sequences Si and 
hence all the Su are disjoint, so their total size is at most 211. This is where the fact 
that we are using a decoding function comes into play_ allowing us to establish a useful 
bound. 
To conclude, 
= 2c-"n/3p + ((1 ~ p)' 2-' r 
As long as we choose E sufficiently small that 
C~P)'r8<1, 
then, when n is sufficiently large, z < y, which proves Theorem 9.8. 
• 
Shannon's theorem demonstrates the existence of codes that transmit arbitrarily closely 
to the capacity of the binary symmetric channel over long enough blocks. It does not 
give explicit codes, nor does it say that such codes can be encoded and decoded ef-
ficiently. It took decades after Shannon's original work before practical codes with 
near-optimal performance were found. 
9.6. Exercises 
Exercise 9.1: (a) Let S = 
L~~I l/k 2. Consider a random variable X such that 
Pr(X = k) = I/Sk 2 for integers k = 1, ... ,10. Find H(X). 
(b) Let S = L~~I l/k 3. Consider a random variable X such that Pr(X = k) = 
I/Sk 3 for integers k = 1, ... ,10. Find H(X). 
245 

ENTROPY, RANDOMNESS, AND INFORMATION 
(c) Consider Sa = L~~I 1/ka, where ex > 1 is a constant. Consider random vari-
ables Xa such that Pr(Xa = k) = 1/Saka for integers k = 1, .... 10. Give an intuitive 
explanation explaining whether H( Xa) is increasing or decreasing with ex and why. 
Exercise 9.2: Consider an n-sided die, where the ith face comes up with probability 
Pl. Show that the entropy of a die roll is maximized when each face comes up with 
equal probability lin. 
Exercise 9.3: (a) A fair coin is repeatedly flipped until the first heads occurs. Let X 
be the number of flips required. Find H (X). 
(b) Your friend flips a fair coin repeatedly until the first heads occurs. You want to 
determine how many flips were required. You are allowed to ask a series of yes-no 
questions of the following form: you give your friend a set of integers, and your friend 
answers "yes" if the number of flips is in that set and "no" otherwise. Describe a strat-
egy so that the expected number of questions you must ask before determining the 
number of flips is H( X). 
(c) Give an intuitive explanation of why you cannot come up with a strategy that 
would allow you to ask fewer than H (X) questions on average. 
Exercise 9.4: (a) Show that 
is finite. 
(b) Consider the integer-valued discrete random variable X given by 
I 
Pr (X = k) = SkI n:2 k ' 
k 2: 2. 
Show that H ( X) is unbounded. 
Exercise 9.5: Suppose p is chosen uniformly at random from the real interval [0, IJ. 
Calculate E[H (p)]. 
Exercise 9.6: The conditioneZ/ entropy H (Y I X) is defined by 
H(Y I X) = L Pr«X = x) n (Y = y)) log2 Prey = y I X = x). 
x.r 
If Z = (X, Y), show that 
H(Z) = H(X) + H(Y I X). 
Exercise 9.7: One form of Stirling's formula is 
J2rrn (~)" < n' < J2rrn (~)" e'/(I',,'-
Using this, prove 
246 

9.6 EXERCISES 
( 
n ) > 2
11
H(q). 
qn 
-
2jll 
which is a tighter bound than that of Lemma 9.2. 
Exercise 9.8: We have shown in Theorem 9.S that we can use a recursive procedure 
to extract, on average, at least Llog2 mJ - 1 independent, unbiased bits from a num-
ber X chosen uniformly at random from S = {O ..... m -
I}. Consider the following 
extraction function: let ex = Llog2 m J, and write 
where each f3i is either ° 
or 1. 
Let k be the number of values of i for which f3i equals 1. Then we split S into k dis-
joint subsets in the following manner: there is one set for each value of f3i that equals 
1, and the set for this i has 2i elements. The assignment of S to sets can be arbitrary, 
as long as the resulting sets are disjoint. To get an extraction function, we map the ele-
ments of the subset with 2 i elements in a one-to-one manner with the 2i binary strings 
of length i. 
Show that this mapping is equivalent to the recursive extraction procedure given in 
Theorem 9.S in that both produce i bits with the same probability for all i. 
Exercise 9.9: We have shown that we can extract. on average. at least ling ~ III J -I inde-
pendent, unbiased bits from a number chosen uniformly at random from {O ..... III -
I}. 
It follows that if we have k numbers chosen independently and uniformly at random 
from {o, .... m -
I} then we can extract. on average. at least k Jng~ mJ - k indepen-
dent, unbiased bits from them. Give a better procedure that extracts. on average. at 
least k Llog2 mJ - 1 independent. unbiased bits from these numbers. 
Exercise 9.10: Suppose that we have a means of generating independent. fair coin flips. 
(a) Give an algorithm using the coin to generate a number uniformly from {O, l. .... 
11 -
I}, where n is a power of 2, using exactly log2 11 flips. 
(b) Argue that, if n is not a power of 2, then no algorithm can generate a number uni-
formly from {a, 1, ... , n -
I} using exactly k coin flips for any fixed k. 
(c) Argue that, if n is not a power of 2, then no algorithm can generate a number uni-
formly from {a, 1, ... , 11 -
I} using at most k coin flips for any fixed k. 
(d) Give an algorithm using the coin to generate a number uniformly from {O. 1. .... 
n -
I}, even when n is not a power of 2, using at most 2ilog2 III expected flips. 
Exercise 9.11: Suppose that we have a means of generating independent. fair coin flips. 
(a) Give an algorithm using the fair coin that simulates flipping a biased coin that 
comes up heads with probability p. The expected number of flips your algorithm 
uses should be at most 2. (Hint: Think of p written as a decimal in binary, and 
use the fair coin to generate binary decimal digits.) 
247 

ENTROPY, RANDOMNESS, AND INFORMATION 
X 
HHTTHTHHHTHHHTTTHTTT 
y 
H 
T 
H 
H 
T 
T 
Z 
H 
H 
T 
H 
T 
H 
T 
H 
T 
H 
y 
HTHHTT 
Z 
HHTHTHTHTH 
H 
T 
H 
T 
H 
H 
H 
T 
T 
T 
T 
Figure 9.3: After running A on the input sequence X, we can derive further sequences Y and Z; 
after running A on each of Y and Z, we can derive further sequences from them; and so on. 
(b) Give an algorithm using the coin to generate a number uniformly from {O, 1, ... , 
n -
I}. The expected number of flips your algorithm uses should be at most 
1l0g1 nl + 2. 
Exercise 9.12: Here is an extraction algorithm A whose input is a sequence X 
Xl, X2, ... , Xn of n independent flips of a coin that comes up heads with probability 
p> 1/2. Break the sequence into Ln/2J pairs,ai = (X2i-I,X2i) fori = 1, ... ,Ln/2J. 
Consider the pairs in order. If .Vi = (heads, tails) then output a 0; if ai = (tails, heads) 
then output a 1; otherwise, move on to the next pair. 
(a) Show that the bits extracted are independent and unbiased. 
(b) Show that the expected number of extracted bits is Ln/2J2p(l- p) ~ np(l -
p). 
(c) We can derive another set of flips Y = )'1, )'2, ... from the sequence X as follows. 
Start with j, k = 1. Repeat the following operations until j = Ln / 2 J: If aj = 
(heads, heads), set J'k to heads and increment j and k; if aj = (tails, tails), set 
J'k to tails and increment j and k; otherwise, increment j. See Figure 9.3 for an 
example. 
The intuition here is that we take some of the randomness that A was unable 
to use effectively and re-use it. Show that the bits produced by running A on Y 
are independent and unbiased, and further argue that they are independent of those 
produced from running A on X. 
(d) We can derive a second set of flips Z = 21,22, ... ,Zln/2J from the sequence X as 
follows: let 2i be heads if)'i = (heads, heads) or (tails, tails), and let Zi be tails 
otherwise. See Figure 9.3 for an example. Show that the bits produced by running 
A on Z are independent and unbiased, and further argue that they are independent 
of those produced from running A on X and Y. 
(e) After we derive and run A on Y and Z, we can recursively derive two further se-
quences from each of these sequences in the same way, run A on those, and so on. 
See Figure 9.3 for an example. Let A (p) be the average number of bits extracted 
for each flip (with probability p of coming up heads) in the sequence X, in the 
limit as the length of the sequence X goes to infinity. Argue that A(p) satisfies the 
recurrence 
248 

9.6 EXERCISES 
1 
') 
2 
( 
p2 
) 
1) 
2 
A(p)=p(l-P)+2(P-+q)A p2+q2 
+2A(p~+(l-p)). 
(f) Show that the entropy function H (p) satisfies this recurrence for A (p). 
(g) Implement the recursive extraction procedure explained in part (e). Run it 1000 
times on sequences of 1024 bits generated by a coin that comes up heads with prob-
ability p = 0.7. Give the distribution of the number of flips extracted over the 1000 
runs and discuss how close your results are to 1024· H(0.7). 
Exercise 9.13: Suppose that, instead of a biased coin, we have a biased six-sided die 
with entropy h > O. Modify our extraction function for the case of biased coins so that 
it extracts, on average, almost h random bits per roll from a sequence of die rolls. Prove 
formally that your extraction function works by modifying Theorem 9.5 appropriately. 
Exercise 9.14: Suppose that, instead of a biased coin. we have a biased six-sided die 
with entropy h > O. Modify our compression function for the case of biased coins so 
that it compresses a sequence of 11 die rolls to almost II h bits on average. Prove for-
mally that your compression function works by modifying Theorem 9.6 appropriately. 
Exercise 9.15: We wish to compress a sequence of independent. identically distrib-
uted random variables XI, X 2, .... Each XJ takes on one of II values. We map the ith 
value to a codeword, which is a sequence of t i bits. We wish these codewords to have 
the property that no codeword is the prefix of any other codeword. 
(a) Explain how this property can be used to easily decompress the string created by 
the compression algorithm when reading the bits sequentially. 
(b) Prove that the £i must satisfy 
This is known as the Kraft inequality. 
Exercise 9.16: We wish to compress a sequence of independent, identically distrib-
uted random variables XI, X 2 , .... Each X) takes on one of n values. The ith value 
occurs with probability Pi, where PI 2: P2 2: ... 2: Pn. The result is compressed as 
follows. Set 
i-I 
Ti = LPi' 
)=1 
and let the ith codeword be the first ilog2(l/Pi)l bits of~·. Start with an empty string, 
and consider the X) in order. If Xi takes on the ith value. append the ith codeword to 
the end of the string. 
(a) Show that no codeword is the prefix of any other codeword. 
(b) Let z be the average number of bits appended for each random variable Xi' Show 
that 
H(X) ~ z ~ H(X) + l. 
249 

ENTROPY, RANDOMNESS, AND INFORMATION 
Exercise 9.17: Arithmetic coding is a standard compression method. In the case where 
the string to be compressed is a sequence of biased coin flips, it can be described as 
follows. Suppose that we have a sequence of bits X = (XI, X 2 , ... , X n ), where each 
Xi is independently ° 
with probability p and 1 with probability 1 -
p. The sequences 
can be ordered lexicographically, so for x = (XI,X2,· .. ,xn) and Y = (Yb Y2,·· ·,Yn) 
we say x < Y if Xi = ° 
and Yi = 1 in the first coordinate i such that Xi i Yi· If Zx is 
the number of zeroes in the string x, then define p(x) = p:x(l -
p)n-:x and q(x) = 
L,.<x p(y). 
(a) Suppose we are given X = (Xl, X 2 , ... , Xn) sequentially. Explain how to com-
pute q (X) in time O(n). (You may assume that any operation on real numbers 
takes constant time.) 
(b) Argue that the intervals [q(x),q(x) + p(x)) are disjoint subintervals of [0,1). 
(c) Given (a) and (b), the sequence X can be represented by any point in the in-
terval [q(X), q(X) + p(X)). Show that we can choose a codeword in [q(X), 
q(X) + p(X)) with ilog2(l/p(X))l + 1 binary decimal digits to represent X in 
such a way that no codeword is the prefix of any other codeword. 
(d) Given a codeword chosen as in (c), explain how to decompress it to determine the 
corresponding sequence (Xl, X 2, ... , Xn). 
(e) Using a Chernoff bound, argue that log2(l/p(X)) is close to nH(p) with high 
probability. Hence this approach yields an effective compression scheme. 
Exercise 9.18: Alice wants to send Bob the result of a fair coin flip over a binary 
symmetric channel that flips each bit with probability p < 1/2. To avoid errors in 
transmission, she encodes heads as a sequence of 2k + I zeroes and tails as a sequence 
of 2k + 1 ones. 
(a) Consider the case where k = 1, so heads is encoded as 000 and tails as Ill. For 
each of the eight possible sequences of 3 bits that can be received, determine the 
probability that Alice flipped a heads conditioned on Bob receiving that sequence. 
(b) Bob decodes by examining the 3 bits. If two or three of the bits are 0, then Bob de-
cides the corresponding coin flip was a heads. Prove that this rule minimizes the 
probability of error for each flip. 
(c) Argue that, for general k, Bob minimized the probability of error by deciding the 
flip was heads if at least k + 1 of the bits are 0. 
(d) Give a formula for the probability that Bob makes an error that holds for general 
k. Evaluate the formula for p = 0.1 and k ranging from I to 6. 
(e) Give a bound on the probability computed in part (d) using Chernoff bounds. 
Exercise 9.19: Consider the following channel. The sender can send a symbol from 
the set {O, 1,2,3, 4}. The channel introduces errors; when the symbol k is sent, the 
recipient receives k + 1 mod 5 with probability 1/2 and receives k - 1 mod 5 with prob-
ability 1/2. The errors are mutually independent when multiple symbols are sent. 
Let us define encoding and decoding functions for this channel. A (j, n) encoding 
function Enc maps a number in {O, 1, ... , j -
I} into sequences from {O, 1,2,3, 4}", 
and a (j, n) decoding function Dec maps sequences from {O, I, 2, 3,4} II back into 
250 

9.6 EXERCISES 
{a, 1, ... , j -I}. Notice that this definition is slightly different than the one we used for 
bit sequences over the binary symmetric channel. 
There are (1, 1) encoding and decoding functions with zero probability of error. The 
encoding function maps ° 
to ° 
and 1 to I. When a 0 is sent. the receiver will receive 
either a 1 or 4, so the decoding function maps I and -+ back to O. When a I is sent. the 
receiver will receiver either a 2 or 0, so the decoding function maps :2 and 0 back to I. 
This guarantees that no error is made. Hence at least one bit can be sent without error 
per channel use. 
(a) Show that there are (5.2) encoding and decoding functions with zero probability 
of error. Argue that this means more than one bit of information can be sent per 
use of the channel. 
(b) Show that if there are (j. II) encoding and decoding functions with zero probabil-
ity of error, then 11 2: log2 .i I( log2 5 -
I). 
Exercise 9.20: A binar.v erasure channel transfers a sequence of 11 bits. Each bit either 
arrives successfully without error or fails to arrive successfully and is replaced by a'?' 
symbol, denoting that it is not known if that bit is a ° 
or a l. Failures occur indepen-
dently with probability p. We can define (k, ll) encoding and decoding functions for 
the binary erasure channel in a similar manner as for the binary symmetric channel. ex-
cept here the decoding function Dec: {G. I, ?}/1 ---+ {a, I}~ must handle sequences with 
the '?' symbol. 
Prove that, for any p > 0 and any constants 6. y > O. if II is sufficiently large then 
there exist (k, n) encoding and decodi ng functions wi th k S II ( I - I) - 6) such that the 
probability that the receiver fai Is to obtain the correct message is at most y for every 
possible k -bit input message. 
Exercise 9.21: In proving Shannon's theorem. we used the following decoding method: 
Look for a codeword that differs from the received sequence of bits in between (fJ -
E) II 
and (p + s) n places, for an appropriate choice of s; if there is only one such codeword. 
the decoder concludes that that codeword was the one sent. Suppose instead that the 
decoder looks for the codeword that differs from the received sequence in the smallest 
number of bits (breaking ties arbitrarily), and concludes that that codeword was the one 
sent. Show how to modify the proof of Shannon's theorem for this decoding technique 
to obtain a similar result. 
251 

CHAPTER TEN 
The Monte Carlo Method 
The Monte Carlo method refers to a collection of tools for estimating values through 
sampling and simulation. Monte Carlo techniques are used extensively in almost all 
areas of physical sciences and engineering. In this chapter, we first present the basic 
idea of estimating a value through sampling, using a simple experiment that gives an 
estimate of the value of the constant Jr. Estimating through sampling is often more com-
plex than this simple example suggests. We demonstrate the potential difficulties that 
can arise in devising an efficient sampling procedure by considering how to appropri-
ately sample in order to estimate the number of satisfying assignments of a disjunctive 
normal form (DNF) Boolean formula. 
We then move to more general considerations, demonstrating a general reduction 
from almost uniform sampling to approximate counting of combinatorial objects. This 
leads us to consider how to obtain almost uniform samples. One method is the Markov 
chain Monte Carlo (MCMC) technique, introduced in the last section of this chapter. 
10.1. The Monte Carlo Method 
Consider the following approach for estimating the value of the constant Jr. Let (X, Y) 
be a point chosen uniformly at random in a 2 x 2 square centered at the origin (0,0). 
This is equivalent to choosing X and Y independently from a uniform distribution on 
[-I, I]. The circle of radius I centered at (0,0) lies inside this square and has area Jr. 
If we let 
z={~ otherwise, 
then - because the point was chosen uniformly from the 2 x 2 square - the probabil-
ity that Z = I is exactly the ratio of the area of the circle to the area of the square. See 
Figure 10.1. Hence 
Jr 
Pr(Z = 1) = -. 
4 
252 

10.1 THE MONTE CARLO METHOD 
(-I, I) ..--......., __ 
~-.., n.l) 
(-I.-I) I.....-_....;;:a __ 
~_-' iI. -II 
Figure 10.1: A point chosen uniforml~ at randum in the 
square has probability rr/4 of landing in the circle. 
Assume that we run this experiment m times (with X and Y chosen independently 
among the runs), with Z i being the value of Z at the i th run. If \\. = L ;'~ I Z i, then 
and hence W' = (4/m)W is a natural estimate for Jr. Applying the Chernoff bound of 
Eqn. (4.6), we compute 
I 
(I 
mJrI 
Elll:T) 
Pr(1 W - Jrl 2: £Jr) = Pr 
W - 4 
2: -.+-
= Pr (I W - E [W] I ~ £ E [ \\' 1 ) 
Therefore, by using a sufficiently large number of samples we can obtain. \\ith high 
probability, as tight an approximation of Jr as we wish. 
This method for approximating Jr is an example of a more general cIa"" of approxi-
mation algorithms that we now characterize. 
Definition 10.1: A randomized algorithm gives an (£, o)-approximationtr)!" lhe rall/e 
V if the output X of the algorithm satisfies 
Pr ( I X -
V I ~ £ V) 2: I - o. 
Our method for estimating Jr gives an (£, o)-approximation. as long a" E < I and we 
choose m large enough to make 
Algebraic manipulation yields that choosing 
m2: 
is sufficient. 
12In(2/o) 
Jr£2 
We may generalize the idea behind our technique for estimating Jr to provide a rela-
tion between the number of samples and the quality of the approximation. We use the 
following simple application of the Chernoff bound throughout this chapter. 
253 

THE MONTE CARLO METHOD 
Theorem 10.1: Let XI, ... , XIII be independent and identical!.v distributed indicator 
random variables, with /1 = E[XiJ. Ifm 2: (31n(2/0))/£2/1, then 
That is, m samples provide an (£,0) -approximation for /1. 
The proof is left as Exercise 10.1. 
More generally, we will want an algorithm that approximates not just a single value 
but instead takes as input a problem instance and approximates the solution value for 
that problem. Here we are considering problems that map inputs x to values Vex). For 
example, given an input graph, we might want to know an approximation to the num-
ber of independent sets in the graph. 
You might ask why we should settle for an approximation; perhaps we should aim 
for an exact answer. In the case of Jr, we cannot obtain an exact answer because Jr is 
an irrational number. Another reason for seeking an approximation is that, as we shall 
see shortly, there are problems for which the existence of an algorithm that gives an ex-
act answer would imply that P = NP, and hence it is unlikely that such an algorithm 
will be found. This, however, does not preclude the possibility of an efficient approxi-
mation algorithm. 
Definition 10.2: A fully polynomial randomized approximation scheme (FPRAS) for 
a problem is a randomized algorithm for which, given an input x and any parameters 
£ and 0 with 0 < £,0 < 1, the algorithm outputs an (£,o)-approximation to Vex) in 
time that is polynomial in 1/£, In 0- 1, and the size afthe input x. 
Exercise 10.3 considers a seemingly weaker but actually equivalent definition of an 
FPRAS that avoids the parameter o. 
The Monte Carlo method essentially consists of the approach we have outlined here 
to obtain an efficient approximation for a value V. We require an efficient process 
that generates a sequence of independent and identically distributed random samples 
XI, X 2 , ... , XII such that ElXtJ = V. We then take enough samples to get an (£,0)-
approximation to V. Generating a good sequence of samples is often a nontrivial task 
and is a major focus of the Monte Carlo method. 
The Monte Carlo method is also sometimes called Monte Carlo simulation. As an 
example, suppose we want to estimate the expected price of a stock sometime in the 
future. We may develop a model where the price p( YI , ... , Yk ) of the stock at that time 
depends on random variables YI , Y2 , ... , Yk . If we can repeatedly generate indepen-
dent random vectors (y 1,."2, ... , Yk) from the joint distribution of the Yi , then we can 
repeatedly generate independent random variables XI, Xl, ... , where 
We can then use the Xi to estimate the expected future price E [p (YI , ••• , Yd] with the 
Monte Carlo method. That is, by simulating the possible future outcomes of the ~. 
many times, we can estimate the desired expectation. 
254 

10.2 APPLICATION: THE DNF COL"NTING PROBLEM 
10.2. Application: The DNF Counting Problenl 
As an example of an estimation problem that reqLlire~ a Ihlntri\ial ~al11pling technique, 
we consider the problem of counting the number of "ati~t\ ing a~~ignmenh of a Boolean 
formula in disjunctive normal form (DNF). A D:\F fl)J'Jl1ub i" ~I Ji"junl'tion (OR) of 
clauses C, v C2 V ... V C r , where each clause is a l'unjunl'tiun (.-'\:'\D I of literal--. Fm 
example, the following is a DNF formula: 
Recall from Section 6.2.2 that, in a standard satisfiability prl)bkm. the input formula 
is a conjunction (AND) of a set of clauses, and each claLhe i" the Ji"JUlh:til1n (OR I uf 
literals. This is commonly called conjunctille l1ormal!()J'lII (C:'\F I. \\'hik Jetermining 
the satisfiability of a formula in CNF form is difficult. determining the "athtiabilit~ uf 
a formula in DNF form is simple. Since a satisfying as"ignment f~'r .1 O\T fllJ'll1Ula 
needs to satisfy only one clause, it is easy to find a satisfyi ng ~b"lgnment ~ 11' pn l\ e that 
it is not satisfiable. 
How hard is it to exactly count the number of satisfying as"ignmenr-- llf .I O:\F fm-
mula? Given any CNF formula H, we can apply de Morgan'" Lt\\" tll llbtain a D\T 
formula for H, the negation of the formula H, with the same number llf \ .triabk" ~lI1J 
clauses as the original CNF formula. The formula H has a ~ati"f~ 1I1g LI""lgnment if 
and only if there is some assignment for the variables that does not "ati"f~ H. ThL"'. H 
has a satisfying assignment if and only if the number of satisfying a""ignment' l If H i, 
strictly less than 2
1l
, the total number of possible assignments for II Bllllkdn \ari~lbk,. 
We conclude that counting the number of satisfying assignment" of ~I O\F fllrIlWLt i, 
at least as hard as solving the NP-complete problem SAT. 
There is a complexity class associated with the problem of l'lHlnting 'lllutilln, tll 
problems in NP, denoted by uP and pronounced "sharp-P'". Formall~ .• 1 prl1bkm i, in 
the class jp if there is a polynomial time, nondeterministic Turing 1l1~1I.:hine 'LII.:h that. 
for any input 1, the number of accepting computations equals the number llf Jitferent 
solutions associated with the input 1. Counting the number of "ati'-f~ ing ~I"lgnment' 
of a DNF formula is actually tiP-complete: that is. this problem j" a" harJ ~I' ~1I1~ llther 
problem in this class. Other complete problems for the class ::P il1l'luJe L'llunting the 
number of Hamiltonian cycles in a graph and counting the number (If perfcl..,t matL'h-
ings in a bipartite graph. 
It is unlikely that there is a polynomial time algorithm that compute" the C\al't num-
ber of solutions of a UP-complete problem, as at the very least such an algl)rithm \\ould 
imply that P = NP. It is therefore interesting to find an FPRAS for the number of sat-
isfying assignments of a DNF formula. 
10.2.1. The Naive Approach 
\Ve start by trying to generalize the approach that we used to approximate ;T. and we 
demonstrate why it is unsuitable in general. We then show hO\\' to improve our sam-
pling technique in order to solve the problem. 
255 

THE MONTE CARLO METHOD 
DNF Counting Algorithm I: 
Input: A DNF formula F with n variables. 
Output: Y = an approximation of c( F). 
1. X +- O. 
2. For k = I to m, do: 
(a) Generate a random assignment for the n variables, chosen uniformly at 
random from all 21l possible assignments. 
(b) If the random assignment satisfies F, then X +- X + 1. 
3. Return Y +- (X/m)21l. 
Algorithm 10.1: DNF counting algorithm 1. 
Let c(F) be the number of satisfying assignments of a DNF formula F. Here we 
assume that c(F) > 0, since it is easy to check whether c(F) = 0 before running 
our sampling algorithm. In Section 10.1 we approximated Jr by generating points uni-
formly at random from the 2 x 2 square and checking to see if they were in the target: 
a circle of radius 1. We try a similar approach in Algorithm 10.1: we generate assign-
ments uniformly at random for the n variables and then see if the resulting assignment 
is in the target of satisfying assignments for F. 
Let X k be 1 if the kth iteration in the algorithm generated a satisfying assignment and 
o otherwise. Then X = L%~l Xb where the X k are independent 0-1 random variables 
that each take the value 1 with probability c(F)/21l. Hence, by linearity of expectations, 
E[X]21l 
E[Y] = 
= c(F). 
In 
Applying Theorem 10.1, we see that X/In gives an (E, o)-approximation of c(F)/2'Z, 
and hence that Y gives an (E, o)-approximation of c(F), when 
3 . 211 In(2/o) 
In > 
'J 
• 
-
E-c(F) 
If c(F) 2: 2
11/a(n) for some polynomial a, then the foregoing analysis tells us we 
only need a number of samples In that is polynomial in n, lie, and InO/o). We cannot, 
however, exclude the possibility that c(F) is much less than 21l. In particular, c(F) 
might be polynomial in n. Since our analysis requires a number of samples In that is 
proportional to 2
11/c(F), our analysis does not yield that the run time of the algorithm 
is always polynomial in the problem size. 
This is not simply an artifact of the analysis. We provide a rough sketch of an ar-
gument that is elaborated in Exercise lOA. If the number of satisfying assignments is 
polynomial in n and if at each step we sample uniformly at random from all 211 pos-
sible assignments, then with high probability we must sample an exponential number 
of assignments before finding the first satisfying assignment. We can conclude, for 
example, that we cannot distinguish between instances with n, n 2, and n 3 satisfying 
256 

10.2 APPLICATION: THE DNF COUNTING PROBLEM 
assignments without considering exponentially Illany random assignments, since with 
high probability we would obtain zero satisfying assignments in all three cases. 
The problem with this sampling approach is that the set of satisfying assignments 
might not be sufficiently dense in the set of all assignments. This is an additional re-
quirement of our sampling technique that was not explicit before. In the phrasing of 
Theorem 10.1, the value f1 that we are attempting to approxi mate needs to be sut1i-
ciently large that sampling is efficient. 
To obtain an FPRAS for this problem. we need to de\ise a better sampling scheme 
that avoids wasting so many steps on assignments that do not satisfy the formula. \Vc 
need to construct a sample space that includes all the satisfying assignments of F and. 
moreover, has the property that these assignments are sufficiently dense in the sample 
space to allow for efficient sampling. 
10.2.2. A Fully Polynomial Randomized Scheme for DNF Counting 
We now revise our sampling procedure to obtain an FPRAS. Let F = C I V C 2 v· .. v C" 
and assume without loss of generality that no clause includes a variable and its nega-
tion. (If there is such a clause, it is not satisfiable and we can eliminate it from the 
formula.) A satisfying assignment of F needs to satisfy at least one of the clauses 
C], ... , Ct. Each clause is a conjunction of literals, so there is only one assignment of 
the variables appearing in the clause that satisfies the clause. All other variables can 
have arbitrary values. For example, for the clause (XI 1\ X~ 1\ X3) to be satisfied. XI and 
.r~ must be set to True and X2 must be set to False. 
It follows that if clause C has e 
i literals then there are exactly 2
11
- 1 i satisfying as-
signments for Ci. Let SCi denote the set of assignments that satisfy clause i, and let 
v = {(i. (/) I 1 ~ i ~ t and a ESC}. 
~otice that we know the size of U. since 
LISCI = lVI, 
i=1 
and we can compute I SCi I. 
The value that we want to estimate is given by 
C(F)=IUscl· 
1=1 
Here c( F) ~ I V I, since an assignment can satisfy more than one clause and thus ap-
pear in more than one pair in U. 
To estimate c(F), we define a subset S of V with size c(F). We construct this set 
by selecting, for each satisfying assignment of F, exactly one pair in V that has this 
.lssignment; specifically, we can use the pair with the smallest clause index number, 
giving 
S={(i,a) 11 ~i ~t, (lESC·, atf-SCjforj <i}. 
257 

THE MONTE CARLO METHOD 
DNF Counting Algorithm II: 
Input: A DNF formula F with n variables. 
Output: Y = an approximation of c( F). 
1. X +- O. 
2. For k = I to m, do: 
(a) With probability ISCil/L~=IISCI choose, uniformly at random, an 
assignment a E SCi. 
(b) If a is not in any SCj , j < i, then X +- X + 1. 
3. Return Y +- (X/m) L~=IISCil. 
Algorithm 10.2: DNF counting algorithm II. 
Since we know the size of V, we can estimate the size of S by estimating the ratio 
I SI/ I V I. We can estimate this ratio efficiently if we sample uniformly at random from 
V using our previous approach, choosing pairs uniformly at random from V and count-
ing how often they are in S. We can avoid the problem we encountered when simply 
sampling assignments at random. because S is relatively dense in U. Specifically, since 
each assignment can satisfy at most t different clauses, ISI/IVI ~ lit. 
The only question left is how to sample uniformly from V. Suppose that we first 
choose the first coordinate. i. Because the ith clause has I SCi I satisfying assignments, 
we should choose i with probability proportional to I SC I. Specifically, we should 
choose i with probability 
ISC'I 
ISCil 
IVI 
We then can choose a satisfying assignment uniformly at random from SCi. This is 
easy to do: we choose the value True or False independently and uniformly at random 
for each literal not in clause i. Then the probability that we choose the pair (i, a) is 
Pre (i, a) is chosen) = Pr(i is chosen) . Pr(a is chosen I i is chosen) 
ISC; I 1 
----
IVI ISCil 
I 
WI' 
giving a uniform distribution. 
These observations are implemented in Algorithm 10.2. 
Theorem 10.2: DNF counting algorithm 11 is a fully polynomial randomized approxi-
mation scheme (FPRAS)for the DNF counting problem when m = 1(3t/£2) In(2/<5)l 
Proof: Step 2(a) of the algorithm chooses an element of V uniformly at random. The 
probability that this element belongs to S is at least lit. Fix any £ > 0 and <5 > 0, 
and let 
258 

10.3 FROM APPROXIMATE SAMPLING TO APPROXIMATE COUNTING 
r
3t 2l 
m = 
-----:;-In-
. 
8-
0 
Then m is polynomial in t, 8. and InO/o), and the processing time of each sample is 
polynomial in t. By Theorem 10.1, with this number of samples, X/Ill gives an (E. <5)-
approximation of c( F )/1 U I and hence Y gives an (8, o)-approximation of c( F ). 
• 
10.3. From Approximate Sampling to Approximate Counting 
The example of DNF formulas demonstrates that there is a fundamcntal connection 
between being able to sample from an appropriate space and being able to count the 
number of objects with some property in that space. In this section \\c present the 
outline of a general reduction that shows that, if you can sample alrno"t uniformly a 
,olution to a "self-reducible" combinatorial problem, then you can construct a random-
ized algorithm that approximately counts the number of solutions to that problem. \\"e 
demonstrate this technique for the problem of counting thc number of independent sets 
in a graph. In the next chapter, wc also consider the problem of Cl)Lll1tin~ the number 
l)f proper colorings in a graph, applying this technique there a' \\ell. 
We first need to formulate the concept of approximate uniform "ampiing. In this 
"etting we are given a problem instance in the form of an input x. and there is an un-
derlying finite sample space Q (.r) associated with the input. 
Definition 10.3: Let U' be tlze ( rolldolll) outpur or 0 sOlllpling oigorirlllli for 0 finite 
\(/mple space Q. The SOl11plillg olgorirlzlll genemres on [-uniform sample qf Q zf, for 
tilly sllbset S (~f Q. 
Pr(WES)--
<E. 
I 
lSI 1 
IQI -
-\ sampling olgorithm is 0 fully polynomial almost uniform sampler (FPAUS) for a 
{1mblem (f, given Oil input x and a parameter E > 0, it generates an E-uniform sample 
(lr Q (x) a/ld runs in time that is polynomial in In E-
1 {lnd the si~e (if the input x. 
In the next chapter. we introduce the notion of total variation distance, which allows 
fl)r an equivalent definition of an E-uniform sample. 
As an example, an FPAUS for independent sets would take as input a graph G = 
, V. E) and a parameter 8. The sample space would be all independent sets in the graph. 
The output would be an E-uniform sample of the independent sets. and the time to pro-
Juce such a sample would be polynomial in the size of the graph and In E' - I. In fact. in 
the reduction that follows we only need the running time to be polynomial in [-I. but 
\\ e use the standard definition given in Definition 10.3. 
Our goal is to show that. given an FPAUS for independent sets. we can construct an 
FPRAS for counting the number of independent sets. Assume that the input G has m 
-:dges. and let e I •...• em be an arbi trary orderi ng of the edges. Let E[ be the set of the 
rirst i edges in E and let G[ = (V, Ej). Note that G = Gill and that Gi -I is obtained 
:rom Gi by removing a single edge. 
259 

THE MONTE CARLO METHOD 
We let Q (Gi) denote the set of independent sets in Gi. The number of independent 
sets in G can then be expressed as 
Since Go has no edges, every subset of V is an independent set and Q (Go) = 2n. 
In order to estimate I Q (G) I, we need good estimates for the ratios 
More formally, we will develop estimates ri for the ratios ri, and then our estimate for 
the number of independent sets in G will be 
m 
i=l 
while the true number is 
In 
IQ(G)I = 2n IT rio 
i=l 
To evaluate the error in our estimate, we need to bound the ratio 
m 
_ 
IT
r 
R= 
...:.... 
r· 
i=l 
I 
Specifically, to have an (E,o)-approximation, we want Pr(IR -11 :S E) ::::: 1- o. We 
will make use of the following lemma. 
Lemma 10.3: Suppose thatforal! i, I :S i :S m, ri is an (E/2m,0/m)-approximation 
for rio Then 
Pr ( I R -
11 :S E) ::::: I - o. 
Proof: For each 1 :S i :S m, we have 
pr(lr. - r·1 < ~r.) > 1 -
~. 
I 
I 
-
2m 
I 
-
m 
Equivalently, 
Pr Iri -ril > -ri <-. 
( 
_ 
E 
) 
0 
2m 
m 
By the union bound, the probability that Iri - ril > (E/2m)ri for any i is at most 0; 
therefore, Iri - ril :S (E/2m)ri for all i with probability at least I - o. Equivalently, 
E 
ri 
E 
1--<-<1+-
2m -
ri -
2m 
holds for all i with probability at least I - o. When these bounds hold for all i, we can 
combine them to obtain 
260 

10.3 FROM APPROXIMATE SAMPLING TO APPROXIMATE COUNTING 
Estimating ri: 
Input: Graphs Gi-I = (V, E i - I ) and Gi = (V. EI ). 
Output: ri = an approximation of ri . 
1. X +- O. 
2. Repeat for M = iI296m 2c:-2 In(2mI3)l independent tri~ls: 
(a) Generate an (c:16m )-uniform sample from Q I G I -I). 
(b) If the sample is an independent set in G 1 • let X +- )( -r I. 
3. Return ri +- XI M. 
Algorithm 10.3: 
btirn~Jtll1g r!. 
( 
)
111 
l-c:.:'S 
l-~ 
2m 
giving the lemma. 
11/ 
_ 
< n~ < 
1=1 r l 
( I ~ _f )"; < I ~ E. 
'2111 
• 
Hence all we need is a method for obt~ining ~n (E 2111. is III )-~ppro\.im~tion for the ri. 
\\"e estimate each of these r~tios by ~ Monte C~rlo ~Igorithm th~t u"e" the FPACS for 
'~lllpling independent sets. To estim~te rl' we s~mple independent "eh in G! _I ~nd 
~:ompute the fraction of these sets that are ~Iso independent sets in G I • ~s de"cribed in 
.-\Igorithm 10.3. The constants in the procedure were chosen to f~cilitate the proof of 
Lemma lOA. 
Lemma 10.4: When m ::: I and 0 < c: .:'S I, the procedure for estimating ri yields an 
IE 121ll, 31m )-approximation for ri. 
Proof: We first show that ri is not too small, avoiding the problem that we found in 
Section 1O.2.l. Suppose that Gi -I and Gi differ in that edge {u, v} is present in Gi but 
not in Gi -I. An independent set in Gi is also an independent set in Gi -I, so 
.-\n independent set in Q (Gi -I) \ Q (Gi ) contains both u and V. To bound the size of 
the set Q(Gi - l ) \ Q(Gi ), we associate each I E S"2(Gi -l) \ Q(Gi ) with an indepen-
dent set I \ {v} E Q (Gi ). In this mapping an independent set I' E Q (G i ) is associ-
ated with no more than one independent set I' U {v} E Q (Gi -I) \ Q (G i). and thus 
Q (Gi -I) \ Q (Gi ) 1 .:'S 1 Q (Gi ) I· It follows that 
IQ(Gi)1 
IQ(Gi)1 
I 
n= 
= 
>-
IQ(Gi-I)1 
IQ(Gi)1 + IQ(Gi- l ) \ Q(Gi)1 -
2 
Now consider our M samples, and let X k = I if the kth sample is in Q(Gi ) and 
() otherwise. Because our samples are generated by an (c: 16m )-uniform sampler, by 
Definition 10.3 each Xi must satisfy 
261 

THE MONTE CARLO METHOD 
Since the X k are indicator random variables, it follows that 
and further, by linearity of expectations, 
We therefore have 
IE[I:t:l Xk] _ IQ(Gi)1 I < ~. 
M 
IQ(Gi-1)1 -
6m 
8 
<-
- 6m 
We now complete the lemma by combining (a) the fact just shown that E [ri] is close 
to ri and (b) the fact that 1; will be close to E[r;] for a sufficiently large number of 
samples. Using ri ~ 1/2, we have 
8 
1 
8 
1 
E[r·] > r· -
-
> - -
-
> -. 
I 
-
I 
6m -
2 
6m -
3 
Applying Theorem 10.1 yields that, if the number of samples M satisfies 
3ln(2m/8) 
272m 
M > 
= 1296m 8-- In -, 
-
(8/12m)?(1/3) 
8 
then 
pr(l~ - 11> _8 ) = pr(lr -Err]1 > _8 E[r']) < ~. 
E [/~ ] 
-
12171 
I 
I 
-
12 m 
I 
-
m 
Equivalently, with probability 1 -
8/171. 
8 
ri 
8 
1- -
< -- < 1+-. 
12m -
E[ri] -
12m 
(10.1) 
As IE[r;] - ri I :s 8/6m, we have that 
8 
E [r;] 
8 
1 - -- < -- < 1 + --. 
6mri -
ri 
-
6mri 
U sing that ri ~ 1/2 then yields 
8 
E[~] 
8 
1 -
-
< -- < 1 + -. 
3m -
ri 
-
3m 
(10.2) 
Combining equations (10.1) and (10.2), it follows that, with probability 1 - 8/m, 
1-~«1-~)(1--8 )<~«1+~)(1+_8 )<1+~. 
2m -
3 m 
12m 
-
ri -
3 m 
12m 
-
2m 
This gives the desired (8/2m, 8/m)-approximation. 
• 
262 

10.4 THE MARKOV CHAIN MONTE CARLO METHOD 
The number of samples M is polynomial in Ill. E'. and In 3-
1
• and the time for each sam-
ple is polynomial in the size of the graph and In E,-I. \Ve therefore hel\e the following 
theorem. 
Theorem 10.5: Given a fully po!.vnomial allllOST IIIzij()mz sWllpler (FPJ..US )fol" inde-
pendent sets in an)' graph, ~ve can consTmcr 0 .fidl.' I)()/Yn(lllziol rwzdomi;ed opproxi-
mation scheme (FPRAS)for the numher (~lindepelld{'n! SL'!.\ in 0 gmplz G. 
In fact, this theorem is more often used in the follm\ing form. 
Theorem 10.6: Given a ful!.v po!.vn0111 ial OIIlIOS! IIIzifi If'Ill \([mpler (F PA.US) for inde-
pendent sets in an)' graph }vith maxinllllll degree ([! IIlO\! ~. \I{' C([11 consTruct a full.v 
polynomial randomized approximation sclzeme I FPRAS ) ti Ir !Ize Illlllzher of indepen-
dent sets in a graph G with maxilllum degree (/f IIzm! ~. 
This version of the theorem follows from our pre\iolb argument. ,ince our graphs G/ 
are subgraphs of the initial graph G. Hence. if \\e "tart \\ ith a graph l)f maximum de-
gree at most .6.. then our FPAUS need only \\ork on graph, \\ ith maximum degree at 
most .6.. In the next chapter. we will see hO\\ to create an FP.-\L·S for graph" \\ith max-
imum degree 4. 
This technique can be applied to a broad range of combinatl)riall.'l)unting problems. 
For example, in Chapter II we consider its applicatinn tn tinding proper l.'oloring" of 
a graph G. The only requirement is that \\e can con-.truct a ,eliuence l)f retinements 
of the problem, starting with an instance that i" easy to CClunt I the number (If indepen-
dent sets in a graph with no edges. in our example) and ending \\ith the actual cnunting 
problem, and such that the ratio between the counts in "ucce",i\e in'tance, i, at mo"t 
polynomial in the size of the problem. 
10.4. The Markov Chain Monte Carlo Method 
The Monte Carlo method is based on sampling. It is often difficult to generate a random 
..;ample with the required probability distribution. For example. \\e "a\\ in the pre\ious 
"ection that we can count the number of independent sets in a graph if \\e can generate 
an almost uniform sample from the set of independent sets. But ho\\ can \\ e generate 
an almost uniform sample? 
The Markov chain Monte Carlo (MCMC) method provides a \ery general approach 
to sampling from a desired probability distribution. The basic idea i" to detine an 
ergodic Markov chain whose set of states is the sample space and \\ho"e "tationary dis-
tribution is the required sampling distribution. Let Xo, XI ..... XII be a run of the chain. 
The Markov chain converges to the stationary distribution from any "tarting state Xo 
and so. after a sufficiently large number of steps r, the distribution of the state X,. will 
be close to the stationary distribution. so it can be used as a sample. Similarly, repeat-
ing this argument with X,. as the starting point. we can use X ~r as a sample, and so 
on. We can therefore use the sequence of states X,..X2".X~)"" as almost indepen-
dent samples from the stationary distribution of the Markov chain. The efficiency of 
263 

THE MONTE CARLO METHOD 
this approach depends on (a) how large r must be to ensure a suitably good sample 
and (b) how much computation is required for each step of the Markov chain. In this 
section. we focus on finding efficient Markov chains with the appropriate stationary 
distribution and ignore the issue of how large r needs to be. Coupling, which is one 
method for determining the relationship between the value of r and the quality of the 
sample, is discussed in the next chapter. 
In the simplest case, the goal is to construct a Markov chain with a stationary distri-
bution that is uniform over the state space Q. The first step is to design a set of moves 
that ensures the state space is irreducible under the Markov chain. Let us call the set 
of states reachable in one step from a state x (but excluding x) the neighbors of x, de-
noted by N(x). We adopt the restriction that if y E N(x) then also x E NC.v). Generally 
N(x) will be a small set, so that performing each move is simple computationally. 
We again use the setting of independent sets in a graph G = (V, E) as an example. 
The state space is all of the independent sets of G. A natural neighborhood framework 
is to say that states x and y, which are independent sets, are neighbors if they differ in 
just one vertex. That is, x can be obtained from y by adding or deleting just one ver-
tex. This neighbor relationship guarantees that the state space is irreducible, since all 
independent sets can reach (respectively, can be reached from) the empty independent 
set by a sequence of vertex deletions (respectively, vertex additions). 
Once the neighborhoods are established, we need to establish transition probabil-
ities. One natural approach to try would be performing a random walk on the graph 
of the state space. This might not lead to a uniform distribution, however. We saw in 
Theorem 7.13 that, in the stationary distribution of a random walk, the probability of 
a vertex is proportional to the degree of the vertex. Nothing in our previous discus-
sion requires all states to have the same number of neighbors, which is equivalent to 
all vertices in the graph of the state space having the same degree. 
The following lemma shows that, if we modify the random walk by giving each 
vertex an appropriate self-loop probability, then we can obtain a uniform stationary 
distribution. 
Lemma 10.7: For a finite state space Q and neighborhood structure {N(X) I x E Q}, 
let N = maxrEQIN(x)l. Let M be an)~ number such that M 2: N. Consider a Markov 
chain where 
(fx f y and y E N(x), 
~fx f y and y ~ N(x), 
{fx = y. 
ff this chain is irreducible and aperiodic, then the stationary distribution is the uniform 
distribution. 
Proof: We show that the chain is time reversible and then apply Theorem 7.10. For 
any x f y, if Trx = Tr, then 
264 

10.4 THE MARKOV CHAIN MONTE CARLO METHOD 
since P\.\, = P\.x = II M. It follows that the uniform distribution 7[\ 
stationary distribution. 
l/lQI is the • 
Consider now the following simple Markov chain, whose states are independent sets 
in a graph G = (V, E). 
1. Xo is an arbitrary independent set in G. 
2. To compute X i + 1: 
(a) choose a vertex v uniformly at random from \': 
(b) if v E Xi then X i +1 = Xi \ {v}: 
(c) if v tj. Xi and if adding v to Xi still gin? .... an independent set. then Xi+ 1 
XiU{v}: 
(d) otherwise, X i + 1 = Xi. 
This chain has the property that the neighbors of a .... tate X, are all independent sets 
that differ from Xi in just one vertex. Since e\t:~ry state can reach and i .... reachable from 
the empty set. the chain is irreducible. Assuming that G h~h at lea .... t one edge (u. i ' ), 
then the state {v} has a self-loop (PI I > 0 I. and the chain i .... aperindic. Further. v,hen 
y f x, it follows that Pr .\ = l/lVi or O. Lemma 10.7 theret"me applie ..... and the sta-
tionary distribution is the uniform distributinn. 
10.4.1. The Metropolis Algorithm 
We have seen how to construct chains with a uniform stationary distribution. In some 
cases, however. we may v..ant to sample from a chain with a nonuniform stationary 
distribution. The Metropolis algorithm refers to a general construction that transforms 
any irreducible Markov chain on a state space Q to a time-reversible Markov chain 
with a required stationary distribution. The approach generalizes the idea we used be-
fore to create chains with uniform stationary distributions: add self-loop probabilities 
to states in order to obtain the desired stationary distribution. 
Let us again assume that we have designed an irreducible state space for our Markov 
chain: now we want to construct a Markov chain on this state space with a station-
ary distribution 7Tx = h(x)IB, where for all )( E Q we have b(x) > 0 and such 
that B = LXEQ b(x) is finite. As we see in the following lemma (which generalizes 
Lemma lO.7), we only need the ratios between the required probabilities: the SUIll B 
can be unknown. 
Lemma 10.8: For a finite state space Q and neighborhood structure {N ( X) I .r E Q I. 
let N = maxrEQIN(x)l. Let M he any number such that M 2:: N. For all.r E Q. let 
:TI > 0 be the desired probahility (~l state x in the stationary distrihurim1. C()nsider a 
.'v1arkov chain where 
j 
01 M) min(L 7T\ 17T, ) 
~lx f y and y E NLr). 
PI. \ = 
0 
~l x f y and y tj. N (,r ). 
I - L \ 
cic I p\ \ 
~l x = y. 
265 

THE MONTE CARLO METHOD 
Then, if this chain is irreducible and aperiodic, the stationary distribution is given by 
the probabilities Jrx . 
Proof: As in the proof of Lemma 10.7, we show that the chain is time reversible and 
apply Theorem 7.10. For any x f y, if Jrx ::s Jrr then P"r = 1 and Pr,x = Jrx/Jr". It 
follows that Jrx Pr. r = Jrr P,',x' Similarly, if Jrx > Jrr then PCY = Jry/Jrx and P",x = 1, 
and it follows that Jrx Pr. r = lrr Pr,x. By Theorem 7.10, the stationary distribution is 
given by the values Jrx . 
• 
As an example of how to apply Lemma 10.8, let us consider how to modify our previ-
ous Markov chains on independent sets. Let us suppose that now we want to create a 
Markov chain where, in the stationary distribution, each independent set I has proba-
bility proportional to ).11
1 for some constant parameter A > O. That is, Jrx = AI/xl/B, 
where I, is the independent set corresponding to state x and where B = Lx AI/rl. 
When A = 1, this is the uniform distribution; when A > 1, larger independent sets have 
a larger probability than smaller independent sets; and when A < 1, larger independent 
sets have a smaller probability than smaller independent sets. 
Consider now the following variation on the previous Markov chain for independent 
sets in a graph G = (V, E). 
1. Xo is an arbitrary independent set in G. 
2. To compute Xi + 1: 
(a) choose a vertex v uniformly at random from V; 
(b) if v EXi , set Xi+1 = Xi \ {v} with probability minO, l/A); 
(c) if v tJ- Xi and if adding v to Xi still gives an independent set, then put Xi+1 = 
Xi U {v} with probability minO, A); 
(d) otherwise, set Xi + 1 = Xi. 
We now follow a two-step approach. We first propose a move by choosing a ver-
tex v to add or delete. where each vertex is chosen with probability 1/ M; here M = 
I VI· This proposal is then accepted with probability minO, Jrv/Jrx ), where x is the cur-
rent state and y is the proposed state to which the chain will move. Here, Jry /lrx is A 
if the chain attempts to add a vertex and is 1/ A if the chain attempts to delete a vertex. 
This two-step approach is the hallmark of the Metropolis algorithm: each neighbor is 
selected with probability 1/ M, and then it is accepted with probability min(l, ny/n,). 
Using this two-step approach, we naturally obtain that the transition probability Pt.v is 
P~ " = -
mIn 1, ---'-
, 
1 
. ( 
Jry) 
", 
M 
Jrx 
so Lemma 10.8 applies. 
It is important that, in designing this Markov chain, we never needed to know B = 
Lx All, I. A graph with n vertices can have exponentially many independent sets, and 
calculating this sum directly would be too expensive computationally for many graphs. 
Our Markov chain gives the correct stationary distribution by using the ratios n" / Jrx , 
which are much easier to deal with. 
266 

10.5 EXERCISES 
10.5. Exercises 
Exercise 10.1: Formally prove Theorem 10.1. 
Exercise 10.2: Another method for approximating :T using Monte Carlo techniques is 
based on Buffon's needle experiment. Research and explain ButTon's needle experi-
ment, and further explain how it can be used to obtain an approximation for :T. 
Exercise 10.3: Show that the following alternati\"e definition is equiyalent to the defi-
nition of an FPRAS given in the chapter: Afit/ly polynomial rwzdomi:.ed appmximmion 
~'cheme (FPRA5) for a problem is a randomized algorithm for which. giyen an input x 
and any parameter 8 with 0 < 8 < 1. the algorithm outputs an (E. 1/4 )-approximation 
in time that is polynomial in 1/ E and the size of the input x. (Hint: To boost the prob-
ability of success from 3/4 to 1 -
(S. consider the median of several independent runs 
of the algorithm. Why is the median a better choice than the mean?) 
Exercise 10.4: Suppose we have a class of instances of the DNF satisfiability prob-
lem, each with a(l1) satisfying truth assignments for some polynomial a. Suppose we 
apply the nai've approach of sampling assignments and checking whether they satisfy 
the formula. Show that, after sampling 2
11
/ 2 assignments, the probability of finding 
even a single satisfying assignment for a given instance is exponentially small in n. 
Exercise 10.5: (a) Let 5 1,52, ... , 5m be subsets of a finite universe U. We know 151 I 
for 1 2: i 2: m. We wish to obtain an (8, 8)-approximation to the size of the set 
III 
We have available a procedure that can, in one step, choose an element uniformly at 
random from a set 5i . Also, given an element x E U, we can determine the number of 
"ets 51 for which x E 51. We call this number c(x). 
Define PI to be 
Pi = "Ill 
15.'1' 
L...J=I 
I 
The jth trial consists of the following steps. We choose a set 5 j • where the probability 
l)f each set 51 being chosen is PI, and then we choose an element xJ uniformly at ran-
Jom from 5j • In each trial the random choices are independent of all other trials. After 
r trials, we estimate 151 by 
Determine - as a function of m, 8, and <5 - the number of trials needed to obtain an 
1,(. <5)-approximation to 151. 
267 

THE MONTE CARLO METHOD 
(b) Explain how to use your results from part (a) to obtain an alternative approxi-
mation algorithm for counting the number of solutions to a DNF formula. 
Exercise 10.6: The problem of counting the number of solutions to a knapsack instance 
can be defined as follows: Given items with sizes aI, a2, ... , all > ° 
and an integer b > 
0, find the number of vectors (Xl, X2,"" XII) E {a, l}1I such that L;l=l aiXi :s b. The 
number b can be thought of as the size of a knapsack, and the Xi denote whether or 
not each item is put into the knapsack. Counting solutions corresponds to counting the 
number of different sets of items that can be placed in the knapsack without exceeding 
its capacity. 
(a) A naiVe way of counting the number of solutions to this problem is to repeatedly 
choose (Xl, X2, ... , Xn) E {a, l}n uniformly at random, and return the 21l times the 
fraction of samples that yield valid solutions. Argue why this is not a good strat-
egy in general; in particular, argue that it will work poorly when each ai is 1 and 
b= 01. 
(b) Consider a Markov chain Xo, Xl, ... on vectors (Xl,X2, ... ,Xn ) E {a, l}1l. Suppose 
Xi is (Xl, X2, ... ,XII)' At each step, the Markov chain chooses i E [1, n] uniformly 
at random. If Xi = 1, then Xj + l is obtained from Xj by setting Xi to 0. If Xi = 0. 
then Xj + l is obtained from Xi by setting Xi to 1 if doing so maintains the restriction 
L;l=l aixi :s b. Otherwise, Xj + l = Xi' 
Argue that this Markov chain has a uniform stationary distribution whenever 
L;l=l ai > b. Be sure to argue that the chain is irreducible and aperiodic. 
(c) Argue that, if we have an FPAUS for the knapsack problem, then we can derive an 
FPRAS for the problem. To set the problem up properly, assume without loss of 
generality that al :s a2 :s ... :s all' Let bo = ° 
and bi = L~=l ai. Let Q (b i ) be 
the set of vectors (Xl, X2, ... , XII) E {a, l}1I that satisfy L;l=l ai Xi :s bi . Let k be 
the smallest integer such that bk 2: b. Consider the equation 
IQ(b)1 = 
IQ(b)1 
x IQ(bk-dl x .. , x IQ(bdl x IQ(bo)l. 
IQ(bk-I)1 
IQ(bk- 2 )1 
IQ(bo)1 
You will need to argue that I Q (b i - d I / I Q (b i ) I is not too small. Specifically, argue 
that IQ(bi)1 :s (11 + 1)IQ(bi-dl. 
Exercise 10.7: An alternative definition for an s-uniform sample of Q is as follows: A 
sampling algorithm generates an s-uniform sample w if, for all X E Q, 
iFr (w = x) -
1/1 Q II 
-------- < s. 
l/IQI 
-
Show that an s-uniform sample under this definition yields an s-uniform sample as 
given in Definition 10.3 
Exercise 10.8: Let S = L~l i-2 = Jr2/6. Design a Markov chain based on the Me-
tropolis algorithm on the positive integers such that, in the stationary distribution, Jri = 
1/ S i 2. The neighbors of any integer i > 1 for your chain should be only i-I and i + 1, 
and the only neighbor of 1 should be the integer 2. 
268 

10.5 EXERCISES 
Exercise 10.9: Recall the Bubblesort algorithm of Exercise 2.22. Suppose we have n 
cards labeled 1 through n. The order of the cards X can be the state of a Markov chain. 
Let f( X) be the number of Bubblesort moves necessary to put the cards in increasing 
sorted order. Design a Markov chain based on the !'v1etropolis algorithm such that, in 
the stationary distribution, the probability of an order X is proportional to i) (x) for a 
given constant A > O. Pairs of states of the chain arc connected if they correspond to 
pairs of orderings that can be obtained by interchanging at most two cards. 
Exercise 10.10: A ~-coloring C of an undirected graph G = (V. E) is an assignment 
labeling each vertex with a numher. representing a color, from the set {I, 2 ..... ~ l. An 
edge (u, v) is improper if both [( and l' are assigned the same color. Let I(C) be the 
number of improper edges of a coloring C. Design a Markov chain based on the Me-
tropolis algorithm such that. in the stationary distribution, the probability of a coloring 
C is proportional to Ie /(C) for a given constant A > O. Pairs of states of the chain are 
connected if they correspond to pairs of colorings that differ in just one vertex. 
Exercise 10.11: In Section lOA.1 we constructed a Markov chain on the independent 
sets of a graph where, in the stationary distribution, Jrx = All, 1/ B. Here 1\ is the in-
dependent set corresponding to state x and B = Lx AII,I. Using a similar approach, 
construct a Markov chain on the independent sets of a graph excluding the empty set, 
where Jrx = 11\ 1/ B for a constant B. Because the chain excludes the empty set, you 
should first design a neighborhood structure that ensures the state space is connected. 
Exercise 10.12: The following generalization of the Metropolis algorithm is due to 
Hastings. Suppose that we have a Markov chain on a state space Q given by the tran-
sition matrix Q and that we want to construct a Markov chain on this state space with 
a stationary distribution ]f\ = b(x)/B, where for all x E Q, h(x) > 0 and B = 
Lxd2 b(x) is finite. Define a new Markov chain as follows. When XII = x, gener-
ate a random variable Y with Pr(Y = y) = Qr.,\' Notice that Y can be generated by 
simulating one step of the original Markov chain. Set XI/+l to Y with probability 
mm 
' 
, 
,1 , 
. (Jr", Q\,'X 
) 
]f\ Q,\,\ 
and otherwise set X Il+ l to XII' Argue that, if this chain is aperiodic and irreducible. 
then it is also time reversible and has a stationary distribution given by the :T \ . 
Exercise 10.13: Suppose we have a program that takes as input a number x on the real 
interval [0, I] and outputs f(x) for some bounded function f taking on \alues in the 
range [1. b]. We want to estimate 
Lo f(x)dx. 
Assume that we have a random number generator that can generate independent uni-
form random variables Xl, X 2, .... Show that 
269 

THE MONTE CARLO METHOD 
t f(X i ) 
m 
i=l 
gives an (8, (5)-approximation for the integral for a suitable value of m. 
10.6. An Exploratory Assignment on Minimum Spanning Trees 
Consider a complete, undirected graph with G) edges. Each edge has a weight, which 
is a real number chosen uniformly at random on [0,1]. 
Your goal is to estimate how the expected weight of the minimum spanning tree 
grows as a function of n for such graphs. This will require implementing a minimum 
spanning tree algorithm as well as procedures that generate the appropriate random 
graphs. (You should check to see what sorts of random number generators are avail-
able on your system and determine how to seed them - say, with a value from the 
machine's clock.) 
Depending on the algorithm you use and your implementation, you may find that 
your program uses too much memory when n is large. To reduce memory when n is 
large, we suggest the following approach. In this setting, the minimum spanning tree 
is extremely unlikely to use any edge of weight greater than k(n) for some function 
k(n). We can first estimate k(n) by using repeated runs for small values of n and then 
throwaway edges of weight larger than k( n) when n is large. If you use this approach, 
be sure to explain why throwing away edges in this manner will not lead to a situation 
where the program finds a spanning tree that is not actually minimal. 
Run your program for n = 16, 32, 64, 128, 256, 512, lO24, 2048, 4096, 8192, and 
larger values, if your program runs fast enough. Run your program at least five times 
for each value of n and take the average. (Make sure you re-seed the random number 
generator appropriately!) You should present a table listing the average tree size for the 
values of n that your program runs successfully. What seems to be happening to the 
average size of the minimum spanning tree as n grows? 
In addition, you should write one or two pages discussing your experiments in more 
depth. The discussion should reflect what you have learned from this assignment and 
might address the following topics. 
• What minimum spanning tree algorithm did you use, and why? 
• What is the running time of your algorithm? 
• If you chose to throwaway edges, how did you determine k(n), and how effective 
was this approach? 
• Can you give a rough explanation for your results? (The limiting behavior as n 
grows large can be proven rigorously, but it is very difficult; you need not attempt 
to prove any exact result.) 
• Did you have any interesting experiences with the random number generator? Do 
you trust it? 
270 

CHAPTER ELEVEN * 
Coupling of Markov Chains 
In our study of discrete time Markov chains in Chapter 7, we founu that er~l)uic \lark.t)\ 
chains converge to a stationary distribution. However, we did not ueterI11Ine ht)\\ (jllidly 
they converge. which is important in a number of algorithmic applicatiun". "uL'h a" "al11-
pIing using the Markov chain Monte Carlo technique. In thi~ l'hapter. \\ e intruuuce 
the concept of coupling, a powerful method for bounding the rate of L'UI1\ er~elk'e uf 
Markov chains. We demonstrate the coupling method in ~everal applil..'atiun". IIk'lud-
ing card-shuffling problems, random walks, and Markov chain Monte C .. lrll) ".lmplin~ 
of independent sets and vertex coloring. 
11.1. Variation Distance and Mixing Time 
Consider the following method for shuffling 11 cards. At each step. a L'ard i" dlO"en in-
dependently and uniformly at random and put on the top of the Uel'k., \\'e can think 
of the shuffling process as a Markov chain, where the state is the current l)rdcr l)f the 
cards. You can check that the Markov chain is finite, irreducible. and .. lperiuuic. anu 
hence it has a stationary distribution. 
Let x be a state of the chain, and let N(x) be the set of states that can reach x in nne 
~tep. Here I N( x) I = 11, since the top card in x could have been in II uifferent place" 
in the previous step. If Jr\ is the probability associated with ~tate ,\ in the "tationar~ 
distribution, then for any state x we have 
Jr\ 
L Jr\" 
n 
\'E.V( \l 
The uniform distribution satisfies these equations, and hence the unique "tationary dis-
tribution is uniform over all possible permutations. 
We know that the stationary distribution is the limiting di~tribution of the Markov 
chain as the number of steps grows to infinity. If we could run the chain "forever", 
then in the limit we would obtain a state that was uniformly di~tributed. In practice, 
271 

COUPLING OF MARKOV CHAINS 
4/10 
r-----. 
I 
I 
I 
I 
I 
I 
r-----
I 
I 
I 
- + - -:- - - - -{ --
I 
I 
I 
____ -' 
L __________ I 
3/10 
2/10 
1/10 
o 
2 
3 
4 
x 
Figure 11.1: Example of variation distance. The areas shaded by upward diagonal lines correspond 
to values x where DI (x) < D2 (x); the areas shaded by downward diagonal lines correspond to val-
ues x where DI(x) > D 2(x). The total area shaded by upward diagonal lines must equal the total 
area shaded by downward diagonal lines, and the variation distance equals one of these two areas. 
we run the chain for a finite number of steps. If we want to use this Markov chain to 
shuffle the deck, how many steps are necessary before we obtain a shuffle that is clos~ 
to uniformly distributed? 
To quantify what we mean by "close to uniform", we must introduce a distance 
measure. 
Definition 11.1: The variation distance between two distributions D, and D2 on (f 
countable state space S is given by 
1 
liD, - D211 = 2: LID,(x) - D2(x)l. 
XES 
A pictorial example of the variation distance is given in Figure 11.1. 
The factor 1/2 in the definition of variation distance guarantees that the variation dis-
tance is between 0 and l. It also allows the following useful alternative characterization. 
Lemma 11.1: Foran), A ~ S, let Di(A) = LXEA Di(x)jori = 1,2. Then 
A careful examination of Figure 11.1 helps make the proof of this lemma transparent. 
Proof: Let S+ ~ S be the set of states such that D,(x) ~ D2(X), and let S- ~ S b~ 
the set of states such that D2 (x) > D, (x). 
Clearly, 
and 
But since D,(S) = D2(S) = 1, we have 
D,(S+) + D,(S-) = D2(S+) + D2(S-) = 1, 
272 

which implies that 
Hence 
Finally, since 
we have 
11.1 VARIATION DISTANCE AND MIXING TIME 
maxID,(A) -
D=(,4) = 
D; -
f)= 
AS;S 
completing the proof. 
• 
As an application of Lemma 11.1, suppose that \\e run our "huftling \larkm chain until 
the variation distance between the distribution of the chain and the uniform distribution 
is less than c. This is a strong notion of close to uniform. hecau"e e\ery permutation of 
the cards must have probability at most 1 /11 ~ ~ f-. 1 n fact the hound on the \ariation dis-
tance gives an even stronger statement: For any "uh"et .4 ;; S. the prohability that the 
final permutation is from the set A is at most .1 (.4 ) -
f-. For example ..... uppose .... ome-
one is trying to make the top card in the deck an ace. If the \ariation di"tance from the 
distribution to the uniform distribution i .... Ie"" than E. \\e L'an "afel~ "ay that prohahil-
ity that an ace is the first card in the deck i" at mo"t ,l greater than if \\ e had a perfect 
shuffle. 
As another example, suppose we take a 52-L'ard deck and ...;hume all the cards - but 
leave the ace of spades on top. In thi .... ca"e. the \ariation distance between the result-
ing distribution D[ and the uniform distrihution D= could be bounded by considering 
the set B of states where the ace of space" i...; on the top of the deck: 
The definition of variation distance coincides with the definition of an c-uniform 
sample (given in Definition 10.3) .. -\ sampling algorithm returns an c-uniform sample 
on Q jf and only if the variation distance between its output distribution D and the uni-
form distribution V satisfies 
liD - VII:::; c. 
Bounding the variation distance between the uniform distribution and the distribution 
of the state of a Markov chain after some number of steps can therefore be a useful 
way of proving the existence of efficient c-uniform samplers, which (as we showed in 
Chapter 10) can in turn lead to efficient approximate counting algorithms. 
We now consider how to bound this variation distance after t steps. In what follows, 
we assume that the Markov chains under consideration are ergodic discrete space and 
discrete time chains with well-defined stationary distributions. The following defini-
tions will be useful. 
273 

COUPLING OF MARKOV CHAINS 
Definition 11.2: Let if be the stationar.v distribution of a Markov chain with state 
space S. Let p~ represent the distribution of the state of the chain starting at state x 
after t steps. We define 
~ xC t) = II p~ - if II ; 
~(t) = max ~At). 
XES 
That is, ~At) is the variation distance between the stationary distribution and p'~, and 
~(t) is the maximum o.f these values over all states x. 
We a/so define 
rxCe) = min{t : ~xCt) :'S c}; 
r (e) = max rxC e) . 
XES 
That is, rAe) is the fIrst step t at which the variation distance between p_~ and the sta-
tionary distribution is less than e, and r (e) is the maximum of these values over ([If 
states x. 
When r (e) is considered as a function of e, it is generally called the mixing time of 
the Markov chain. A chain is called rapid!.v mixing if r(e) is polynomial in log(l/E'l 
and the size of the problem. The size of the problem depends on the context; in th~ 
shuffling example, the size would be the number of cards. 
11.2. Coupling 
Coupling of Markov chains is a general technique for bounding the mixing time of a 
Markov chain. 
Definition 11.3: A coupling of a Markov chain M, with state space S is a Markov chain 
Z, = (X" Y,) 011 the state space S x S such that: 
Pr(X r+! = x' I Z, = (x. y)) = Pr(M,+! = Xl I M, = x); 
Pr(Y'~1 = .\" 
I Z, = (x. y)) = Pr(M,+! = yl I M, = y). 
That is, a coupling consists of two copies of the Markov chain M running simultane-
ously. These two copies are not literal copies; the two chains are not necessarily in th~ 
same state, nor do they necessarily make the same move. Instead, we mean that each 
copy behaves exactly like the original Markov chain in terms of its transition probabil-
ities. One obvious way to obtain a coupling is simply to take two independent runs of 
the Markov chain. As we shall see, such a coupling is generally not very useful for our 
purposes. 
Instead, we are interested in couplings that (a) bring the two copies of the chain to 
the same state and then (b) keep them in the same state by having the two chains mak~ 
identical moves once they are in the same state. When the two copies of the chain reach 
the same state, they are said to have coupled. The following lemma motivates why we 
seek couplings that couple. 
274 

11.2 COUPLING 
Lemmall.2[CouplingLemma]: LetZt = (Xt. Yt)beacoupfingJoraMarkm'chain 
M Oil a state space S. Suppose that there exists (f T such that, for eVel~\' x. yES. 
Pr( X T =j=. Y T 
I X () = x, Yo = Y) :s e. 
Then 
r(e) :s T. 
That is, for any initial state, the variation distance between the distri/){(tiol/ (~r the st(f{e 
of the chain after T steps and the stationary distribution is (f{ most ,c. 
Proof: Consider the coupling when Yo is chos~n according to the stationary distribu-
tion and Xo takes on any arbitrary value. For th~ gi\~n T and f and for any A ~ S, 
Pr ( X TEA) ~ Pr ( ( X r = Y r) " ( r r E .--\ ) ) 
= 1 - Pr ( (XI ~ r I ) ~ ( r r E .--\ ) ) 
~ (1 - Pr ( Y T tt .--\ )) - Pr ( X I 
:::i= r I ) 
~ Pr ( Yr E .--\ ) -
f-
=JT(A)-E. 
Here the second line follows from the union bound. For th~ third lin~. \\~ used the 
fact that Pr( X T =j=. Yr) :s E for any initial stat~s XII and L,: in particular. this holds 
when Yo is chosen according to the stationary distribution. For th~ la..;t lin~. \\~ us~d that 
Pr(YT E A) = JT(A). since Yr isalsodistributedaccordingtoth~ ..;tationarydi..;tribution. 
The same argument for the set S -
.--\ shows that Pr( Xl E .--\) > :7 (S -
.--\) -
E. or 
Pr ( X TEA) :s JT ( A) + e. 
It follows that 
maxlp;(A) -
JT(A)I :s f. 
r. A 
so by Lemma 11.1 the variation distance from the stationary distribution after the chain 
runs for T steps is bounded above bye. 
• 
11.2.1. Example: Shuffling Cards 
To apply the coupling lemma effectively to the card-shuffling Markov chain. v.:e must 
choose an appropriate coupling. Given two copies Xt and Yt of the chain in difkr~nt 
states, one possibility for the coupling is to choose a position j uniformly at random 
from I to 11 and simultaneously move to the top the jth card from the top in both chains. 
This is a valid coupling, because each chain individually acts as the original shuffling 
Markov chain. Although this coupling is natural, it does not appear iml11~diatdy use-
ful. Since the chains start in different states, the jth cards from the top in th~ two chains 
will usually be ditferent. Moving these two different cards to the top does not seem to 
bring the two copies of the chain toward the same state. 
A more useful coupling is to choose a position j uniformly at random from 1 to n 
and then obtain X t+ 1 from X t by moving the jth card to the top. Denote the value of 
this card by C. To obtain Yt+ 1 from Yt. move the card with value C to the top. The 
coupling is again valid. because in both chains the probability a specific card is moved 
275 

COUPLING OF MARKOV CHAINS 
to the top at each step is l/n. With this coupling, it is easy to see by induction that, 
once a card C is moved to the top, it is always in the same position in both copies of 
the chain. Hence, the two copies are sure to become coupled once every card has been 
moved to the top at least once. 
Now our coupling problem for the shuffling Markov chain looks like a coupon col-
lector's problem; to bound the number of steps until the chains couple, we simply bound 
how many times cards must be chosen uniformly at random before every card is cho-
sen at least once. We know that when the Markov chain runs for n In n + en steps, the 
probability that a specific card has not been moved to the top at least once is at most 
( 
1 )11111 II+CIl 
e-c 
1 - -
::s e-(lnll+c) = -, 
n 
n 
and thus (by the union bound) the probability that any card has not been moved to the 
top at least once is at most e- c. Hence, after only n In n + n In(i/E') = n In(n/E') steps, 
the probability that the chains have not coupled is at most E'. The coupling lemma al-
lows us to conclude that the variation distance between the uniform distribution and 
the distri bution of the state of the chain after n In (n / E') steps is bounded above by E'. 
11.2.2. Example: Random Walks on the Hypercube 
Recall from Section 4.5.1 that an n-dimensional hypercube, or n-cube, consists of N = 
211 nodes numbered from 0 to N -
1. Let.t = (.r I, ... , XII) be the binary representation 
of x. Nodes x and yare connected by an edge if and only if x and }' differ in exactly 
one bit. 
We consider the following Markov chain defined on the n-cube. At each step, choose 
a coordinate i uniformly at random from lL 11]. The new state Xl is obtained from the 
current state x by keeping all coordinates of x the same, except possibly for Xi. The co-
ordinate Xi is set to 0 with probability 1/2 and to 1 with probability 1/2. This Markov 
chain is exactly the random walk on the hypercube, except that with probability 1/2 
the chain stays at the same vertex instead of moving to a new one, which removes the 
potential problem of periodicity. It follows easily that the stationary distribution of the 
chain is uniform over the vertices of the hypercube. 
We bound the mixing time r (E') of this Markov chain by using the obvious coup-
ling between two copies XI and YI of the Markov chain: at each step, we have both 
chains make the same move. With this coupling, the two copies of the chain will surely 
agree on the ith coordinate, once the ith coordinate has been chosen for a move of the 
Markov chain. Hence the chains will have coupled after all n coordinates have each 
been chosen at least once. 
The mixing time can therefore be bounded by bounding the number of steps until 
each coordinate has been chosen at least once by the Markov chain. This again reduces 
to the coupon collector's problem, just as in the case of the shuffling chain. By the 
same argument, the probability is less than E' that after n In(nE'-I) steps the chains have 
not coupled, and hence by the coupling lemma the mixing time satisfies 
r(E') ::s n In(nE'-I). 
276 

11.2 COUPLING 
11.2.3. Example: Independent Sets of Fixed Size 
We consider a Markov chain whose states are all independent sets of size exactly k in 
a graph G = (V, E). Because we restrict oursel\es to independent sets of a fixed size. 
we need a different Markov chain than the chain for all independent sets developed in 
Section 10.4. A move is made from the independ~nt set XI by choosing a vertex v E XI 
uniformly at random and a vertex WE V uniformly at random. The move mev, W, XI) 
can be described as follows: if W tf- XI and (X, - {r}) 0 {w} is an independent set, 
then XI+ 1 = (XI -
{v}) U {w}: otherwise. XI_ 1 = X r • L~t II be the number of vertices 
in the graph and let ~ be the maximum degr~~ of any \~rtex. We show here that this 
chain is rapidly mixing whenever k :s II /( 3~ + 3). \Ye kav~ the task of showing that 
the Markov chain is ergodic and has a uniform stationary distribution as Exercise 11.11, 
and assume this in the following argument. 
We consider a coupling on ZI = (X" 
}~). Our coupling will require an arbitrary 
perfect matching M between the vertices of X: - rand y. - X: at ~ach step: for ex-
ample. we may label the vertices 1 to II and match th~ ~kIl1~nts of X: -
Y, in sorted 
order via a one-to-one mapping with the elements of }'r -
X, in sort~d ord~r. For our 
coupling, we first choose a transition for the chain X, by choosing rEX,. and /1' E V 
uniformly at random and then perform the mO\e III (r. U'. X,), Ckarly. th~ copy of the 
chain XI follows the original Markov chain faithfully as required by Definition 11.3. 
For the transition of Y,. if v E YI then we use the same pair of vertices ~' and U' and 
perform the move m( v. w. YI ): if v tf- YI. we perform the move III (M (r). U'. Y, ) (where 
H (v) denotes the vertex matched to v). The copy of the chain YI also follows the origi-
nal Markov chain faithfully, since each pair of vertices with v E YI and W E V is chosen 
with probability l/kn. 
An alternative way of establishing the coupling is as follows. We again choose v E 
X, and 111 E V uniformly at random and then perform the move m(v. w, XI) in the chain 
X" If v E YI• we perform the move m ( v. W, YI ) in the chai n YI; otherwi se. we choose 
uniformly at random a vertex v' E YI -
XI and perform the move mev', w, YI) in the 
chain YI' We see in Exercise 11.10 that this also satisfies Definition 11.3. 
Let cil = I XI -
YII measure the difference between the two independent sets after t 
"teps. Clearly cil can change by at most 1 at each step. We show that cil is more likely 
to decrease than increase. and we use this fact to establish an upper bound on the prob-
ahility that d l > 0 for sufficiently large t. 
Suppose that dl > O. In order for cil+ 1 = dl + I. it must be that at time I the ver-
t~x v is chosen from XI n YI' and 11' is chosen so that there is a transition in exactly 
one of the chains. Thus. 11' must be either a vertex or a neighbor of a vertex in the set 
\ X, -
YI) U (YI - XI)' It follows that 
Similarly. in order for d l + 1 = d l -
I. it is sufficient that at time I \ve have t' tf- YI and 
/I' neither a vertex nor a neighbor of a vertex in the set XI U YI -
{t'. v'}. Note that 
XI U YII = k + d l • Therefore. 
277 

COUPLING OF MARKOV CHAINS 
d t n - (k + d t -
2)(~ + 1) 
Pr (d t+ I = d t -
1 I d t > 0) ~ k 
n 
. 
We thus have, for d t > 0, 
E[dt+1 I dt] = dt + Pr(dt+1 = dt + 1) - Pr(dt+1 = dt -
1) 
k - dt 2dt(~ + 1) 
dt n - (k + dt -
2)(~ + 1) 
< d + --
- - ---------
-
t 
k 
n 
k 
n 
-
d t 1 - ----------
_ 
( 
n - (3k - d t -
2)( ~ + 1)) 
kn 
< d (1 _ n - (3k -
3)(~ + 1)). 
-
t 
kn 
Once dt = 0, the two chains follow the same path and so E[dt+1 I dt = 0] = O. 
Using the conditional expectation equality, we have 
( 
(n - 3k + 3)(~ + 1)) 
E[dt+IJ = E[E[dt+1 I dtn :s E[dt] 1 -
kn 
. 
By induction, we find that 
( 
n - (3 k - 3)( ~ + 1))t 
E[dt ] :s do 1 -
kn 
. 
Since do :s k and since d t is a nonnegative integer, it follows that 
( 
n - (3k -
3)(~ + 1))t 
Pr(dt ~ 1) :s E[dt] :s k 1 -
kn 
:s e-t(Il-(3k-3)(lHl))/kn. 
A consequence of this result is that the variation distance converges to zero whenever 
k :s 11 /(3~ + 3), and in this case 
kn InE- 1 
r(E) < 
. 
-
n-(3k-3)(~+1) 
We thus find that r(E) is polynomial in nand InO/E), implying that the chain is rapidl) 
mixing, whenever k :s 11/(3~ + 3). 
We can actually improve upon this result. In Exercise 1l.12 we use a slightly more 
sophisticated coupling to obtain a bound that holds for any k :s n/2(~ + 1). 
11.3. Application: Variation Distance Is Nonincreasing 
We know that an ergodic Markov chain eventually converges to its stationary distri-
bution. In fact, the variation distance between the state of a Markov chain and its 
stationary distribution is nonincreasing in time. To show this, we start with an interest-
ing lemma that gives another useful property of the variation distance. 
Lemma 11.3: Given distributions ax and ay on a state space S, let Z = (X, Y) be (/ 
random variable on S x S, where X is distributed according to ax and Y is distributed 
according to ay. Then 
278 

11.3 APPLICATION: VARIATION DISTANCE IS NONINCREASING 
Pr(X =j=. Y) ~ Ilax - ayll. 
(11.1) 
Moreover, there exists a joint distribution Z = (X. Y). ~vhere X is distributed accord-
ing to ax and Y is distributed according to ar .for ~rhich equality holds. 
Again, examining a specific example (such as in Figure 11.1) helps us understand the 
following proof. 
Proof: For each s E S, we have 
Pr(X = Y = x) :s min(Pr(X = .n.Pr(Y = x)). 
Hence 
Pr(X = Y):s Lmin(Pr(X = xI.Pr(}' = x)). 
rES 
and therefore 
Pr(X =j=. Y) ~ 1- Lmin(Pr(X = XI. Prj}' = .\)1 
XES 
= L (Pr( X = x) - minI Pr( X = x I. Prl }" = x) I), 
rES 
Hence we are done if we can show 
Ilax -
ay II = L (Pr( X = x) - mint Pr( X = x). Pr(}" = x I)), 
(11.2) 
XES 
But Pr(X = x) - min(Pr(X = x). Pre Y = x)) = 0 when a\ (x) < a} I x I. and wh~n 
ax(x) ~ ay(x) it is 
Pr (X = x) - Pr (Y = x) = a x ( x) -
a y( x ) . 
If we let S+ be the set of all states for which ax (x) ~ ay (x). then th~ right-hand sid~ 
of Eqn. (11.2) is equal to ax(S+) - ay(S+), which equals Ilax - ar from th~ argu-
ment in Lemma 11.1. This gives the first part of the lemma. 
Equality holds in Eqn. (11.1) if we take a joint distribution wher~ X = }" as mllch 
as possible. Specifically. let m(x) = min(Pr( X = x). Pre Y = .\')). If L 1 1/1 (x) = I. 
then X and Y have the same distribution and we are done. Otherwise. kt Z = IX. Y) 
be defined by 
{ 
m(x) 
Pr (X = x, Y = y) = 
(a x (x) - m ( x ) )( a y ( Y) -
III ( y ) ) 
1 - L;: m(z) 
if x = y: 
otherwise. 
The idea behind this choice of Z is to first match X and Y as much as possible and then 
force X and Y to behave independently if they do not match. 
For this choice of Z, 
Pr (X = Y) = L 1/1 ( x) = 1 -
II a x -
a y II. 
x 
279 

COUPLING OF MARKOV CHAINS 
It remains to show that, for this choice of Z, Pr(X = x) = ax(x); the same argu-
ment will hold for Prey = y). If m(x) = ax(x) then Pr(X = x, Y = x) = m(x) and 
Pr(X = x, Y = y) = 0 when x =j=. y, so Pr(X = x) = ax(x). If m(x) = ay(x), then 
Pr (X = .X") = L Pr (X = x, Y = y) 
'" (ax(x) - m(x))(ay(y) - m(y)) 
= m(x) + ~ 
\'~.' 
1 - L; m(z) 
( a x (x) -
m (x)) L \4'( a y ( y) -
m ( y) ) 
=m(x)+ 
.. 
1 - L;m(z) 
(ax(x) -m(x))(I -ay(x) - (L;m(z) -m(x))) 
=m(x)+-------------------------------------
1 - L;- m(z) 
= m(x) + (ax (x) - m(x)) 
= ax(x), 
completing the proof. 
• 
Recall that ~(t) = max, ~xCt), where ~x(t) is the variation distance between the sta-
tionary distribution and the distribution of the state of the Markov chain after t steps 
when starting at state x. Using Lemma 11.3, we can prove that ~(t) is nonincreasing 
over time. 
Theorem 11.4: For any ergodic Markm' chain M r, ~(T + 1) :s ~(T). 
Proof: Let x be any given state, and let y be a state chosen from the stationary distri-
bution. Then 
~,(T) = lip;· - p;ll. 
Indeed, if X T is distributed according to P.~ and if Y T is distributed according to p.;, then 
by Lemma 1l.3 there exists a random variable ZT = (XT, Yd with Pr(XT =j=. Yd = 
~x(T). From this state Z T, consider anyone-step coupling for the Markov chain that 
takes ZT = (XT, Yd to ZT+l = (XT+ 1, YT+ 1) in such a way that, whenever XT = 
YT, the coupling makes the same move, so that XT+ 1 = YT+ 1• Now X T+ 1 is distributed 
according to p'~+l and YT+ 1 is distributed according to p;+l, which is the stationary 
distribution. Hence, by Lemma 11.3, 
~x(T) = Pr(XT =j=. Yd 
~ Pr ( X T + I =j=. Y T + d 
~ 11p';+1 - p;+lll 
= ~xCT + 1). 
The second line follows from the first because the one-step coupling assures X T + I = 
Y T + I whenever X T 
Y T. The result follows since the foregoing relations hold for 
every state x. 
• 
280 

11.4 GEOMETRIC CONVERGENCE 
11.4. Geometric Convergence 
The following general result, derived from a trivial coupling, is useful for bounding the 
mixing time of some Markov chains. 
Theorem 11.5: Let P be the transition matrixfor afinite, irreducible. ([IJeriodic .\1([1"-
kov chain. Let mj be the smallest entr,V in the jth column ql'the IIl(fll"i.\. ([lief ler III 
Lj mj. Then, for all x and t, 
Proof: If the minimum entry in column j is IIIj' then in one ~tep the chain readle~ state 
j with probability at least mj from every state. Hence \\e can design a L'oupling \\here 
the two copies of the chain both move to state j together \\ith prohahility at least III i in 
every step. Since this holds for all j, at each step the t\\O chain .... can be made to couple 
with probability at least III. Hence the probability they hel\e not ct)llpled after III steps 
is at most (1 - m)t, yielding the theorem \'ia the coupling lemma. 
• 
Theorem 11.5 is not immediately helpful if there i .... a zero entry in each column. in 
which case m = O. In Exercise 11.6. we consider hO\\ it make it lheful for an~ tinite. 
irreducible, aperiodic Markov chain. Theorem 11.5 ~hl)\\~ that. under \er~ general 
conditions, Markov chains converge quickly to their stationary di .... tributiolb. \\ ith the 
\'ariation distance converging geometrically in the number of steps. 
A more general related result is the following. Suppose that \\e can obtain an upper 
bound on T(e) for some constant e < 1/2. For example. such a bound might be found 
by a coupling. This is sufficient to bootstrap a bound for T (E) for any E > O. 
Theorem 11.6: Let P be the transition matrix for a finite, irreducible, aperiodic Mar-
km' chain M t vvith T(C) :'S T for some c < 1/2. Then, for this Markm' elwin, T(E) :'S 
-In E/ln(2cn T. 
Proof: Consider any two initial states Xo = x and Yo = y. By the definition of T (C). 
\\'e have II p~' -
IT II :'S c and II p; - IT II :'S c. It follows that lip;' - P; II :'S :2 C and hence. 
by Lemma 11.3, there exists a random variable Zr.r,,r = (XT' Yd with X T distributed 
according to p; and YT distributed according to PIT such that Pr(XT -# Yrl :'S 2e. 
Now consider the Markov chain M: given by the transition matrix pT. which cor-
responds to a chain that takes T steps of Mt for each of its steps: the Z r. \ \ gi\'e a 
,,:oupling for this new chain. That is, given two copies of the chain /v( in the paired 
.... tate (x, y), we can let the next paired state be given by the distribution Z J\ \ • which 
guarantees that the probability the two states have not coupled in one step is at most 2e. 
The probability that this coupling of the chain M: has not coupled O\'er k steps is then 
,It most (2c)" by induction. By the coupling lemma, M/ is within \ariation distance E 
\)1' its stationary distribution after k steps if 
281 

COUPLING OF MARKOV CHAINS 
It follows that, after at most lIn e:/ln(2c)l steps, M: is within variation distance e: of 
its stationary distribution. But M: and M t have the same stationary distribution, and 
each step of M/ corresponds to T steps of Mt • Therefore, 
r 
In e: l 
r(e:) < --
T 
-
In (2c) 
for the Markov chain M t • 
• 
11.5. Application: Approximately Sampling Proper Colorings 
A vertex coloring of a graph gives each vertex v a color from a set C, which we can 
assume without loss of generality is the set {I, 2, ... , c}. In a proper coloring, the two 
endpoints of every edge are colored by two different colors. Any graph with maximum 
degree ~ can be colored properly with ~ + 1 colors by the following procedure: choose 
an arbitrary ordering of the vertices, and color them one at a time, labeling each vertex 
with a color not already used by any of its neighbors. 
Here we are interested in sampling almost uniformly at random a proper coloring 
of a graph. We present a Markov chain Monte Carlo (MCMC) process that generates 
such a sample and then use a coupling technique to show that it is rapidly mixing. In 
the terminology of Chapter 10, this gives an FPAUS for proper colorings. Applying the 
general reduction from approximate counting to almost uniform sampling, as in The-
orem 10.5, we can use the FPAUS for sampling proper colorings to obtain an FPRAS 
for the number of proper colorings. The details of this reduction are left as part of 
Exercise 11.15. 
To begin, we present a straightforward coupling that allows us to approximately 
sample colorings efficiently when there are c > 4~ + 1 colors. We then show how to 
improve the coupling to reduce the number of colors necessary to 2~ + 1. 
Our Markov chain on proper colorings is the simplest one possible. At each step. 
choose a vertex v uniformly at random and a color .e uniformly at random. Recolor 
vertex v with color t if the new coloring is proper (that is, v does not have a neighbor 
colored -€), and otherwise let the state of the chain be unchanged. This finite MarkO\ 
chain is aperiodic because it has nonzero probability of staying in the same state. When 
c :::: ~ + 2, it is also irreducible. To see how from any state X we can reach any other 
state Y, consider an arbitrary ordering of the vertices. Recolor the vertices in X to match 
Y in this order. If there is a conflict at any step, it must arise because a vertex v that 
needs to be colored is blocked by some other vertex Vi later in the ordering. But t" 
can be recolored to some other nonconflicting color, since c :::: ~ + 2, allowing thl? 
process to continue. Hence, when c :::: ~ + 2, the Markov chain has a stationary dis-
tribution. The fact that this stationary distribution is uniform over all proper colorings 
can be verified by applying Lemma 10.7. 
When there are 4~ + 1 colors. we use a trivial coupling on the pair of chains (X t , ~ ): 
choose the same vertex and color on both chains at each step. 
282 

11.5 APPLICATION: APPROXIMATELY SAMPLING PROPER COLORINGS 
Theorem 11.7: For any graph with n vertices alld llI(v,:imullI degree ~, the mixing time 
of the graph-coloring Markov chain satisfies 
I 
I1C 
(lI)l 
Tee:) :::: 
In -
. 
c -
4~ 
f:' 
provided that c ~ 4~ + 1. 
Proof: Let D t be the set of vertices that have difkrent colors in the two chains at time 
t, and let d t = 1 D t I. At each step in which d l > O. either d l remains at the same value 
or d t increases or decreases by at most 1. We show that ell is actually more likely to de-
crease than increase; then we use this fact to bound the probability that dt is nonzero 
for sufficiently large t. 
Consider any vertex v that is colored differently in the two chains. Since the degree 
of v is at most ~, there are at least c -
2~ colors that do not appear on the neighbors 
of v in either of the two chains. If the vertex is recolored to one of these c -
2~ colors, 
it will have the same color in both chains. Hence 
II 
C 
Now consider any vertex v that is colored the same in both chain ..... For t' to be col-
ored differently at the next step. it must ha\e some neighbor U' that i .... differently colored 
in the two chains; in that case. it is possible that trying to recolor r lhing a color that 
the neighbor w has in one of the two chains will recolor the \erlex r in one chain but 
not the other. Every vertex colored differently in the t\\'O chaim can affect at I11Cht ~ 
neighbors in this way. Hence. when d t > O. 
We find that 
~dt 2 
Pr(dt+ I = dt + 1 1 dt > 0) :::: -
- . 
11 
C 
E [d t+ lid t] = d t + Pr (d t+ I = d t + 1) - Pr (dt + I = d I 
-
I) 
~dt 2 
dt c -
2~ 
::::dt +--------
n c 
n 
c 
( 
c -
4~) 
.:::; dt 1 -
, 
nc 
which also holds if d t = O. 
Using the conditional expectation equality, we have 
( 
c -
4~) 
E[dt+IJ = E[E[dt+1 
1 dt]] :::: E[dt] 1-
. 
/1C 
By induction, we find 
( 
C -
4~)t 
E[dt ] .:::; do I -
. 
nc 
Since do .:::; n and since d t is a nonnegative integer, it follows that 
283 

COUPLING OF MARKOV CHAINS 
Pr(dt ~ 1) :::; E[dt] :::; 11 (1 __ 
C_-_4_~_)t 
HC 
Hence the variation distance is at most E after 
t = I 
I1C 
In(~)l 
I C -
4~ 
E 
steps. 
• 
Assuming that each step of the Markov chain can be accomplished efficiently in time 
that is polynomial in n, Theorem 11.7 gives an FPAUS for proper colorings. 
Theorem 11.7 is rather wasteful. For example, when bounding the probability that 
d t decreases, we used the loose bound c -
2~. The number of colors that decrease 
dt could be much higher if some of the vertices around v have the same color in both 
chains. By being a bit more careful and slightly more clever with the coupling, we can 
improve Theorem 11.7 to hold for any c ~ 2~ + 1. 
Theorem 11.8: Given an n-vertex graph 'rvith maximum degree ~, the mixing time Of 
the graph-coloring Markov chain satisfies 
T (E) :::; 
In - , 
I
n(c-~) (n)l 
c -
2~ 
E 
provided that c ~ 2~ + l. 
Proof: As before, let Dt be the set of vertices that have different colors in the two 
chains at time t, with I Dt I = dt. Let At be the set of vertices that have the same color 
in the two chains at time t. For a vertex v in At. let d'(v) be the number of vertices ad-
jacent to v that are in Dt : similarly, for a vertex w in Dt , let d' (w) be the number of 
vertices adjacent to w that are in At. Note that 
L d'(t') = L d'(11'), 
since the two sums both count the number of edges connecting vertices in At to vertice" 
in D t . Denote this summation by III'. 
Consider the following coupling: if a vertex v E D t is chosen to be recolored, \\'e 
simply choose the same color in both chains. That is, when v is in D t , we are using 
the same coupling we used before. The vertex v will have the same color whenever the 
color chosen is different from any color on any of the neighbors of v in both copies of 
the chain. There are c -
2~ + d'(t') such colors: notice that this is a tighter bound than 
we used in the proof of Theorem 11.7. Hence the probability that dt+ I = dt -
I when 
d t > 0 is at least 
I ~ 
c-2~+d'(v) 
I 
, 
- L 
= -((c -
2~)dt + III ). 
n 
c 
cn 
I'ED{ 
Assume now that the vertex to be recolored is v E At. In this case we change the 
coupling slightly. Recall that, in the previous coupling, recoloring a vertex v E ACt) re-
sults in v becoming differently colored in the two chains if the randomly chosen color 
284 

11.5 APPLICATION: APPROXIMATELY SAMPLING PROPER COLORINGS 
(a) 
Xt+1 
Yt+1 
(b) 
Xt+1 
Yt+1 
Figure 11.2: (a), original coupling: (b), improved coupling. In the original coupling of part (a), 
the gray vertex has the same color in both chains and has a neighbor \\ith different colors in the two 
chains, one black and one white. If an attempt is made to recolor the gra: \ erte.\. black, then the 111me 
will succeed in one chain but not the other. increasing el!, Similarl:. if an attempt i" made to recolor 
the gray vertex white, then the mO\e will succeed in one chain but not the other. gi\ing a -;el'ond mo\e 
that increases eli' In the improwd coupling of part (h I. if the gra: \erte.\. i" recolort'd \\ hite in X; then 
the gray vertex is recolored black in Yr and \ice \ersa, gi\ing just ont' mow that increase" c/', 
appears on a neighbor of v in one chain but not the other. For example: if v is colored 
green, and a neighbor w is colored red in one chain and blue in the other, and no other 
neighbor of v is colored red or blue in either chain, then attempting to color v either 
red or blue will cause v to be recolored in one chain but not the other. Hence there are 
two potential choices for v's color that increase dt . 
In this specific case where just one vertex w neighboring v has different colors in 
the two chains, we could improve the coupling as follows: when we try to recolor v 
blue in the first chain, we try to recolor it red in the second chain; and when we try to 
recolor it red in the first chain, we try to recolor it blue in the second chain. Now v 
either changes color in both chains or stays the same in both chains. By changing the 
coupling, we have collapsed two potentially bad moves that increase d t into just one 
bad move. See Figure 11.2 for an example. 
More generally, if there are d'(v) differently colored vertices around t' then we can 
couple the colors so that at most d'(v) color choices cause d t to increase. instead of up 
to 2d'(v) choices in the original coupling. Concretely, let S,(t,) be the sd of colors on 
neighbors of v in the first chain but not the second. and similarly let S2 ( r) be the set 
of colors on neighbors of v in the second chain but not the first. Couple pairs of colors 
c, E SI (v) and C2 E S2 (v) as much as possible. so that when ('I is chosen in one chain 
C2 is chosen in the other. Then the total number of ways to color t' that increases dt is 
at most max(SI(v), S2(V)) :s d'(t'). 
As a result, the probability that dt- I = cit + 1 when cit > 0 is at most 
~ ~ 
d'(t,) = III 
II L 
c 
ell 
IE ,lr 
285 

COUPLING OF MARKOV CHAINS 
We therefore find that 
( 
C -
2~) 
E[dt+1 I dt] :'S dt 1-
. 
nc 
Following the same reasoning as in the proof of Theorem 11.7, we have 
Pr(dt :::: 1) :'S E[dt] :'S n (I _ c - 2~)t :'S ne-t(c-2/':,.)/Ill·, 
I1C 
and the variation distance is at most E after 
r 
nc 
(n)l 
t -
In -
C -
2~ 
E 
steps. 
• 
Hence we can use the Markov chain for proper colorings to give us an FPAUS when-
everc > 2~. 
11.6. Path Coupling 
In Section 10.3 we showed that, if we can obtain an FPAUS for independent sets for 
graphs of degree at most ~, then we can approximately count the number of indepen-
dent sets in such graphs. Here we present a Markov chain on independent sets, together 
with a coupling argument, to prove that the chain gives such an FPAUS when ~ :'S 4. 
The coupling argument uses a further technique, path coupling. We demonstrate this 
technique specifically for the Markov chain sampling independent sets in a graph, al-
though with appropriate definitions the approach can be generalized to other problems. 
Interestingly, it is very difficult to prove that the simple Markov chain for sampling 
independent sets given in Section lOA, which removes or attempts to add a random 
vertex to the current independent set at each step, mixes quickly. Instead, we consider 
here a different Markov chain that simplifies the analysis. We assume without loss of 
generality that the graph consists of a single connected component. At each step, the 
Markov chain chooses an edge (u, v) in the graph uniformly at random. If X t is the 
independent set at time I, then the move proceeds as follows. 
• With probability 1/3, set X t+ 1 = Xt -
{u, v}. (This move removes u and v, if they 
are in the set.) 
• With probability 1/3, let Y = (X t -
{u}) U {v}. If Y is an independent set, then 
Xt+l = Y: otherwise, Xt+1 = Xt. (This move tries to remove u ifit is in the set and 
then add v.) 
• With probability 1/3, let Y = (Xt -
{v}) U {u}. If Y is an independent set, then 
Xt+1 = Y; otherwise, Xt+1 = Xt. (This move tries to remove v ifit is in the set and 
then add u.) 
It is easy to verify that the chain has a stationary distribution that is uniform on all in-
dependent sets. We now use the path coupling argument to bound the mixing time of 
the chain. 
286 

11.6 PATH COUPLING 
(1) 
(2) 
(3) 
Figure 11.3: Three cases for the independent set Markov chain. Vertices colored black arc in h\)th 
independent sets of the coupling. Vertex X is colored gray. to represent that it i.., a member of the 
independent set of one chain in the coupling but not the other. 
The idea of path coupling is to start with a coupling for pair.., of ..,tat\:.., ( x" r. ) that 
differ in just one vertex. This coupling is then extended to a gl2neral cuupling U\L'r 
all pairs of states. When it applies. path coupling is \ery pO\\ erful. becau..,12 it i.., uften 
much easier to analyze the situation where the two state.., differ in a ..,mall \\a: (here. 
in just one vertex) than to analyze all possible pairs of ..,tate". 
Consider a graph G = (V. E). \\-'e say that a \ertex j" held if it i" an dement of Xf or 
Yr but not both; otherwise. the \'ertex is good. Let df = Xi - rf :-
i rr - Xf j. so that df 
counts the number of bad vertices. Assume that Xf and rr differ in exactly one vertex 
(i.e., d f = 1). We apply a simple coupling. performing the same mme in both states. 
and show that under this coupling E[dr-i-l I dr J :s dr when dr = lor. equivalently. that 
E[df+ l -
df I df = 1] :s O. 
Without loss of generality, let Xf = I and Yf = I U {x}. A change in df can oc-
cur only when a move involves a neighbor of x. Thus. in analyzing this coupling, we 
can restrict our discussion to moves in which the chosen random edge is adjacent to a 
neighbor of x. Let 8 ~ = 1 if the vertex;, i=- x goes from good to bad between step t and 
step t + l. Similarly, let 8\ = -1 if the vertex x goes from bad to good between step t 
and step t + l. By linearity of expectations, 
E[dt+ 1 -dt I dt = II =E[ ~8" I dt = 1] = ~Er8", I dt = II· 
.-\S we shall see, in the summation we need only consider those w that are equal to x. 
a neighbor of x, or a neighbor of a neighbor of x, since these are the only \'ertices that 
can change from good to bad or bad to good in one step of the chain. We shall demon-
'itrate how to balance the moves in such a way that it becomes clear that E[dr~l - dr I 
dr = 1] :s 0 as long as ~ :s 4. 
Assume that x has k neighbors. and let y be one of these neighbors. For each vertex 
r that is a neighbor of x. we consider all of the moves that choose an edge adjacent to 
r. The subsequent analysis makes use of the restriction ~ :s 4. There are three cases, 
as shown in Figure 11.3. 
287 

COUPLING OF MARKOV CHAINS 
1. Suppose that y has two or more neighbors in the independent set I = X t . Then no 
move that involves y can increase the number of bad vertices, and hence d t+ 1 cannot 
be larger than d t as a result of any such move. 
2. Suppose that y has no neighbors in I. Then d t can increase by 1 if the edge (y,::'i) 
(where 1 :s i :s 3) is chosen and an attempt is made to add y and remove Zi. These 
moves are successful on X t but not on Yt , and hence 8y = 1 with probability at most 
3· 1/31EI = l/IEI. No other move involving y increases dt. 
The possible gain from 8\, is balanced by moves that decrease 8,. Any of the 
three possible moves on the edge (x, y) match the vertex x, so that 8x = -1, and no 
other bad vertices are created. Hence 8, = -1 with probability at least 1 I I E I. We 
see that the total effect of all of these moves on L U' E [8 U' I d t = 11 is 
1 
1 
1·--1·-=0 
lEI 
lEI 
' 
so that the moves from this case do not increase E[dt+1 - dr I dr = 1]. 
3. Suppose that y has one neighbor in I. If the edge (x, y) is chosen, then two moves 
can give 8, = -1: the move that removes both x and y, or the move that removes y 
and adds x. The third move, which tries to add y and remove x, fails in both chains 
because y has a neighbor in I. Hence 8, = -1 with probability at least ~(l/IEI). 
Let z be the neighbor of y in I. Both y and z can become bad in one step if the 
edge (y, z) is chosen and an attempt is made to add y and remove z. This move 
is successful on Xt but not on Yr, causing dt to increase by 2 since 8\, and 8::: both 
equal 1. No other move increases dt . Hence the probability that the number of bad 
vertices is increased in this case is 1 I 31 E I, and the increase is by 2. Again, the total 
effect of all of these moves on L 11' E [8 u' I dt = 1] is 
1 
2 1 
2· -
-1· -- =0 
31EI 
31EI 
' 
so that the moves from this case do not increase E[dt+! -
dt I dt = 1]. 
The case analysis shows that if we consider moves that involve a specific neighbor 
y, they balance so that every move that increases d t+ 1 -
d t is matched by correspond-
ing moves that decrease dt+1 - dt . Summing over all vertices, we can conclude that 
E[dt+1 - dt I dt = IJ = E[L8u' I dt = 1] = L
E [8 u' I dt = l]:s O. 
11' 
(I' 
We now use an appropriate coupling to argue that E[dt+1 I dt ] :s dt for any pair of 
states (Xt. Yt ). The statement is trivial if d t = 0, and we have just shown it to be true 
if dt = l. If dt > 1, then create a chain of states Z(), Z I, ... ,Zdr as follows: Zo = X r• 
and each successive Zi is obtained from Zi~1 by either removing a vertex from X t - Yr 
or adding a vertex from Yt -
X t . This can be done, for example, by first removing all 
vertices in X t -
Yt one by one and then adding vertices from Yt -
X t one by one. Our 
coupling now arises as follows. When a move is made in X t = Z(), the coupling for 
the case when d t = 1 gives a corresponding move for the state Z I. This move in Z 1 can 
288 

11.7 EXERCISES 
similarly be coupled with a move in state Z2, and so on, until the move in Zd,~1 yields 
a move for Z d, = Yt . Let Z: be the state after the move is made from state Z i, and let 
~(Z;~I'Z:) = IZ;~I - Z;I + IZ; -
Z:~II. 
Note that Zb = Xt+1 and Z;/, = Yt+l · We have shown that E[dt+1 - dt I dt = 1] .s o. 
so we can conclude that 
that is, because the two states Z i ~ I and Z i differ in just one vertex, the expected number 
of vertices in which they differ after one step is at most l. Using the triangle inequality 
for sets, 
we obtain 
or 
Hence, 
IA - BI .s IA - CI + IC - BI, 
ii, 
IXt+1 -
Yt+11 + IYt+1 -
Xt+11 .s L(IZ;~, - Z;I + IZ; -
Z;~II) 
i=1 
eI, 
dt+1 = IXt+1 -
Yt+11 + IYr+1 -
Xr+11 .s L ~(Z;~I' Z;). 
i=1 
eI, 
= L E[~(Z;~I' Z;)] 
i=1 
In previous examples we were able to prove a strict inequality of the form 
for some f3 < 1, and we used this strict inequality to bound the mixing time. How-
ever, the weaker condition E [dt+ I I dt] .s dt that we have here is sufficient for rapid 
mixing, as we shall see in Exercise 11.7. Thus, the Markov chain gives an FPAl'S for 
independent sets in graphs when the maximum degree is at most 4; as we showed in 
Section 10.3, this can be used to obtain an FPRAS for this problem. 
11.7. Exercises 
Exercise 11.1: Write a program that takes as input two positive integers III and 112 and 
t\vo real numbers PI, P2 with 0 .s PI, P2 .s 1. The output of your program should be 
the variation distance between the binomial random variables B( 111. PI) and B(n2, P2), 
rounded to the nearest thousandth. Use your program to compute the variation distance 
289 

COUPLING OF MARKOV CHAINS 
between the following pairs of distributions: B(20,0.5) and B(20,0.49); B(20,0.5) 
and B(21, 0.5): and B(21, 0.5) and B(21, 0.49). 
Exercise 11.2: Consider the Markov chain for shuffling cards, where at each step a 
card is chosen uniformly at random and moved to the top. Suppose that, instead of 
running the chain for a fixed number of steps, we stop the chain at the first step where 
every card has been moved to the top at least once. Show that, at this stopping time, the 
state of the chain is uniformly distributed on the n! possible permutations of the cards. 
Exercise 11.3: Consider the Markov chain for shuffling cards, where at each step a 
card is chosen uniformly at random and moved to the top. Show that, if the chain is 
run for only (1 -
c:) n In n steps for some constant c: > 0, then the variation distance is 
1 - o(l). 
Exercise 11.4: (a) Consider the Markov chain given by the transition matrix 
1/2 
0 
1/2 
0 
0 
0 
1/2 
1/2 
0 
0 
p= 
1/4 
1/4 
0 
1/4 
1/4 
0 
0 
1/2 
1/2 
0 
0 
0 
1/2 
0 
1/2 
Explain why Theorem 11.5 is not useful when applied directly to P. Then apply The-
orem 11.5 to the Markov chain with transition matrix p2, and explain the implications 
for the convergence of the original Markov chain to its stationary distribution. 
(b) Consider the Markov chain given by the transition matrix 
1/2 
0 
1/2 
0 
0 
0 
1/2 
1/2 
0 
0 
p= 
1/5 
1/5 
1/5 
1/5 
1/5 
0 
0 
1/2 
1/2 
0 
0 
0 
1/2 
0 
1/2 
Apply Theorem 11.5 to P. Then apply Theorem 11.5 to the Markov chain with transi-
tion matrix p2, and explain the implications for the convergence of the original Markov 
chain to its stationary distribution. Which application gives better bounds on the vari-
ation distance? 
Exercise 11.5: Suppose I repeatedly roll a standard six-sided die and obtain a sequence 
of independent random variables Xl, X 2, ... , where Xi is the outcome of the ith roll. 
Let 
j 
Yj = LXi mod 10 
i=l 
be the sum of the first j rolls considered modulo 10. The sequence lj forms a Markov 
chain. Determine its stationary distribution, and determine a bound on r(c:) for this 
chain. (Hint: One approach is to use the method of Exercise 11.4.) 
290 

11.7 EXERCISES 
Exercise 11.6: Theorem 11.5 is useful only if there exists a nonzero entry in at least 
one column of the transition matrix P of the MarkO\ chain. Arguc that for any finite. 
aperiodic, irreducible Markov chain, there exists a time T such that e\ery entry of pT 
is nonzero. Explain how this can be used in conjunction with Theorem 11.5. 
Exercise 11.7: A technique we use repeatedly in the chapter is to define a distance 
function d t that represent the distance between the two ...,tates of our coupling after t 
steps, and then show that when d t > ° 
there exists a f} < I such that 
(a) Under this condition, give an upper bound for r (f ) in term..., of f} and £I"'. where £1* 
is the maximum distance over all possible pair" of initial "tatc" for the coupling. 
( b) Suppose that instead we have 
E[dt + l I dtJ :S elr • 
Suppose we have the additional conditions thatel:~i i" onL' llf d .. d. - I. or d r + I 
and that Pr(d t #- dt+ l ) :::: y. Give an upper bound for r (," I in krill" uf f. d'. and 
y. Your answer should by polynomial in d'" and I /y. (Him: Think uf d. a" heing 
similar to a random walk on the line.) 
(c) Using (a) and (b), show that the mixing time of the coloring chain llf SL'\.:tion I\.) 
is polynomial in the number of vertices in the graph and In (I 
,c I. L'\ L'n \\hen the 
number of colors is only 2~. 
(d) By extending the argument of part (b), show that the mixing tillle of the \ larkll\ 
chain for independent sets given in Section 11.6 is polynomial in the numhL'r llf 
vertices in the graph and In (1 I c:). 
Exercise 11.8: Consider the random walk on a non-bipartite. connected grarh on II 
vertices, where each vertex has the same degree d > n12. ShO\\ that 
Inc: 
r(c:) < 
. 
-In(l-(2d-n)ld) 
Exercise 11.9: Consider a Markov chain on n points [0. n -
IJ lying in order on a l:ir-
cleo At each step, the chain stays at the current point with probability I .2 ur Illme" 
to the next point in the clockwise direction with probability 1/2. Find the "tatiunar: 
distribution and show that. for any c: > 0, the mixing time r(c:) is O( 11 2 In( If)). 
Exercise 11.10: In Section 11.2.3. we suggested the following coupling Z: = ( .\':. L ). 
First choose a transition for the chain Xr, with v E X t and /1' E V. If l' E:: Y .. Lhe the 
same vertices v and w for the transition of the chain Yr: othenvise. chOlbe uniformly 
at random a vertex v I E ~ - X t and then perform the transition in the chain Yr with the 
pair Vi and W. Show that this is a valid coupling that satisfies Definition 11..3. 
Exercise 11.11: Show that the Markov chain for sampling all independent sets of size 
exactly k :::; nI3(~ + 1) in a graph with 11 nodes and maximum degree ~. as defined 
in Section 1l.2.3, is ergodic and has a uniform stationary distribution. 
291 

COUPLING OF MARKOV CHAINS 
Exercise 11.12: We wish to improve the coupling technique used in Section 1l.2.3 in 
order to obtain a better bound. The improvement here is related to the technique used 
to prove Theorem 1l.S. As with the coupling in Section 1l.2.3, if an attempt is made to 
move v EXt - Yt to a vertex w then the same attempt is made with the matched ver-
tex in the other chain. If, however, an attempt is made to move a vertex v EXt n Yt in 
both chains, we no longer attempt to make the same move. 
(a) Assume there exists a set SI of exactly d t (t::,. + 1) distinct vertices that are member..., 
of or neighbors of vertices in X t - Yt and, likewise, a set S2 of exactly dt(l~ + I) 
distinct vertices that are members of or neighbors of vertices in Yt -
X t ; assume 
further that S2 and SI are disjoint. Suppose that we match up the vertices in SI and 
S2 in a one-to-one fashion. Argue that the moves can be coupled so that, when one 
chain attempts and fails to move v to a vertex in SI in one chain, it also attempt..., 
and fails to move v to the matching vertex in S2 in the other chain. Similarly, ar-
gue that the moves can be coupled so that, when one chain attempts and succeed..., 
in moving v to a vertex in SI in one chain, it also attempts and succeeds in movin~ 
v to the matching vertex in S2 in the other chain. Show that the coupling gives 
k - dt dt(!~. + 1) 
Pr(dt+ 1 = dt + I) :s ------
k 
11 
(b) In the general case, SI and S2 are not necessarily disjoint or of equal size. ShO\\ 
that in this case, by pairing up failing moves as much as possible, the number of 
choices for w that can increase dt is max(ISII, IS21) :s dt(!~. + 1). Then argue that 
k - dt dt(!~. + 1) 
Pr(dt+1 = dt + 1):s ------
k 
11 
holds in all cases. 
(c) Use this coupling to obtain a polynomial bound on T (8) that holds for any k < 
11/2(6. + 1). 
Exercise 11.13: For a Markov chain with state space S and for any nonnegative inte-
ger t, let 
Li(t) = max II P.~ -
p~ II· 
X.YES 
. 
Assume also that the Markov chain has a stationary distribution. 
(a) Prove Li(s + t) :s Li(s)Li(t) for any positive integers sand t. 
(b) Prove 6.(s + t) :s 6.(s)Li(t) for any positive integers sand t. 
(c) Prove 
for any positive integer t. 
Exercise 11.14: Consider the following variation on shuffling for a deck of 11 card...,. 
At each step, two specific cards are chosen uniformly at random from the deck, and 
their positions are exchanged. (It is possible both choices give the same card, in which 
case no change occurs.) 
292 

11.7 EXERCISES 
(a) Argue that the following is an equivalent process: at each step, a specific card is 
chosen uniformly at random from the deck, and a position from [I, 11] is chosen 
uniformly at random; then the card at position i exchanges positions with the spe-
cific card chosen. 
(b) Consider the coupling where the two choices of card and position are the same 
for both copies of the chain. Let XI be the number of cards whose positions are 
different in the two copies of the chain. Show that XI is nonincreasing over time. 
(c) Show that 
Pr(X,+ 1 :" X, -
I I X, > 0) » (:')'. 
(d) Argue that the expected time until X I is 0 is O( n ~ ). regardless of the starting state 
of the two chains. 
Exercise 11.15: Modify the arguments of Lemma I ().~ and Lemma IO.-l- to show that, 
if we have an FPAUS for proper colorings for any c ~ ~ :- :2. then \\c also have an 
FPRAS for this value of c. 
Exercise 11.16: Consider the following simple Markov chain whose states are inde-
pendent sets in a graph G = (V E). To compute X i + l from X( 
• choose a vertex v uniformly at random from V, and flip a fair coin; 
• if the flip is heads and v E Xi, then Xi+l = Xi \ {v}: 
• if the flip is heads and v tJ- Xi, then Xi +l = Xi; 
• if the flip is tails. v tJ- Xi, and adding v to Xi still gives an independent set, then 
X i+ l = Xi U {v}; 
• if the flip is tails and v E Xi, then Xi+ l = Xi. 
(a) Show that the stationary distribution of this chain is uniform over all independent 
sets. 
(b) We consider this Markov chain specifically on cycles and line graphs. For a line 
graph with n vertices. the vertices are labeled 1 to 11. and there is an edge from I to 
2, 2 to 3, . , ., n -
I to 11. A cycle graph on n vertices is the same with the addition 
of an edge from n to I. 
Devise a coupling (XI' YI ) for this Markov chain such that. on line graphs and 
cycle graphs, if d l = IXI - YI I + I YI - XI I is the number of \'t~rtices on \\hich the 
two independent sets disagree. then at each step the coupling is at least as likely to 
reduce d I as to increase d I' 
(c) With the coupling from part (b). argue that you can use this chain to obtain an 
FPAUS for independent sets on a cycle graph or line graph, You may \\'ant to use 
Exercise 11.7. 
(d) For the special cases of line graphs and cycle graphs. \\c can dcri\c exact formu-
las for the number of independent sets, Derive exact formulas for these cases and 
prove that your formulas are correct. (Hint: You may want to express your results 
in terms of Fibonacci numbers.) 
293 

COUPLING OF MARKOV CHAINS 
Exercise 11.17: For integers a and b, an a x b grid is a graph whose vertices are all 
ordered pairs of integers (x, y) with 0 :s x < a and 0 :s y < b. The edges of the graph 
connect all pairs of distinct vertices (x, y) and (x', y') such that Ix - x'i + Iy - y'l = 
1. That is, every vertex is connected to the neighbors up, down, left, and right of it, 
where vertices on the boundary are connected to the relevant points only. Consider the 
following problems on the graph given by the 10 x 10 grid. 
(a) Implement an FPAUS to generate an 8-uniform proper 1O-coloring of the graph, 
where 8 is given as an input. Discuss how many steps your Markov chain runs for, 
what your starting state is, and any other relevant details. 
(b) Using your FPAUS as a subroutine, implement an FPRAS to generate an (8,8)-
approximation to the number of proper 1O-colorings of the graph. Test your code 
by running it to obtain a (O.L O.OOl)-approximation. (Note: This may take a sig-
nificant amount of time to run.) Discuss the ordering you choose on the edges. 
how many samples are required at each step, how many steps of the Markov chain 
you perform in total throughout the process, and any other relevant details. 
Exercise 11.18: In Section 11.2.3 we considered the following Markov chain on inde-
pendent sets: a move is made from the independent set X t by choosing a vertex v E 
X t uniformly at random and picking a vertex w uniformly at random from the graph. 
If Xt -
{v} + {w} is an independent set, then XI+! = XI -
{v} + {w}; otherwise. 
X t +! = Xt . We have shown that the chain converges quickly to its stationary distribu-
tion via bounding T (8) by an expression that is polynomial in 11 and In (1 /8) whenever 
k :s 11/2(6. + 1). Use the idea of path coupling to simplify the proof. 
Exercise 11.19: In Section 11.5, we considered a simple Markov chain for coloring. 
Suppose that we can apply the path coupling technique. (You do not need to show this.) 
In this case, we can just consider the case where d l = 1. Give a simpler argument that. 
when d l = 1 and c > 26., E[d l +! I d l ] :s f3d l for some f3 < l. Also show that, when 
dl = 1 and c = 26., E[d l +! I d l J :s d l • 
294 

CHAPTER TWELVE 
Martingales 
Martingales are sequences of random variables satisfying certain conditions that arise 
in numerous applications, such as random walks and gambling problems. We focus 
here on three useful analysis tools related to martingales: the martingale stopping theo-
rem, Wald 's inequality, and the Azuma-Hoeffding inequality. The martingale stopping 
theorem and Wald 's equation are important tools for computing the expectation of com-
pound stochastic processes. The Azuma-Hoeffding inequality is a powerful technique 
for deriving Chernoff-like tail bounds on the values of function..., of dependent ran-
dom variables. We conclude this chapter with applications of the Azuma-Hoeffding 
inequality to problems in pattern matching, balls and bins. and random graphs. 
12.1. Martingales 
Definition 12.1: A sequence of random variables Zo, ZI .... is u martingale H'ith re-
spect to the sequence Xo, XI, ... If, for all n 2: 0, thefollmring conditions hold: 
• Zn is a function of Xo, XI,"" Xn; 
• E[IZnl] < 00; 
• E[Zn+1 I Xo,···, XII] = Zn. 
A sequence of random variables Zo, ZI,'" is called martingale H'hen if is (/ lIlurtin-
gale with respect to itse(f. That is, E[IZnl] < 00, and E[ZII~I I ZI).,.,. Znl = Zn' 
A martingale can have a finite or a countably infinite number of elements, The index-
ing of the martingale sequence does not need to start at 0. In fact. in many applications 
it is more convenient to start it at 1. When we say that Zo. Z I. ' , , is a martingale with 
respect to XI, X2, " ., then we may consider Xo to be a constant that is omitted. 
For example, consider a gambler who plays a sequence of fair games. Let Xi be the 
amount the gambler wins on the ith game (Xi is negative if the gambler loses), and let 
Zi be the gambler's total winnings at the end of the ith game. Because each game is 
fair, E[XiJ = ° 
and 
295 

MARTINGALES 
Thus, Z I, Z 2, ... , Z/l is a martingale with respect to the sequence XI, X 2, ... , XI/' 
Interestingly, the sequence is a martingale regardless of the amount bet on each game. 
even if these amounts are dependent upon previous results. 
A Doob martingale refers to a martingale constructed using the following general 
approach. Let Xo, XI,"" X/l be a sequence of random variables, and let Y be a ran-
dom variable with E[IYI] < 00. (Generally, Y will depend on Xo, ... , XII') Then 
Zi=E[YIXo, ... ,XiJ, i=O,I, ... ,n, 
gives a martingale with respect to X(), XI, ... , XI/' since 
E[Zi+1 I Xo, ... , XiJ = E[E[Y I X(), ... , Xi+ l ] I X(), ... , XiJ 
= E[Y I X(), ... , Xd 
Here we have used the fact that E[Y I Xo, ... , Xi+11 is itself a random variable and that 
Definition 2.7 for conditional expectation yields 
E[V I W] = E[E[V I U, W] I W]. 
In most applications we start the Doob martingale with Zo = E[Y], which corre-
sponds to Xo being a trivial random variable that is independent of Y. To understand 
the concept of the Doob martingale, assume that we want to predict the value of the 
random variable Y and that the value of Y is a function of the values of the random 
variables XI, ... , XI/' The sequence Z o. Z I .... , ZI/ represents a sequence of refined es-
timates of the value of Y, gradually using more information on the values of the random 
variables XI, X 2, ... , XI/' The first element. Zo, is just the expectation of Y. Element 
Zi is the expected value of Y when the values of XI, ... , Xi are known, and if Y is fully 
determined by XI, ... , XI/ then ZII = Y. 
We now consider two examples of Doob martingales that arise in evaluating the 
properties of random graphs. Let G be a random graph from GII • p ' Label the 111 = (;) 
possible edge slots in some arbitrary order, and let 
(
I 
X -
J -
0 
if there is an edge in the jth edge slot, 
otherwise. 
Consider any finite-valued function F defined over graphs; for example, let F( G) 
be the size of the largest independent set in G. Now let Zo = E[F(G)] and 
Zi=E[F(G)IXI, ... ,XiJ, i=1, ... ,111. 
The sequence Z 0, Z I, ... , Z //I is a Doob martingale that represents the conditional ex-
pectations of F( G) as we reveal whether each edge is in the graph, one edge at a time. 
This process of revealing edges gives a martingale that is commonly called the edge 
exposure martingale. 
Similarly, instead of revealing edges one at a time, we could reveal the set of edges 
connected to a given vertex, one vertex at a time. Fix an arbitrary numbering of the 
296 

12.2 STOPPING TIMES 
vertices 1 through n, and let Gi be the subgraph of G induced by the first i vertices. 
Then, setting Zo = E[F(G)] and 
Zi = E[F(G) I G I , ... , Gil, 
i = 1, .. . ,n, 
gives a Doob martingale that is commonly called the \'ertex exposure martingale. 
12.2. Stopping Times 
Returning to the gambler who participates in a sequence of fair gambling rounds, we 
saw in the previous section that ZI, Z2,'" is a martingale, where Zi is the gambler's 
winnings after the ith game. If the player decides (before starting to play) to quit after 
exactly k games, what are the gambler's expected winnings? 
Lemma 12.1: fl the sequence Zo, Z I, ., ., Z/I is a martingale with respect to Xo, XI, 
... , Xu, then 
E[Zul = E[Zo]. 
Proof: Since Z 0, Z I, ... is a martingale with respect to Xo, XI, ... , XII' it follows that 
Taking the expectation of both sides and using the definition of conditional expecta-
tion, we have 
E[ZiJ = E[E[Zi-+-1 I X() ..... X,]] = E[Z,_i I. 
Repeating this argument yields 
E[Zu] = E[Zo]. 
• 
Thus, if the number of games played is initially fixed then the expected gain from the 
sequence of games is zero. Suppose now that the number of games played is not fixed. 
For example, the gambler could choose to playa random number of games. An even 
more complex (and realistic) situation arises when the gambler's decision to quit play-
ing is based on the outcome of the games already played. For example, the gambler 
could decide to keep playing until his winnings total at least a hundred dollars. The 
following notion proves quite powerful. 
Definition 12.2: A nonnegative, integer-valued random variable Tis ([ stopping time 
for the sequence {Z,P n ~ O} ~f the e\'ent T = n depends ollly 011 the \'ullle (~f tize /"(/11-
dom variables Zo, ZI,"" Zu. 
A stopping time corresponds to a strategy for determining when to stop a sequence 
based only on the outcomes seen so far. For example, the first time the gamhler wins 
five games in a row is a stopping time, since this can be determined by looking at the 
outcomes of the games played. Similarly. the first time the gamhler has won at least a 
hundred dollars is also a stopping time. Letting T be the I({st time the gambler wins 
297 

MARTINGALES 
five games in a row, however, would not be a stopping time, since determining whether 
T = 11 cannot be done without knowing ZII+I, ZI1+2, .... 
In order to fully utilize the martingale property, we need to characterize condition~ 
on the stopping time T that maintain the property E[Z T] = E[Zo] = O. It would seem. 
if the game is fair, that E[Z T] = 0 should always hold. But consider the case where 
the gambler's stopping time is the first T such that ZT > B, where B is a fixed con-
stant greater than O. In this case, the expected gain when the gambler quits playing i~ 
greater than O. The subtle problem with this stopping time is that it might not be finite. 
so the gambler may never finish playing. The martingale stopping theorem shows that. 
under certain conditions and in particular when the stopping time is bounded or has 
bounded expectation, the expected value of the martingale at the stopping time is equal 
to E[Zo]. We state a version of the martingale stopping theorem (sometimes called the 
optional stopping theorem) without proof. 
Theorem 12.2 [Martingale Stopping Theorem]: flZo, ZI,'" is a martingale with 
respect to XI. X 2, ... and if T is a stopping time for XI, X 2 •.... then 
whenever one (~lthe following holds: 
• the Zi are bounded, so there is a constant c such that, for all i, I Zi I .:'S c; 
• T is bounded; 
• E[T] < ex), and there is a constant c such that E[I Zi+1 - Zi I I Xl .... , Xil < c. 
We use the martingale stopping theorem to derive a simple solution to the gambler'~ 
ruin problem introduced in Section 7.2.1. Consider a sequence of independent, fair 
gambling games. In each round. a player wins a dollar with probability 1/2 or loses a 
dollar with probability 1/2. Let Zo = O.let Xi be the amount won on the ith game, and 
let Zi be the total won by the player after i games (again, Xi and Zi are negative if the 
player loses money). Assume that the player quits the game when she either loses f I 
dollars or wins £ 2 dollars. What is the probability that the player wins £ 2 dollars before 
losing £ I dollars? 
Let the time T be the first time the player has either won £ 2 or lost £ I. Then T is a 
stopping time for XI, X 2 , .... The sequence Zo. ZI,'" is a martingale and, since the 
values of the Zi are clearly bounded, we can apply the martingale stopping theorem. 
We therefore have 
E[ZT] = O. 
Let q be the probability that the gambler quits playing after winning £2 dollars. Then 
gIvmg 
£1 
q=--, 
£1 + £2 
matching the result found in Section 7.2.1. 
298 

12.2 STOPPING TIMES 
12.2.1. Example: A Ballot Theorem 
The following ballot theorem is another application of the martingale stopping theo-
rem. Suppose that two candidates run for an election. Candidate A obtains a votes, 
and candidate B obtains b < a votes. The votes are counted in a random order, chosen 
uniformly at random from all permutations on the (/ -+- h votes. We show that the prob-
ability that candidate A is always ahead in the cOllnt is ((/ - b)/(a + b). Although this 
can be determined combinatorially, we pro\'ide an elegant martingale argument. 
Let n = a + b be the total number of \'otes. anu let 5 ~ be the number of votes by 
which candidate A is leading after k votes are counted (51, can be negative). Then 5 11 = 
a-b. For 0 :'S k :'S n -
I, define 
5n-~ 
X/.; = --. 
Il-k 
We first show that the sequence Xo, X 2, .... Xn -
I form" a martingale. Note that the 
sequence Xo, XI, ... , XII relates to the counting proce"" in a bacK\\ard order; Xo is a 
function of 5", XII _I is a function of 51, and so on. Con"ider 
Conditioning on Xo, ... , X/.;_I is equivalent to conditioning on S,. S, 
I.,··. 511 
~_I. 
which in turn is equivalent to conditioning on the values of the l.'lwnt \\!1en l.'ounting 
the last k -
I votes. 
Conditioning on 5,,-/.;+1, the number of votes that candidate A hau after l.'uunting the 
first n - k + I votes is 
11 -
k + I + 511-/.;+1 
2 
and the number of votes that candidate B had is 
n-k+I-511 _/.;+1 
2 
The (n - k + I)th vote in the count is a random vote from among the"e tir"t II - k -+- I 
votes. Also, 5 11 -/.; is equal to 5 11 -/.;+1 + I if the (11 - k + l)th \'ote \\,1>, for l.'anuiuate B 
and equal to 5 11 -/.;+1 -
I if that vote was for candidate A. Thus. for k ~ 1. 
Therefore, 
11 -k+ 1- 5n-~-1 
E[511 _/.; I 511 - k+IJ = (511 -/.;+1 + 1)-------
2(n-k+l) 
5 
' 
_ I 11 - k + I + 5/1 ; - 1 
+ (11-/.;+1 
) 
2(11 - k + 1) 
n-k 
= 511-/.;+1----
l1-k+1 
E[X/.; I Xo, ... , X/.;-IJ = E -- 15/1, .... 5/1_~_1 
[ 
511-/'; 
] 
l1-k 
5 11 -/.;+1 
n-k+l 
= X/.;_I, 
showing that the sequence Xo. XI,. '" X II _ I is a martingale. 
299 

MARTINGALES 
Define T to be the minimum k such that X" = 0 if such a k exists, and T = n -
I 
otherwise. Then T is a bounded stopping time, satisfying the requirements of the mar-
tingale stopping theorem, and 
E[5111 
a-b 
E[Xr] = E[Xo] = -- = --. 
n 
a +b 
We now consider two cases. 
Case 1: Candidate A leads throughout the count. In this case, all 5 11 -" (and therefore 
all X,,) are positive for 0 ~ k ~ n -
1, T = n - I, and 
Xr = XII _ I = 51 = 1. 
That 51 = I follows because candidate A must receive the first vote in the count to 
be ahead throughout the count. 
Case 2: Candidate A does not lead throughout the count. In that case we claim that 
for some k < n - I, X" = O. Candidate A clearly has more votes at the end. If can-
didate B ever leads, then there must be some intermediate point k where 5" (and 
therefore Xd is O. In this case, T = k < n - I and Xr = O. 
Observe that 
a-b 
E[Xr] = -- = I . Pr(Case 1) + O· Pr(Case 2), 
a+b 
and thus the probability of Case I, in which candidate A leads throughout the count, is 
(a - b)/(a + b). 
12.3. Wald's Equation 
An important corollary of the martingale stopping theorem is known as Wald's equa-
tion. Wald's equation deals with the expectation of the sum of independent random 
variables in the case where the number of random variables being summed is itself a 
random variable. 
Theorem 12.3 [Wald 's Equation]: Let XI, X 2., ... be nonnegative, independent, iden-
tical!.v distributed random \'ariables with distribution X. Let T he a stopping time for 
this sequence. 1fT and X hare bounded expectation, then 
T 
E[ LXi] = E[T] . EfX] 
[=1 
In fact, Wald 's equation holds more generally; there are different proofs of the equalit) 
that do not require the random variables XI, X 2., ... to be nonnegative. 
Proof: For i ~ I, let 
Zi = L(X) - E[X]), 
j=1 
The sequence ZI, Z2,'" is a martingale with respect to XI, X2, ... , and E[ZI1 = 0, 
300 

12.3 WALD'S EQUATION 
Now, E[T] < 00 and 
Hence we can apply the martingale stopping theorem to compute 
We now find 
T 
E[ZT] = E[ I)Xj - E[X])] 
J=I 
= E[ (t Xj ) -
TE[Xl] 
J=I 
T 
= E[ LXi] - EfT] EfX] 
J=I 
= 0, 
which gives the result. 
• 
In the case of a sequence of independent random variables. we have an equi\alent. sim-
pler definition of stopping time that is easier to apply. 
Definition 12.3: Let Z 0, Z I, .... Z /I be a sequence qf'independent randolll \'ariahles. 
A nonnegative, integer-valued random variable T is a stopping time f(n' [he sequence 
if the event T = n is independent of ZII+I, ZII+2, .. ·. 
As a simple example, consider a gambling game in which a player first rolls one stan-
dard die. If the outcome of the roll is X then she rolls X new standard dice and her gain 
Z is the sum of the outcomes of the X dice. What is the expected gain of this game? 
For 1 ~ i ~ X, let }j be the outcome of the ith die in the second round. Then 
x 
E[Z] = E[ LY} 
[=1 
By Definition 12.3, X is a stopping time, and hence by Wald's equality we obtain 
E[Z] = E[X]· E[Y;] = G)' 
49 
4 
Wald's equation can arise in the analysis of Las Vegas algorithms. which always 
give the right answer but have variable running times, as we saw for the randomized 
algorithm for the median described in Section 3.4. In a Las Vegas algorithm we often 
repeatedly perform some randomized subroutine that mayor may not return the right 
answer. We then use some deterministic checking subroutine to determine whether or 
not the answer is correct; if it is correct then the Las Vegas algorithm terminates with 
the correct answer, and otherwise the randomized subroutine is run again. If N is the 
301 

MARTINGALES 
number of trials until a correct answer is found and if Xi is the running time for the 
two subroutines on the ith trial, then - as long as the Xi are independent and identi-
cally distributed with distribution X - Wald 's equation gives that the expected running 
time for the algorithm is 
E[ t Xi] = E[N] . E[XI. 
[=1 
An example of this approach is given in Exercise 12.12. 
As another example, consider a set of n servers communicating through a shared 
channel. Time is divided into discrete slots. At each time slot, any server that need" 
to send a packet can transmit it through the channel. If exactly one packet is sent at 
that time, the transmission is successfully completed. If more than one packet is sent. 
then none are successful (and the senders detect the failure). Packets are stored in the 
server's buffer until they are successfully transmitted. Servers follow the following 
simple protocol: at each time slot, if the server's buffer is not empty then with proba-
bility lin it attempts to send the first packet in its buffer. Assume that servers have an 
infinite sequence of packets in their buffers. What is the expected number of time slOb 
used until each server successfully sends at least one packet? 
Let N be the number of packets successfully sent until each server has successfull,: 
sent at least one packet. Let ti be the time slot in which the ith successfully transmit-
ted packet is sent, starting from time to = 0, and let ri = ti - ti -I. Then T, the number 
of time slots until each server successfully sends at least one packet, is given by 
N 
T= Lri. 
i=1 
You may check that N is independent of the ri, and N is bounded in expectation; hence 
N is a stopping time for the sequence of ri. 
The probability that a packet is successfully sent in a given time slot is 
The ri each have a geometric distribution with parameter p, so E[ri J = lip ~ e. 
Given that a packet was successfully sent at a given time slot, the sender of that 
packet is uniformly distributed among the 11 servers, independent of previous steps. 
Using our analysis of the expectation of the coupon collector's problem from Chap-
ter 2, we deduce that ErN] = nH(n) = 11 Inn + O(n). We now use Wald's equalit,: 
to compute 
which is about en In n. 
N 
E[T] =E[ I>] 
[=1 
= ErN] . E[riJ 
nH(n) 
p 
302 

12.4 TAIL INEQUALITIES FOR MARTINGALES 
12.4. Tail Inequalities for Martingales 
Perhaps the most useful property of martingales for the analysis of algorithms is that 
Chernoff-like tail inequalities can apply, even when the underlying random variables 
are not independent. The main results in this area are Azuma's inequality and Hoeff-
ding's inequality. They are quite similar, so they are often together referred to as the 
Azuma-Hoeffding inequality. 
Theorem 12.4 [Azuma-Hoeffding Inequality]: Let Xu ..... XII he a martingale such 
that 
Then, for all t .:::: ° 
and any A > 0, 
Pr(IX/ -
Xol.:::: i.) ~ 2e-i~ (~~:~;(~). 
Proof: The proof follows the same format as that for Chernoff hounds (Section 4.2). 
We first derive an upper bound for E[e UI x, - .\111]. Toward that end. \\c dctine 
Note that l}j I ~ Ci and, since Xo, XI, ... is a martingale. 
E[}j I Xo, XI,···, Xi-IJ = E[Xi - Xi- I I Xo, Xl,···. Xi-IJ 
= E[Xi I Xo, XI,.·., Xi-IJ - Xi- I = 0. 
Now consider 
Writing 
and using the convexity of eaY" we have that 
Y 
I -
}j/Ci 
_at" 
I + Yi/Ci 
at" 
ea , < 
e 
'+ 
e 
I 
2 
2 
Since E[}j I Xo, XI, ... , Xi-II = 0, we have 
Here we have used the Taylor series expansion of e\ to find 
303 

MARTINGALES 
e
IXCi + e-
IXCi < e(IXCi )2/2 
2 
-
, 
in a manner similar to the proof of Theorem 4.7. It follows that 
Hence, 
Pr(X/ - Xo ~ A) = Pr(eIX(x/-Xo) ~ eIXA ) 
E[eIX(x/-Xo)] 
<-----
where the last inequality comes from choosing ex = A/L~=l c;' A similar argument 
gives the bound for Pr(X/ -
Xu ::::: -A), as can be seen for example by replacing Xi 
everywhere by -Xi, giving the theorem. 
• 
The following corollary is often easier to apply. 
Corollary 12.5: Let Xo, Xl, . " he a martingale such that. for all k ~ I, 
Then. for all t ~ I and A > 0, 
Pr(1 X/ - Xo I ~ AC0) ::::: 2e-
A2
/ 2• 
We now present a more general form of the Azuma-Hoeffding inequality that yield" 
slightly tighter bounds in our applications. 
Theorem 12.6 [Azuma-Hoeffding Inequality]: Let Xo, ... , X I1 be a martingale sllch 
that 
for some constants dk and for some random variables Bk that ma,V be functions of 
Xo, Xl,.'" X k- l • Then, for all t ~ ° 
and any A > 0, 
Pr(IX/ - Xol ~ A) ::::: 2e-n2/(L~=ldn. 
304 

12.5 APPLICATIONS OF THE AZUMA-HOEFFDING INEQUALITY 
This version of the inequality generalizes the requirement ofa bound on IXk -
Xk-ll. 
The key is the gap dk between the lower and upper bounds for X k -
X k- l . Notice that. 
when we have the bound IXk -
Xk-ll ~ Ck, this result is equivalent to Theorem 12.4 
using Bk = -Ck with a gap dk = 2Ck. The proof is similar to that for Theorem 12.4 
and is left as Exercise 12.6. 
12.5. Applications of the Azuma-Hoeffding Inequality 
12.5.1. General Formalization 
Before giving several applications of the Azuma-Hoeffding inequality, we describe a 
useful general technique. Let us say that a function 
satisfies the Lipschit~ conditioll with bound c if. for any i and for any set of values 
Xl, ... , x/l and Yi. 
That is. changing the value of any single coordinate can change the function value by 
at most c. 
Let 
and 
The sequence Zo. Zl, ... is a Doob martingale, and if the X k are independent ran-
dom variables then we claim that there exist random variables Bko depending only on 
Zo ..... Zk-l. with Bk ~ Zk -
Zk-l ~ Bk + c. The gap between the lower and upper 
bounds on Zk -
Zk-l is then at most c, so the Azuma-Hoeffding inequality of Theo-
rem 12.6 applies. 
We prove this for the case of discrete random variables (although the result holds 
more generally). To ease the notation. we use Sk as shorthand for Xl, X2 •...• X k , so 
that we write 
E[J(X) I Sd 
for 
Also, let us abuse notation and define 
fdX,x) = f(Xl •...• Xk-I,X,Xk+I •... ,XI/). 
That is, .fk (X, x) is f(X) with the value x in the kth coordinate. We shall likewise 
write 
305 

MARTINGALES 
Given this notation, we have 
-
-
Zk - Zk-I = E[f(X) I Sk] - E[f(X) I Sk-I]. 
Hence Zk - Zk-I is bounded above by 
-
-
supE[f(X) I Sk-I, Xk = x] - E[f(X) I Sk-JJ 
x 
and bounded below by 
-
-
infE[f(X) I Sk-I,Xk = y] - E[f(X) I Sk-JJ· 
\' 
(If we are dealing with random variables that can take on only a finite number of val-
ues, we could use max and min in place of sup and inf.) Therefore, letting 
-
-
Bk = infE[f(X) I Sk-I, Xk = y] - E[f(X) I Sk-I], 
\' 
if we can bound 
-
-
supE[f(X) I Sk-I,Xk = x] - infE[f(X) I Sk-I,Xk = y]:s c 
x 
\' 
then we will have appropriately bounded the gap Zk - Zk-I' Now 
supE[f(X) I Sk-I,Xk = x] - infE[f(X) I Sk-I,Xk = y] 
x 
\' 
= sup(E[f(X) I Sk-I, X k = x] - E[f(X) I Sk-I, Xk = yJ) 
x,r 
-
-
= sup E[fdX, x) - fdX, y) I Sk-JJ· 
X.V 
Because the Xi are independent, the probability of any specific set of values for 
X k+ 1 through XII does not depend on the values of XI,.'" X k • Hence, for any val-
ues x, y . .:1, ...• ':k-I we have that 
-
-
E [fd X. x) - fd X, y) I X I = Z I, ... , X k -I = Z k - I ] 
is equal to 
But 
h (Z, x) - fd Z, y) :s c, 
and hence 
-
-
E[fdX,x) - fk(X,y) I Sk-I]:S c, 
giving the required bound. 
The requirement that the Xi be independent random variables is essential to applying 
this general framework. Finding a counterexample when the Xi are not independent j" 
left as Exercise 12.20. 
306 

12.5 APPLICATIONS OF THE AZUMA-HOEFFDING INEQUALITY 
12.5.2. Application: Pattern Matching 
In many scenarios. including examining DNA structure, a goal is to find interesting 
patterns in a sequence of characters. In this context. the phrase "interesting patterns" 
often refers to strings that occur more often than one would expect if the characters were 
simply generated randomly. This notion of "interesting" is reasonable if the number 
of occurrences of a string is concentrated around its expectation in the random model. 
We show concentration using the Azuma-Hoeffding inequality for a simple random 
model. 
Let X = (X" ... , XII) be a sequence of characters chosen independently and uni-
formly at random from an alphabet L, where s = 1 L I. Let B = (h" ... , hd be a fixed 
string of k characters from L. Let F be the number of occurrences of the fixed string 
B in the random string X. Clearly, 
( 
1 . ~ 
E[FJ=(n-k+ll ~). 
We use a Doob martingale and the Azuma-Hoeffding inequality to show that. if k is 
relatively small with respect to n, then the number of occurrences of B in X is highly 
concentrated around its mean. 
Let 
Zo=E[F], 
and for 1 :'S i :'S n let 
Zi = E[F 1 X" ... , Xi]. 
The sequence Zo .... , ZII is a Doob martingale, and 
Since each character in the string X can participate in no more than k possible 
matches, for any 0 :'S i :'S 11 we have 
In other words. the value of Xi~' can affect the value of F by at most k in either direc-
tion, since Xi+, participates in no more than k possible matches. Hence the difference 
must be at most k. Applying Theorem 12..+ yields 
or (from Corollary 12.5) 
We can obtain slightly better bounds by applying the general framework of Theo-
rem 12.6. Let F = l(X" X2 •...• XII)' Then. by our preceding argument. changing the 
307 

MARTINGALES 
value of any single Xi can change the value of F by at most k, and hence the function 
satisfies the Lipschitz condition with bound k. Theorem 12.6 then applies to give 
Pr (I F - E [ F 11 ~ £) :'S 2 e - 2 E 2/11 k 2 , 
improving the value in the exponent by a factor of 4. 
12.5.3. Application: Balls and Bins 
Suppose that we are throwing m balls independently and uniformly at random into n 
bins. Let Xi be the random variable representing the bin into which the ith ball falls. 
Let F be the number of empty bins after the m balls are thrown. Then the sequence 
is a Doob martingale. We claim that F = f( XI, X 2, ... , Xn) satisfies the Lipschitz 
condition with bound 1. Consider how F changes from the placement of the ith ball. 
If the ith ball lands in a bin on its own, then changing Xi so that the ith ball lands in 
a bin with some other ball will increase F by 1. Similarly, if the ith ball lands in a 
bin with other balls, then changing Xi so that the ith ball lands in an otherwise empty 
bin decreases F by 1. In all other cases, changing Xi leaves F the same. We therefore 
obtain 
Pr ( 1 F - E r F ] 1 
:::: £) :'S 2 e - 2 E 2/11/ 
by the Azuma-Hoeffding inequality of Theorem 12.6. We could also apply Theo-
rem 12.4 with 1 Z i + I -
Z i 1 :'S 1, but this gives a slightly weaker result. Here 
( 
1)111 
E[F]=n I-~ , 
but we could obtain the concentration result without knowing E[F]. 
This result can be improved by taking more care in bounding the gap between the 
bounds on Zi+1 - Zi. This is considered in Exercise 12.19. 
12.5.4. Application: Chromatic Number 
Given a random graph G in G II . fI " the chromatic number X(G) is the minimum number 
of colors needed in order to color all vertices of the graph so that no adjacent vertices 
have the same color. We use the vertex exposure martingale defined in Section 12.1 to 
obtain a concentration result for X(G). 
Let Gi be the random subgraph of G induced by the set of vertices 1, ... , i, let Zo = 
E[X(G)], and let 
Since a vertex uses no more than one new color, again we have that the gap between Zi 
and Zi-I is at most 1, so we can apply the general framework of the Azuma-Hoeffding 
inequality from Theorem 12.6. We conclude that 
Pr (I X ( G) - E [ X ( G ) ] 1 :::: A ~) :'S 2 e - 2A 
2 
• 
This result holds even without knowing E[x(G)l. 
308 

12.6 EXERCISES 
12.6. Exercises 
Exercise 12.1: Show that. if Zo, ZI,.'" ZI/ is a martingale with respect to Xo, Xl, .... 
X n , then it is also a martingale with respect to itself. 
Exercise 12.2: Let Xo = 0 and for j :::: 0 let Xj + 1 be chosen uniformly over the real 
interval [Xj' 1]. Show that, for k :::: 0, the sequence 
Y" = 2"(1 - Xd 
is a martingale. 
Exercise 12.3: Let XI, X2 , '" be independent and identically distributed random vari-
ables with expectation 0 and variance cr 2 < x. Let 
z" = (t x,)' - /I,,'. 
1=1 
Show that Z I, Z 2, ... is a martingale. 
Exercise 12.4: Consider the gambler's ruin problem. where a player plays a sequence 
of independent games. either winning one dollar with probability 1/.2 or losing one dol-
lar with probability 1/2. The player continues until either losing t I dollars or winning 
£2 dollars. Let XI/ be 1 if the player wins the nth game and -1 otherwise. Let ZI/ = 
( L ;1= I Xi) 2 - n. 
(a) Show that Z I. Z 2 •... is a martingale. 
(b) Let T be the stopping time when the player finishes playing. Determine E[Zr]. 
(c) Calculate E[T]. (Hint: You can use what you already know about the probability 
that the player wins.) 
Exercise 12.5: Consider the gambler's ruin problem, where now the independent 
games are such that the player either wins one dollar with probability p < 1/2 or loses 
one dollar with probability 1 - p. As in Exercise 12.4, the player continues until either 
losing £ I dollars or winning t 2 dollars. Let XI! be 1 if the player wins the nth game and 
-1 otherwise, and let ZI/ be the player's total winnings after n games. 
(a) Show that 
_ (~)LI1 
AI/ -
P 
is a martingale with mean 1. 
(b) Determine the probability that the player wins £2 dollars before losing t I dollars. 
(c) Show that 
BI/ = ZI/ -
(2p -
l)n 
is a martingale with mean O. 
309 

MARTINGALES 
(d) Let T be the stopping time when the player finishes playing. Determine E [Z T], 
and use it to determine E[TI. (Hint: You can use what you already know about 
the probability that the player wins.) 
Exercise 12.6: Prove Theorem 12.6. 
Exercise 12.7: In the bin-packing problem, we are given items with sizes a I, a2, ... , a l1 
with ° ::::: (ll ::::: I for I ::::: i ::::: n. The goal is to pack them into the minimum number of 
bins. with each bin being able to hold any collection of items whose total sizes sum to 
at most 1. Suppose that each of the (ll is chosen independently according to some dis-
tribution (which might be different for each i). Let P be the number of bins required 
in the best packing of the resulting items. Prove that 
Pr ( I P -
E [ P ] I 2': A) ::::: e -?J 
2 
/11. 
Exercise 12.8: Consider an n-cube with N = 2'1 nodes. Let 5 be a non empty set of 
vertices on the cube, and let x be a vertex chosen uniformly at random among all ver-
tices of the cube. Let D(x, 5) be the minimum number of coordinates in which x and 
y differ over all points y E 5. Give a bound on 
Pr ( I D ( x, 5) - E [ D ( x. 5) ] I > A). 
Exercise 12.9: In Chapter 4 we developed a tail bound for the sum of {O, I} random 
variables. We can use martingales to generalize this result for the sum of any random 
variables whose range lies in [0,11. Let X I.X2"",X I1 be independent random vari-
ables such that Pr(O ::::: XI ::::: 1) = 1. If 51/ = L;I=I XI, show that 
Exercise 12.10: A parking-lot attendant has mixed Lip n keys for 11 cars. The 11 car 
owners arrive together. The attendant gives each owner a key according to a permuta-
tion chosen uniformly at random from all permutations. If an owner receives the key to 
his car, he takes it and leaves: otherwise, he returns the key to the attendant. The atten-
dant now repeats the process with the remaining keys and car owners. This continues 
until all owners receive the keys to their cars. 
Let R be the number of rounds until all car owners receive the keys to their cars. We 
want to compute E[R]. Let XI be the number of owners who receive their car keys in 
the ith round. Prove that 
}j = L(XI - E[XI I XI, ... , XI_I]) 
j=1 
is a martingale. Use the martingale stopping theorem to compute ErR]. 
Exercise 12.11: Alice and Bob play each other in a checkers tournament, where the 
first player to win four games wins the match. The players are evenly matched, so the 
probability that each player wins each game is 1/2, independent of all other games. 
310 

12.6 EXERCISES 
The number of minutes for each game is uniformly distributed over the integers in the 
range [30,60], again independent of other games. What is the expected time they spend 
playing the match? 
Exercise 12.12: Consider the following extremely inefficient algorithm for sorting n 
numbers in increasing order. Start by choosing one of the n numbers uniformly at ran-
dom, and placing it first. Then choose one of the remaining 11 -
1 numbers uniformly 
at random, and place it second. If the second number is smaller than the first, start over 
again from the beginning. Otherwise, next choose one of the remaining n - 2 num-
bers uniformly at random, place it third, and so on. The algorithm starts over from the 
beginning whenever it finds that the kth item placed is smaller than the (k - l)th item. 
Determine the expected number of times the algorithm tries to place a number, assum-
ing that the input consists of n distinct number". 
Exercise 12.13: Suppose that you are arrangi ng a l'hain of 1/ dominos so that. once 
you are done, you can have them all fall sequentially in a pleasing manner by knocking 
down the lead domino. Each time you try to place a domino in the chain. there is some 
chance that it falls, taking down all of the other domino" you hme already carefully 
placed. In that case, you must start all mer again from the \ery rir"t domino. 
(a) Let us call each time you try to place a domino a trial. Each trial succeeds with 
probability p. Using Wald'.., equation. rind the expected number of trials neces-
sary before your arrangement j" read:. Calculate this number of trials for 1/ = 100 
and p = 0.1. 
(b) Suppose instead that you can break your arrangement into k components, each of 
size n / k, in such a \vay so that once a component is complete, it will not fall when 
you place further dominos. For example: if you have 10 components of size 10, 
then once the first component of 10 dominos are placed successfully they will not 
fall: misplacing a domino later might take down another component, but the first 
will remain ready. Find the expected number of trials necessary before your ar-
rangement is ready in this case. Calculate the number of trials for n = 100, k = 
10, and p = 0.1. and compare \vith your answer from part (a). 
Exercise 12.14: (a) Let XI. X> ... be a sequence of independent exponential random 
variables, each with mean 1. Given a positive real number k, let N be defined by 
IV = minlll: tXi > k}. 
1=1 
That is, N is the smallest number for which the sum of the first N of the Xi is larger 
than k. Use Wald's inequality to determine ErN]. 
(b) Let XI, X 2 , ... be a sequence of independent uniform random variables on the 
interval (0,1). Given a positive real number k with ° 
< k < 1, let N be defined by 
N = minl,,: fI Xi < k}. 
i=1 
311 

MARTINGALES 
That is, N is the smallest number for which the product of the first N of the Xi is 
smaller than k. Determine E[N]. (Hint: You may find Exercise 8.9 helpful.) 
Exercise 12.15: A subsequence of a string s is any string that can be obtained by delet-
ing characters from s. Consider two strings x and y of length n, where each character 
in each string is independently a 0 with probability 1/2 and a I with probability 1/2. 
We consider the longest common subsequence of the two strings. 
(a) Show that the expected length of the longest common subsequence is greater than 
cln and less than C2n for constants CI > 1/2 and C2 < 1 when n is sufficiently 
large. (Any constants CI and C2 will do; as a challenge, you may attempt to find 
the best constants CI and C2 that you can.) 
(b) Use a martingale inequality to show that the length of the longest common subse-
quence is highly concentrated around its mean. 
Exercise 12.16; Given a bag with r red balls and g green balls, suppose that we uni-
formly sample n balls from the bin without replacement. Set up an appropriate martin-
gale and use it to show that the number of red balls in the sample is tightly concentrated 
around nr/(r + g). 
Exercise 12.17: We showed in Chapter 5 that the fraction of entries that are 0 in a 
Bloom filter is concentrated around 
I 
( 
1 )km 
p = 
1- -
, 
11 
where m is the number of data items, k is the number of hash functions, and n is the 
size of the Bloom filter in bits. Derive a similar concentration result using a martingale 
inequality. 
Exercise 12.18: Consider a random graph from Gn. N, where N = cn for some constant 
c > O. Let X be the expected number of isolated vertices (i.e., vertices of degree 0). 
(a) Determine E[X]. 
(b) Show that 
(Hint: Use a martingale that reveals the locations of the edges in the graph, one at 
a time.) 
Exercise 12.19: We improve our bound from the Azuma-Hoeffding inequality for the 
problem where m balls are thrown into n bins. We let F be the number of empty bins 
after the m balls are thrown and Xi the bin in which the ith ball lands. We define Zo = 
E[F] and Zi = E[F I XI, ... , XiJ. 
(a) Let Ai denote the number of bins that are empty after the ith ball is thrown. Show 
that in this case 
312 

12.6 EXERCISES 
( 
I 
/Il-I~I 
ZI_I = A 1 _
I 1- -;; ) 
(b) Show that, if the ith ball lands in a bin that is empty when it is thrown. then 
(c) Show that, if the ith ball lands in a bin that is not empty when it is thrown. then 
( 
1 )111-/ 
Z/=A1_II--;; 
(d) Show that the Azuma-Hoeffding inequality of Theorem 12.6 applies with di 
(1 -
l/n)lII-i. 
(e) Using part (d), prove that 
Pr( IF - E [F] I 2: A) .::S 2e~;h21l-1)/(112_(E[F])2). 
Exercise 12.20: Let f( XI, X 2, ... , XII) satisfy the Lipschitz condition so that, for any 
i and any values XI, ... , XII and .Vi, 
We set 
and 
Give an example to show that, if the Xi are not independent, then it is possible that 
IZi - Zi-II > c. 
313 

CHAPTER THIRTEEN 
Pairwise Independence and 
Universal Hash Functions 
In this chapter we introduce and apply a limited notion of independence, known as 
k-wise independence, focusing in particular on the important case of pairwise indepen-
dence. Applying limited dependence can allow us to reduce the amount of randomness 
used by a randomized algorithm, in some cases enabling us to convert a randomized 
algorithm into an efficient deterministic one. Limited dependence is also used in the 
design of universal and strongly universal families of hash functions, giving space- and 
time-efficient data structures. We consider why universal hash functions are effective 
in practice and show how they lead to simple perfect hash schemes. Finally, we apply 
these ideas to the design of effective and practical approximation algorithms for finding 
frequent objects in data streams, generalizing the Bloom filter data structure introduced 
in Chapter 5. 
13.1. Pairwise Independence 
Recall that in Chapter 2 we defined a set of events E l , E 2 , ••• , En to be mutually inde-
pendent if, for any subset 1 ~ l L 111, 
pr( n E) = n Pr(E;) 
i E I 
iE I 
Similarly, we defined a set of random variables Xl, X 2, ... , Xn to be mutually indepen-
dent if, for any subset I ~ [1, /1] and any values Xi, i E I, 
pr(nX; =x) = nPr(X; =x;). 
lEI 
iEI 
Mutual independence is often too much to ask for. Here, we examine a more limited 
notion of independence that proves useful in many contexts: k-wise independence. 
314 

13.1 PAIRWISE INDEPENDENCE 
Definition 13.1: 
1. A set of events E l , E 2 , .•• , E/1 is k-wise independent ~f, for any subset I C [1,11] 
withIII~k, 
pr( n Ei) = TI Pr(Ei) 
iEi 
iEI 
2. A set of random variables Xl, X 2, ... , X/1 is k-wise independent if, for any subset 
I s;: [l,n] with III ~ k andforan.v values Xi, i E I, 
pr( n 
Xi = x) = TI Pr(Xi = xi!. 
iEi 
iEi 
3. The random variables Xl, X 2, ... , X/1 are said to be pairwise independent if they are 
2-wise independent. That is, for any pair i, j and any values a, b, 
Pr«Xi = a) n (Xj = h)) = Pr( Xi = a) Pr(Xj = b). 
13.1.1. Example: A Construction of Pairwise Independent Bits 
A random bit is uniform if it assumes the values 0 and 1 with equal probability. Here 
we show how to derive Il1 = 2/J - 1 uniform pairwise independent bits from h indepen-
dent, uniform random bits Xl ..... X/J. 
Enumerate the 2h - I non empty subsets of {I. 2 ..... b} in some order. and let Sj be 
the jth subset in this ordering. Set 
})=EBX;, 
iESi 
where EB is the exclusive-or operation. Equivalently, we could write this as 
}) = LXi mod2. 
iESi 
Lemma 13.1: The Yj are painvise independent uniform bits. 
Proof: We first show that, for any nonempty set Sj, the random bit 
Yj = EBXi 
iESj 
is uniform. This follows easily using the principle of deferred decisions (see Sec-
tion 1.3). Let z. be the largest element of S. Then 
315 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
Suppose we reveal the values for Xi for all i E Sj - {z}. Then it is clear that the value 
of X: determines the value of Jj and that Yj will take on the values ° 
and 1 with equal 
probability. 
Now consider any two variables Yk and Yr with their corresponding sets Sk and St. 
Let z be an element of Sf that is not in Sk and consider, for any values c, dE {a, I}, 
Pr (Y/ = d I Y" = c). 
We claim, again by the principle of deferred decisions, that this probability is 1/2. For 
suppose that we reveal the values for Xi for all i in (S" U StJ - {z}. Even though this 
determines the value of Y", the value of X: will determine YI' The conditioning on the 
value of Y" therefore does not change that Y/ is equally likely to be 0 or 1. Hence 
Pr((Y" = c) n (Yt = d)) = Pr(YI = d I Y" = c) Pr(Y" = c) 
= 1/4. 
Since this holds for any values of c, d E {a, I}, we have proven pairwise independence . • 
13.1.2. Application: Derandomizing an Algorithm for Large Cuts 
In Chapter 6, we examined a simple randomized algorithm for finding a large cut in an 
undirected graph G = (V, E): the algorithm places each vertex on one side of the cut 
independently with probability 1/2. The expected value of a cut generated this way is 
m/2, where m is the number of edges in the graph. We also showed (in Section 6.3) 
that this algorithm could be derandomized effectively using conditional expectations. 
Here we present another way to derandomize this algorithm, using pairwise inde-
pendence. This argument exemplifies the approach of derandomization using k-wise 
independence. 
Suppose that we have a collection Y" Y2 , ...• Yn of pairwise independent bits, where 
11 = I V I is the number of vertices in the graph. We define our cut by putting all ver-
tices i with }j = 0 on one side of the cut and all vertices i with }j = 1 on the other 
side of the cut. We show that. in this case. the expected number of edges that crosses 
the cut remains 111/2. That is. we do not require complete independence to analyze the 
expectation; pairwise independence suffices. 
Recall the argument of Section 6.2.1: number the edges from 1 to 111, and let Zi = 1 
if the ith edge crosses the cut and Zi = ° 
otherwise. Then Z = L7~, Zi is the number 
of edges crossing the cut, and 
Let a and b be the two vertices adjacent to the ith edge. Then 
316 

13.1 PAIRWISE INDEPENDENCE 
where we have used the pairwise independence of ~l and Yh . Hence E[Zi] = 1/2, and 
it follows that E[Z 1= 111/2. 
Now let our 11 pairwise independent bits YI ••.•• Yll be generated from b indepen-
dent, uniform random bits XI. X ~ ..... X" in the manner of Lemma 13.1 (here b = 
ilog2(n + l)l)· Then E[Z] = 1ll/2 for the re~ulting cut. where the sample space is 
just all the possible choices for the initial h random bih. By the probabilistic method 
(specifically. Lemma 6.2). there is some setting of the h bih that gi\es a cut with value 
at least 1ll/2. We can try all possible ],,, settings for the bih to tind such a cut. Since 
2h is O( n) and since. for each cut. the number of crossing edges can easily be calcu-
lated in O(m) time. it follows that we can find a cut with at least m/ .2 cros~ing edges 
deterministically in O(nzn) time. 
Although this approach does not appear to be as efficient as the derandomization of 
Section 6.3, one redeeming feature of the scheme is that it is trivial to parallelize. If 
we have sufficiently many processors available, then each of the Q (n) possibilities for 
the random bits XI, X 2, ... , Xh can be assigned to a single processor, with each pos-
sibility giving a cut. The parallelization reduces the running time by a factor of Q (ll) 
using O( 11) processors. In fact, using OC mn) processors, we can assign a processor for 
each combination of a specific edge with a specific sequence of random bits and then 
determine, in constant time, whether the edge crosses the cut for that setting of the ran-
dom bits. After that. only O(log n) time is necessary to collect the results and find the 
large cut. 
13.1.3. Example: Constructing Pairwise Independent Values Modulo a Prime 
We consider another construction that provides pairwise independent values Yo, YI , ••• , 
~)~ I that are uniform over the values {a, 1, ... , p -
I} for a prime p. Our construction 
requires only two independent, uniform values XI and X 2 over {a, 1, ... , p - I}, from 
which we derive 
Yi = XI + iX2 mod p 
for i = 0, .... p -
1. 
Lemma 13.2: The \'ariable.s· Yo. YI , ••• , Yp~1 are pairvvise independent Ull{/orm ran-
dom variables m'e,. {a. L .... p -
I}. 
Proof: It is clear that each ~. is uniform over {a, 1, ... , p - I}, again by applying the 
principle of deferred decisions. Given X2 • the p distinct possible values for XI give p 
distinct possible values for ~ modulo p. each of which is equally likely. 
Now consider any two \'ariables ~. and lj. We wish to show that. for an) (/. h E 
{O, 1, ... , p -
I}, 
1 
Pr ( (~ = (/) n (Yj = b)) = l' 
p-
which implies pairwise independence. The event Yi = (/ and Yj = b is equivalent to 
XI + iX2 = (/ mod p 
and 
XI + jX2 = bmodp. 
This is a system of two equations and two unknowns with just one solution: 
317 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
b -
a 
i(b - a) 
X 2 = -.--. modp 
and 
XI = a -
. 
. modp. 
J -[ 
J-[ 
Since XI and X 2 are independent and uniform over {O, 1, ... , p - 1}, the result follows . • 
This proof can be extended to the following useful result: given 2n independent, uni-
form random bits, one can construct up to 211 pairwise independent and uniform strings 
of n bits. The extension requires knowledge of finite fields, so we only sketch the result 
here. The setup and proof are exactly the same as for Lemma 13.2 except that, instead 
of working modulo p, we perform all arithmetic in a fixed finite field with 211 elements 
(such as the field GF(2
11 ) of all polynomials with coefficients in GF(2) modulo some 
irreducible polynomial of degree n). That is, we assume a fixed one-to-one mapping 
f from strings of n bits, which can also be thought of as numbers in {O, 1, ... ,211 - I}, 
to field elements. We let 
where XI and X 2 are chosen independently and uniformly over {O, I, ... ,211 -I}, i runs 
over the values {O, 1, ... ,211 -
I}, and the addition and multiplication are performed 
over the field. The }j are then pairwise independent. 
13.2. Chebyshev's Inequality for Pairwise Independent Variables 
Pairwise independence is much weaker than mutual independence. For example, we 
can use Chernoff bounds to evaluate the tail distribution of a sum of independent ran-
dom variables, but we cannot directly apply a Chernoff bound if the Xi are only pair-
wise independent. However, pairwise independence is strong enough to allow for easy 
calculation of the variance of the sum, which allows for a useful application of Cheby-
shev's inequality. 
Theorem 13.3: Let X 
variables. Then 
2..::;1=1 Xi, ~t'here the Xi are pairwise independent random 
/I 
Var[X] = L Var[Xi ]. 
i=1 
Proof: We saw in Chapter 3 that 
[ 
1/ 
] 
/I 
Var LXi = LVar[Xi]+2LCov(Xi,Xj), 
i=1 
i=1 
i<j 
where 
Since Xi, X 2 , ... , XI1 are pairwise independent, it is clear (by the same argument as 
in Theorem 3.3) that for any i "# j we have 
E[XiXj ] - E[XiJE[Xj] = O. 
318 

13.2 CHEBYSHEV'S INEQUALITY FOR PAIRWISE INDEPENDENT VARIABLES 
Therefore, 
II 
Varrxl = L Var[Xi]. 
i=1 
• 
Applying Chebyshev's inequality to the sum of pairwise independent variables yields 
the following. 
Corollary 13.4: Let X = L ;1=1 Xi. where the Xi are pairwise independent random 
variables. Then 
Var[X] 
Pr ( I X - E [ X] I 2: a) .:s 
.., 
a-
L;I=1 Var[Xi ] 
a 2 
13.2.1. Application: Sampling Using Fewer Random Bits 
We apply Chebyshev's inequality for pairwise independent random variables to obtain 
a good approximation through sampling. This uses less randomness than the natural 
approach based on Chernoff bounds. 
Suppose that we have a function f: {O, I}II ---+ [0, I J mapping n-bit vectors into real 
numbers. Let I = (LxEiQ.1}1I f(x) )/2
11 be the average value of f. We wantto compute 
a I - 8 confidence interval for j. That is, we wish to find an interval [] -
£, ] + £] 
such that 
Pr (I E [] -
£, ] + £]) 2: I - 8. 
As a concrete example, suppose that we have an integrable function g: [0, 1] ---+ 
[0, I] and that the derivative of g exists with Ig'(x)1 .:s C for some fixed constant C 
over the entire interval (0, I). We are interested in .f}=o g(x) dx. There may be no di-
rect way to compute this integral exactly, but through sampling we can obtain a good 
estimate. If X is a uniform random variable on [0,1], then Elg(X)] = L
1=o g(x) dx 
by the definition of the expectation of a continuous random variable. By taking the av-
erage of multiple independent samples, we can approximate the integral. If our source 
of randomness generates only random bits instead of random real numbers, then we 
might approximate the integral as follows. For a string of bits x E {O, I }11, we may inter-
pret x as a real number .t E [0. I] by considering it as a decimal in binary: for example. 
11001 would correspond to 0.11001 = 25/32. Let fCr) denote the value of the func-
tion g at the decimal value .t. Then. for any integer i with ° :s i < 211 -
L for y E 
[i/2
11
, (i + 1)/211) we have 
(i) 
C 
(i) 
C 
I ,-
- -
:s ~(\") :s r -
~-
211 
211 
L. 
'211 
211 . 
It follows that 
~ ~ (I(X) - £) :s JI ~(x)dx.:s ~ ~ (((X) + £). 
211 
L 
211 
L 
III L· 
III 
XE(O.I}1I 
,\=(J 
-n=((),W 
-
By taking n sufficiently large. we can guarantee that I = (L\E(O,I}" f(x))/2
11 dif-
fers from the integral of g by at most a constant y. In this case. a confidence interval 
319 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
[j - £, j + £] for j would yield a confidence interval [g - £ - y, g + £ + y] for the 
integral of g. 
We could handle the problem of finding a confidence interval for the average value 
j by using independent samples and applying a Chernoff bound. That is, suppose that 
we sample uniformly with replacement random points in {O, 1 }11, evaluate f at all of 
these points, and take the average of our samples. This is similar to the parameter esti-
mation of Section 4.2.3. Theorem 13.5 is an immediate consequence of the following 
Chernoff bound, which can be derived using Exercises 4.13 and 4.19. If Zl, Z2, ... , Zm 
are independent, identically distributed, real-valued random variables with mean f1 that 
take on one of a finite possible set of values in the range [0,1], then 
pr( It z, - mILl C> Bm ) :s 2c-2",,''. 
1=1 
Theorem 13.5: Let f: {O, l}1l ---+ [0,1] and j = (LXE{O,I}1I f(x) )/2 11. Let Xl,"" XIII 
be chosen independently and un~formly at random from {O, l}/{. Ifm > In(2/8)/2£2, 
then 
Although the exact choice of m depends on the Chernoff bound used, in general 
this straightforward approach requires Q (In(l/8)/£2) samples to achieve the desired 
bounds. 
A possible problem with this approach is that it requires a large number of random 
bits to be available. Each sample of f requires 11 independent bits, so applying The-
orem 13.5 means that we need at least Q(n In(l/8)/£2) independent, uniform random 
bits to obtain an approximation that has additive error at most £ with probability at least 
1 - 8. 
A related problem arises when we need to record how the samples were obtained, so 
that the work can be reproduced and verified at a later time. In this case, we also need 
to store the random bits used for archival purposes. In this case, using fewer random 
bits would lessen the storage requirements. 
We can use pairwise independent samples to obtain a similar approximation using 
less randomness. Let Xl, ... , XIll be pairwise independent points chosen from {O, l}1I, 
and let Y = 
(L:l~l !(Xi ))/I11. Then E[Y] = f, and we can apply Chebyshev's in-
equality to obtain 
-
Var[Y] 
Pr ( I Y - ! I ~ £) :s 
I 
£-
Var[ (L~I~1 f(X i ) )/111 ] 
I 
£-
L~I~1 Var[f( Xi )] 
111 2£2 
m 
1 
<--=--
-
111 2£2 
m£2' 
320 

13.3 FAMILIES OF UNIVERSAL HASH FUNCTIONS 
since Var[f(Xi)J :s E[(f(Xi))2] :s 1. We therefore find Pr(IY - /1 2: £) :s 6 when 
m = 1/6£2. (In fact. one can prove that Var[f(Xi)] :s 1/4, giving a slightly better 
bound; this is left as Exercise 13.4.) 
Using pairwise independent samples requires more samples: E-)(l/ 6£2) instead of 
the ~)(ln(l/6)/£2.) samples when they are independent. But recall from Section 13.1.3 
that we can obtain up to 211 pairwise independent samples with just 2n uniform inde-
pendent bits. Hence, as long as 1/6£2 < 2.
11
• just 2.n random bits suffice; this is much 
less than the number required when using completely independent samples. Usually £ 
and 6 are fixed constants independent of n. and this type of estimation is quite efficient 
in terms of both the number of random bits used and the computational cost. 
13.3. Families of Universal Hash Functions 
Up to this point. when studying hash functions \ve modeled them as being completely 
random in the sense that. for any collection of items X\, X2, ... , Xb the hash values 
h (xd, h (X2), ... , h Crd were considered uniform and independent over the range of 
the hash function. This was the framework we used to analyze hashing as a balls-and-
bins problem in Chapter 5. The assumption of a completely random hash function 
simplifies the analysis for a theoretical ... tudy of hashing. In practice, however, com-
pletely random hash functions are too expensive to compute and store, so the model 
does not fully reflect reality. 
Two approaches are commonly used to implement practical hash functions. In many 
cases, heuristic or ad hoc functions designed to appear random are used. Although these 
functions may work suitably for ",ome applications. they generally do not have any as-
sociated provable guarantees. making their use potentially risky. Another approach is 
to use hash functions for which there are some provable guarantees. We trade away the 
strong statements one can make about completely random hash functions for weaker 
statements with hash functions that are efficient to store and compute. 
We consider one of the computationally simplest classes of hash functions that pro-
vide useful provable performance guarantees: universal families of hash functions. 
These functions are \videly used in practice. 
Definition 13.2: Le! U be a uni\'erse H'i!/Z I U I 2: n and Ie! V = {O. 1 .. , .. n -
I}, A 
family qf hash functions H .limn C !o V is said !o be k-universal (I: f(JI' any elelllell!S 
X\,X2, ... ,x" andfora !ws/z.fitllc!ion/z c/zosen unllorlnlyo! mndolllfi'Oln H. \\'e /zore 
1 
Pr ( Iz ( X \ ) = /z ( X 2. ) = . , , = /z ( x t;}) :s t:=I' 
n 
A famil.v qf hash functions H frolll U !o V is said to be strongly k-universallf, for an,V 
elements x\, X2, ... , Xb any l'olues y \. Y 2. •• , " y" E {o. L ' ... n - I}. and a !wshfunction 
h chosen un(formly at randolllfi'(mi H. H'e !w\'e 
1 
Pr (( h (x d = y d n (/1 (x 2.) = Y 2. ) n ... n (h (x d = yt;}) = k' 
n 
321 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
We will primarily be interested in 2-universal and strongly 2-universal families of hash 
functions. When we choose a hash function from a family of2-universal hash functions, 
the probability that any two elements XI and X::! have the same hash value is at most lin. 
In this respect, a hash function chosen from a 2-universal family acts like a random 
hash function. It does not follow, however, that for 2-universal families the probability 
of any three values XI, X::!, and X3 having the same hash value is at most lin::!, as would 
be the case if the hash values of XI, X2, and X3 were mutually independent. 
When a family is strongly 2-universal and we choose a hash function from that fam-
ily, the values h (XI) and h (X2) are pairwise independent, since the probability that they 
take on any specific pair of values is l/n::!. Because of this, hash functions chosen from 
a strongly 2-universal family are also known as pairwise independent hash functions. 
More generally, if a family is strongly k-universal and we choose a hash function from 
that family, then the values h(xI), h(x::!), ... , h(xd are k-wise independent. Notice that 
a strongly k-universal hash function is also k-universal. 
To gain some insight into the behavior of universal families of hash functions, let us 
revisit a problem we considered in the balls-and-bins framework of Chapter 5. We saw 
in Section 5.2 that, when n items are hashed into 11 bins by a completely random hash 
function, the maximum load is CO-;) (log n / log log n) with high probability. We now con-
sider what bounds can be obtained on the maximum load when n items are hashed into 
n bins using a hash function chosen from a 2-universal family. 
First, consider the more general case where we have m items labeled XI, X::!, ... , XII/' 
For lsi < j S m, let Xi) = 1 if items Xi and x) land in the same bin. Let X = 
L lsi <j sm Xi) be the total number of collisions between pairs of items. By the linear-
ity of expectations, 
EfXl = E[ L xu] = L EfXul 
ISI<JSIl1 
ISi<j-S1l1 
Since our hash function is chosen from a 2-universal family, it follows that 
and hence 
") 
m-
E[X] s ( '211) -;;1 <-. 
2n 
Markov's inequality then yields 
( 
') 
IIr 
Pr X ~ ----;; 
1 
S Pr (X ~ 2 E [X]) s 2:' 
(l3.1) 
If we now suppose that the maximum number of items in a bin is Y, then the number 
of collisions X must be at least (~). Therefore, 
(( Y) 
m'2 ) 
( 
m'2 ) 
Pr 
2 
~ ----;; s Pr X ~ ----;; 
I 
<-
-
2' 
which implies that 
322 

13.3 FAMILIES OF UNIVERSAL HASH FUNCTIONS 
In particular, in the case where m = n, the maximum load is at most vf2n with proba-
bility at least 1/2. 
This result is much weaker than the one for perfectly random hash functions, but it 
is extremely general in that it holds for any 2-unh'ersal family of hash functions. The 
result will prove useful for designing perfect hash functions, as we describe in Sec-
tion 13.3.3. 
13.3.1. Example: A 2-Universal Family of Hash FUllctions 
Let the universe U be the set {O, 1. 2 ..... III -
I} and let the range of our hash func-
tion be V = {a, 1,2, . .. ,n -
I}, with III :::: 1/. Consider the family of hash functions 
obtained by choosing a prime p :::: Ill. letting 
hi/./J(x) = ((ax ~ h) mod p) modI/ 
and then taking the family 
H = {hi/h 11 :s a :s p - L O:s b :s pl· 
Notice that a cannot here take on the \, .. tlue 0. 
Lemma 13.6: H is 2-1111i\'(:'rsal. 
Proof' We count the number of functions in H for which two distinct elements XI and 
X2 from U collide. 
First we note that. for any XI "# X:? 
a X I + b "# a x:? + b mod p. 
This follows because aXI + h = ax:: + b mod p implies that a(xi -
x::) = ° 
mod p, 
yet here both a and (x I -
x::) are nonzero modulo p. 
In fact, for every pair of values ([{. v) such that u "# v and ° 
:s u, v :s p -
1, there 
exists exactly one pair of values (a. b) for which aXI + b = u mod p and ax:: + b = 
v mod p. This pair of equations has two unknowns, and its unique solution is given by: 
V-l{ 
a= 
modp, 
h = [{ - ax I mod p. 
Since there is exactly one hash function for each pair (a. b), it follows that there is ex-
actly one hash function in H for which 
aXI + b = [( mod p 
and 
ax:: + b = v mod p. 
Therefore, in order to bound the probability that hu,h(XI) = h(/,/1(.\'::') when hi/,/) is 
chosen uniformly at random from H. it suffices to count the number of pairs (u, v), 
° :s u, v :s p -
1, for which u "# v but u = v modn. For each choice of u there are 
323 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
at most I pin l - I possible appropriate values for v, giving at most p(l pin l - 1) < 
pep - 1)ln pairs. Each pair corresponds to one of pep -
1) hash functions, so 
pep -
1)ln 
Pr(ha.h(XI) = h a.b(X2)) ~ ----
pep -
1) 
proving that H is 2-universal. 
n 
13.3.2. Example: A Strongly 2-Universal Family of Hash Functions 
• 
We can apply ideas similar to those used to construct the 2-universal family of hash 
functions in Lemma 13.6 to construct strongly 2-universal families of hash functions. 
To start, suppose that both our universe U and the range V of the hash function are 
{a, 1,2, ... , p -
I} for some prime p. Now let 
hu.b(x) = (ax + b) mod p, 
and consider the family 
H = {h({.b I ° 
~ a, b ~ p - I}. 
Notice that here a can take on the value 0, in contrast with the family of hash functions 
used in Lemma 13.6. 
Lemma 13.7: H is strongly 2-universal. 
Proof' This is entirely similar to the proof of Lemma 13.2. For any two elements XI 
and X2 in U and any two values YI and Y2 in V, we need to show that 
The condition that both ha.h(xd = ,vI and h({.b(X2) = Y2 yields two equations mod-
ulo p with two unknowns, the values for a and b: aXI + b = YI mod p and aX2 + b = 
,v2 mod p. This system of two equations and two unknowns has just one solution: 
VI -
VI 
a = . -
. 
modp, 
.r2 -
XI 
b =,vl -axi modp. 
Hence only one choice of the pair (a,b) out of the p2 possibilities results in XI and Xl 
hashing to ,v I and Y2, proving that 
as required. 
• 
Although this gives a strongly 2-universal hash family, the restriction that the universe 
U and the range V be the same makes the result almost useless; usually we want to 
hash a large universe into a much smaller range. We can extend the construction in a 
natural way that allows much larger universes. Let V = {a, 1,2, ... , p - I}, but now 
324 

13.3 FAMILIES OF UNIVERSAL HASH FUNCTIONS 
let U = {O, 1, 2, ... , pk -
I} for some integer k and prime p. We can interpret an ele-
ment U in the universe U as a vector ii = (uo, lll •...• llk~I)' where 0 :s ll/ .:s p - 1 for 
o :s i .:s k - 1 and where L~==-(\ Uipi = u. In fact. this gives a one-to-one mapping 
between vectors of this form and elements of U. 
For any vector a = (a(), al,.'" ak~d with 0 .:s a/ :s p -
1, ° :s i .:s k -
1. and for 
any value b with 0 .:s b .:s p -
I, let 
k~1 
ha.,,(u) = (Lailli + b) modp, 
1=0 
and consider the family 
H = {h({.h IO.:s ai,b.:s P - 1 for all O.:s i .:s k -I}. 
Lemma 13.8: H is strongly 2-universal. 
Proof: We follow the proof of Lemma 13.7. For any two elements UI and U2 with cor-
responding vectors Ui = (Ui,O, Ui,I,"" Ud~l) and for any two values YI and Y2 in V, 
we need to show that 
Since U I and U 2 are different, they must differ in at least one coordinate. Without 
loss of generality let U 1.0 #- U 2.0. For any given values of a I, a2, ... , ak~ I, the condition 
that hii,b(ud = YI and h({,b(U2) = Y2 is equivalent to: 
k~1 
aoul,O +h = (.)'1 - LajUl.j) modp 
./=1 
( 
k~1 
) 
aOu2.0 + b = 
YI - L aj U2.j mod p . 
./=1 
For any given values of 0 1,02, ... , a~ ~ I, this gives a system with two equations and 
two unknowns (namely. (/0 and b), which - as in Lemma 13.8 - has exactly one solu-
tion. Hence, for every 0 I, (/2 •.. ', (/k~ I. only one choice of the pair (00. b) out of the p2 
possibilities results in II I and ll;. hashing to Y I and Y2, proving that 
as required. 
• 
Although we have described both the 2-universal and the strongly 2-uni\'ersal hash fam-
ilies in terms of arithmetic modulo a prime number. \ve could extend these techniques 
to work over general finite fields - in particular. fields with 211 elements represented 
by sequences of 11 bits. The extension requires knowledge of finite fields, so we just 
sketch the result here. The setup and proof are exactly the same as for Lemma 13.8 ex-
cept that, instead of working modulo p, we perform all arithmetic in a fixed finite field 
325 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
with 2" elements. We assume a fixed one-to-one mapping f from strings of n bits, 
which can also be thought of as numbers in {a, 1, ... ,2 11 - I}, to field elements. We let 
k-I 
h",,(u) = r' (L J(a;) . f(u;) + f(h)), 
i=() 
where the ai and b are chosen independently and uniformly over {a, 1, ... ,2
11 
-
I} and 
where the addition and multiplication are performed over the field. This gives a strongly 
2-universal hash function with a range of size 211. 
13.3.3. Application: Perfect Hashing 
Perfect hashing is an efficient data structure for storing a static dictionary. In a static 
dictionary, items are permanently stored in a table. Once the items are stored, the table 
is used only for search operations: a search for an item gives the location of the item 
in the table or returns that the item is not in the table. 
Suppose that a set 5 of m items is hashed into a table of n bins, using a hash func-
tion from a 2-universal family and chain hashing. In chain hashing (see Section 5.5.1), 
items hashed to the same bin are kept in a linked list. The number of operations for 
looking up an item x is proportional to the number of items in x's bin. We have the 
following simple bound. 
Lemma 13.9: Assume that m elements are hashed into an n-bin chain hashing table 
by using a hash function h chosen uniformly at random from a 2-universalfamily. For 
an arbitrary element x, let X be the number of items at the bin h (x). Then 
E[X] ::: I 
m/ll 
1 + (m -
1)/11 
if x ~ 5, 
if XES. 
Proof: Let Xi = 1 if the ith element of 5 (under some arbitrary ordering) is in the 
same bin as x and ° 
otherwise. Because the hash function is chosen from a 2-universal 
family, it follows that 
Pr ( Xi = 1) = 1/ n . 
Then the first result follows from 
[ 
//I 
] 
m 
E[X] = E LXi = L E[X;] ::: :' 
i=1 
i=1 
where we have used the universality of the hash function to conclude that E[Xi ] < 
l/n. Similarly, if x is an element of 5 then (without loss of generality) let it be the first 
element of S. Hence XI = 1, and again 
Pr(Xi = 1) = lin 
when i -# 1. Therefore, 
[
1/1] 
111 
m-l 
E[X] = E LXi = 1 + LE[Xi ]::: 1 + -n-' 
i=1 
i=2 
• 
326 

13.3 FAMILIES OF UNIVERSAL HASH FUNCTIONS 
Lemma 13.9 shows that the average performance of hashing when using a hash func-
tion from a 2-universal family is good. since the time to look through a bin of any 
item is bounded by a small number. For instance. if m = n then. when searching the 
hash table for x. the expected number of items other than x that must be examined is 
at most 1. However. this does not give us a bound on the worst-case time of a lookup, 
Some bin may contain Jli elements or more. and a search for one of these elements 
requires a much longer lookup time. 
This motivates the idea of perfect hashing, Gi\en a set S. we would like to construct 
a hash table that gives excellent worst-case performance. Specifically. by perfect hash-
ing we mean that only a constant number of operations are required to find an item in 
a hash table (or to determine that it isn't there" 
We first show that perfect hashing is easy if we are given sufficient space for the 
hash table and a suitable 2-universal family of hash functions. 
Lemma 13.10: If h E H is chosen lllli{fnmly at random fl'om a 2-universal family qf 
hash functions mapping the lllli\'(!ne C to [O.n -
1] theil, for an)' set S C U of size m, 
the probability q{h being peliect is at least I .2 \I'hen 11 ~ m 2. 
Proof: Let s" S2 • .... Sill be the m items of S. Let Xii be 1 if the h (Si) = h (Sj) and 
o otherwise. Let X = L'~I< 1'--"1 XI/' Then. as we saw in Eqn. (13.1), the expected 
number of collisions when using a .2-uni\'ersal hash function is 
E[X] = E[ I: Xu] = I: E[ Xill ~ 
'''-I < ''--''1 
'Si <./SIII 
Markov's inequality then yields 
( 
m~ ) 
Pr X ~ ----;;-
1 
:s Pr( X ~ 2E[X]) :s 2:' 
Hence, when n ~ 1112. we find X < I with probability at least 1/2. This implies that a 
randomly chosen hash function is perfect with probability at least 1/2. 
• 
To find a perfect hash function when n ~ nr2, we may simply try hash functions chosen 
uniformly at random from the .2-universal family until we find one with no collisions. 
This gives a Las Vegas algorithm. On average we need to try at most two hash functions. 
We would like to have perfect hashing without requiring space for Q (m 2 ) bins to 
store the set of m items. We can use a two-level scheme that accomplishes perfect 
hashing using only D( 111) bins. First. we hash the set into a hash table with 171 bins us-
ing a hash function from a 2-universal family. Some of these bins will have collisions. 
For each such bin, we provide a second hash function from an appropriate 2-universal 
family and an entirely separate second hash table. If the bin has k > I items in it then 
we use k2 bins in the secondary hash table. We have already shown in Lemma 13.10 
that with k 2 bins we can find a hash function from a 2-universal family that will give 
no collisions. It remains to show that, by carefully choosing the first hash function, we 
can guarantee that the total space used by the algorithm is only D( m). 
327 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
Theorem 13.11: The two-level approach gives a perfect hashing scheme for m items 
using O(m) bins. 
Proof' As we showed in Lemma 13.10, the number of collisions X in the first stage 
satisfies 
( 
m2) 
I 
Pr X ~ ----;; 
:::; Pr (X ~ 2 E [X]) :::; 2' 
When n = m, this implies that the probability of having more than m collisions is at 
most 1/2. Using the probabilistic method, there exists a choice of hash function from 
the 2-universal family in the first stage that gives at most m collisions. In fact, such 
a hash function can be found efficiently by trying hash functions chosen uniformly 
at random from the 2-universal family, giving a Las Vegas algorithm. We may there-
fore assume that we have found a hash function for the first stage that gives at most m 
collisions. 
Let Ci be the number of items in the ith bin. Then there are Ci) collisions between 
items in the ith bin, so 
For each bin with Ci > 1 items, we find a second hash function that gives no collisions 
using space C? Again, for each bin, this hash function can be found using a Las Vegas 
algorithm. The total number of bins used is then bounded above by 
//I 
/II () 
/II 
m+Lc}:::;m+2L ~ +L ci:::;m+2m+m=4m. 
i=l 
i=l 
i=l 
Hence, the total number of bins used is only Oem). 
• 
13.4. Application: Finding Heavy Hitters in Data Streams 
A router forwards packets through a network. At the end of the day, a natural ques-
tion for a network administrator to ask is whether the number of bytes traveling from 
a source 51 to a destination d that have passed through the router is larger than a prede-
termined threshold value. We call such a source-destination pair a heav}' hitter. 
When designing an algorithm for finding heavy hitters, we must keep in mind the 
restrictions of the router. Routers have very little memory and so cannot keep a count 
for each possible pair 51 and d, since there are simply too many such pairs. Also, routers 
must forward packets quickly, so the router must perform only a small number of com-
putational operations for each packet. We present a randomized data structure that is 
appropriate even with these limitations. The data structure requires a threshold q; all 
source-destination pairs that are responsible for at least q total bytes are considered 
heavy hitters. Usually q is some fixed percentage, such as 1%, of the total expected 
daily traffic. At the end of the day, the data structure gives a list of possible heavy hit-
ters. All true heavy hitters (responsible for at least q bytes) are listed, but some other 
328 

13.4 APPLICATION: FINDING HEAVY HITTERS IN DATA STREAMS 
pairs may also appear in the list. Two other input constants. F and 6. are used to control 
what extraneous pairs might be put in the list of hea\'y hitters. Suppose that Q repre-
sents the total number of bytes over the course of the day. Our data structure has the 
guarantee that any source-destination pair that constitutes less than q - EQ bytes of 
traffic is listed with probability at most (5. In other \vords. all heavy hitters are listed: 
all pairs that are sufficiently far from being a hea\y hitter are listed with probability at 
most 8: pairs that are close to heavy hitters mayor may not be listed. 
This router example is typical of man) situations where one wants to keep a suc-
cinct summary of a large data stream. In most data stream models, large amounts of 
data arrive sequentially in small blocks. and each block must be processed before the 
next block arrives. In the setting of net\\ork routers. each block is generally a packet. 
The amount of data being handled is often SO large and the time between arrivals is so 
small that algorithms and data structures that use only a small amount of memory and 
computation per block are required. 
We can use a variation of a Bloom tilter. discussed in Section 5.5.3, to solve this 
problem. Unlike our solution there. \\hkh assumed the availability of completely ran-
dom hash functions. here \\e obtain strong. pro\'able bounds using only a family of 
2-universal hash functions. This is important. because efficiency in the router setting 
demands the use of only \ery simple hash functions that are easy to compute. yet at the 
same time we want pro\'able performance guarantees. 
We refer to our data structure as a (Ollllf-mill filter, The count-min filter processes 
a sequential stream of pair" X·I'X~ .. " of the form XI = (i1.eIL where il is an item 
and C I > ° 
is an integer count increment. In our routing setting, il would be the pair 
of source-destination addresses of a packet and ('I would be the number of bytes in the 
packet. Let 
Count(i.T)= 
li,=i,I~I~T 
That is, Count (i. T) is the total count associated with an item i up to time T. In the 
routing setting. Count (i. T) \\ould be the total number of bytes associated with packets 
with an address pair i up to time T. The count-min filter keeps a running approxima-
tion of Count(i. T) for all items i and all times T in such a way that it can track heavy 
hitters. 
A count-min filter consist of m counters, We assume henceforth that our counters 
have sufficiently many bits that \\e do not need to worry about O\·erfto\\: in many prac-
tical situations. 32-bit counters \\ill suffice and are comenient for implementation, A 
count-min filter uses k hash functions, \Ve split the counters into k di"joint groups 
G I, G'2 . ... , Gk of size m/ k, For comenience. we assume in \\hat follO\\s that k di-
vides 111 evenly. We label the counters by C" !' where 1 :s (/ :s k and 0 :s j ~ m / k - L 
so that Cu.) corresponds to the jth counter in the ath group, That is. \\ C L'an think of 
our counters as being organized in a 2-dimensional array. \\ith m/ k counter" per row 
and k columns. Our hash functions should map items from the uni\crse into counters. 
so we have hash functions Hu for 1 :s a :s k. where H(/: C -----+ [0. 1lI/ k -
11. That is. 
each of the k hash functions takes an item from the uni\'erse and map" it into a num-
ber [0,111/ k -
1]. Equivalently. we can think of each hash function as taking an item 
329 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
i and mapping it to the counter Ca, H,,(i)' The Ha should be chosen independently and 
uniformly at random from a 2-universal hash family. 
We use our counters to keep track of an approximation of Count(i, T). Initially, all 
the counters are set to O. To process apair (i1,cIL we compute Ha(il) for each a with 
I :s a :s k and increment Ca,H,,(irl by CI • Let Ct.j(T) be the value of the counter Ca.j 
after processing XI through X T. We claim that, for any item, the smallest counter as-
sociated with that item is an upper bound on its count, and with bounded probability 
the smallest counter associated with that item is off by no more than c times the to-
tal count of all the pairs (it, c l ) processed up to that point. Specifically, we have the 
following theorem. 
Theorem 13.12: For any i in the universe U andfor any sequence (iI, cd, ... , (iT, cd, 
min 
Ca.j(T) ~ Count(i, T). 
j=HlI(i),I"Sa"Sk 
Furthermore, with probability I - (k/mc)k over the choice (~f hashfimctions, 
T 
min 
Cu.j(T) :s Count(i, T) + c LCI ' 
j= HlI(i).I"Su"Sk 
' 
t=1 
Proof' The first bound, 
min 
Ca. jeT) ~ Count(i, T), 
j=H,,(i).I"ScI"Sk 
' 
is trivial. Each counter Ca.) with j = Ha(i) is incremented by C I when the pair (i, c t ) is 
seen in the stream. It follows that the value of each such counter is at least Count(i, T) 
at any time T. 
For the second bound, consider any specific i and T. We first consider the specific 
counter Cl. H[(i) and then use symmetry. We know that the value of this counter is at 
least Count (i. T) after the first T pairs. Let the random variable Z I be the amount the 
counter is incremented owing to items other than i. Let XI be a random variable that is 
I if il -# i and HI (il) = HI (i): XI is 0 otherwise. Then 
1:I"St"S T.il f=.i 
1=1 
H[(irl=H[(i) 
Because HI is chosen from a 2-universal family, for any il -# i we have 
and hence 
It follows that 
k 
ElXtJ:s-. 
m 
T 
T 
T 
E[Z,l = E[ L X,c, ] = L c,E[X,] .:s ; L C, 
1=1 
1=1 
1=1 
330 

13.4 APPLICATION: FINDING HEAVY HITTERS IN DATA STREAMS 
By Markov's inequality, 
pr(ZI 0> F i>,) 
1=1 
kim 
k 
<--=-
E 
mE 
(13.2) 
Let Z2, Z3, ... , Zk be corresponding random variables for each of the other hash func-
tions. By symmetry, all of the Zi satisfy the probabilistic bound of Eqn. (13.2). More-
over, the Zi are independent, since the hash functions are chosen independently from 
the family of hash functions. Hence 
( k )'" 
:s 
~ . 
(13.3 ) 
(13.4) 
• 
It is easy to check using calculus that (k/mE)~ is minimized when k = mE/e. in which 
case 
( k )'" = e - 1>1' 
C 
mE 
Of course, k needs to be chosen so that k and m. k are integer". but thi" does not sub-
stantially affect the probability bounds. 
We can use a count-min filter to track hea\y hitters in the routing setting as follows. 
When a pair (iT, cr) arri\'es. we update the count-min filter. If the minimum hash 
value associated with iT is at least the threshold q for heavy hitters, then we put the 
item into a list of potential hem'y hitters. We do not concern ourselves with the details 
of performing operations on this list. but note that it can be organized to allow updates 
and searches in time logarithmic in its size by using standard balanced search-tree data 
structures; alternatively. it could be organized in a large array or a hash table. 
Recall that we use Q to represent the total traffic at the end of the day. 
Corollary 13.13: Suppose tizat ~\'e use a count-min filter with k = lIn t l hash func-
tions, m = lIn * l . I ~ l counters. and a threshold q. Then all heav:v hitters are put on 
the list, and any source-destinatiol1 pair that corresponds to fewer than q - EQ bytes 
is put on the list with probability at most 8. 
Proof· Since counts increase over time. we can simply consider the situation at the end 
of the day. By Theorem 13.12. the count-min filter will ensure that all true heavy hitters 
are put on the list, since the smallest counter value for a true heavy hitter will be at least 
q. Further, by Theorem 13.12, the smallest counter value for any source-destination 
pair that corresponds to fewer than q - EQ bytes reaches q with probability at most 
k 
(~) :s e- 1n (l/8) = 8. 
mE 
331 
• 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
(i,3) 
3 8 
5 
1 
3 
8 
5 
1 
4 
8 
3 
2 
7 
8 
3 
2 
3 
0 
5 
9 
3 
0 
7 
9 
Figure 13.1: An item comes in, and 3 is to be added to the count. The initial state is on the left; the 
shaded counters need to be updated. Using conservative update, the minimum counter value 4 deter-
mines that all corresponding counters need to be pushed up to at least 4 + 3 = 7. The resulting state 
after the update is shown on the right. 
The count-min filter is very efficient in terms of using only limited randomness in its 
hash functions, only 0 (~ In ±) counters, and only 0 (1n *) computations to process 
each item. (Additional computation and space might be required to handle the list of 
potential heavy hitters, depending on its representation.) 
Before ending our discussion of the count-min filter, we describe a simple improve-
ment known as conservative update that often works well in practice, although it is 
difficult to analyze. When a pair (if, cf ) arrives, our original count-min filter adds Cf to 
each counter CCI.) that the item i hashes to, thereby guaranteeing that 
min 
Cu.j(T) ~ CountU, T) 
j=H(/(il.l::::u::::k 
holds for all i and T. In fact, this can often be guaranteed without adding Cf to each 
counter. Consider the state after the (t -
l)th pair has been processed. Suppose that. 
inductively, up to that point we have. for all i. 
min 
Co.jU -
1) ~ CountU, t -
1). 
j=H({(i).I::::u::::k 
Then. when (if' ( 1 ) arrives. we need to ensure that 
Cu.j(t) ~ CountUf, t) 
for all counters, where j = H(l(if)' a :s 1 :s k. But 
CountUI,t) = Count(if,t -1) + C1 :s 
min 
Cu.j(t -1) +Cf. 
J=H,,(i[ ).I::::o::::k 
. 
Hence we can look at the minimum counter value v obtained from the k counters that 
if hashes to, add Cf to that value, and increase to v + Cf any counter that is smaller than 
v + Cf' An example is given in Figure 13.1. An item arrives with a count of 3; at the 
time of arrival. the smallest counter associated with the item has value 4. It follows that 
the count for this item is at most 7, so we can update all associated counters to ensure 
they are all at least 7. In general, if all the counters if hashes to are equal, conservative 
update is equivalent to just adding C1 to each counter. When the il are not all equal, the 
conservative update improvement adds less to some of the counters. which will tend to 
reduce the errors that the filter produces. 
332 

13.5 EXERCISES 
13.5. Exercises 
Exercise 13.1: A fair coin is flipped n times. Let Xii' with 1 :s i < j :s 11, be 1 if the 
ith and jth flip landed on the same side; let Xij = 0 otherwise. Show that the Xij are 
pairwise independent but not independent. 
Exercise 13.2: (a) Let X and Y be numbers that are chosen independently and uni-
formly at random from (0. L .... 11}. Let Z be their sum modulo 11 + 1. Show that X, Y, 
and Z are pairwise independent but not independent. 
(b) Extend this example to give a collection of random variables that are k-wise in-
dependent but not (k + 1 )-v .. ise independent. 
Exercise 13.3: For any family of hash functions from a finite set V to a finite set V, 
show that. when Iz is chosen at random from that family of hash functions, there exists 
a pair of elements x and Y sLlch that 
1 
1 
Pr(/1(.\") = h(\')) > -
-
-
. 
. 
- IVI 
IVI 
This result should not depend on how the function h is chosen from the family. 
Exercise 13.4: Show that. for any discrete random variable X that takes on values in 
the range [0,1]. Var[X I :s 1/-1-. 
Exercise 13.5: Suppose we have a randomized algorithm Test for testing whether a 
string appears in a language L that works as follows. Given an input x, the algorithm 
Test chooses a random integer r uniformly from the set 5 = {O, 1, ... , p -
I} for some 
prime p. If x is in the language. then Test (x,r) = 1 for at least half of the possible val-
ues of r. A value of r such that Test(x,r) = 1 is called a 'rvitness for x. If x is not in the 
language, then Test(x. r) = 0 always. 
If we run the algorithm Test tv.;ice on an input x E L by choosing two numbers r, 
and r2 independently and uniformly from 5 and evaluating Test(x, r,) and Test(x, r2). 
then we find a witness with probability at least 3/4. Argue that we can obtain a witness 
with probability at least 1 - 1/[ using the same amount of randomness by letting Si = 
r,i + r2 mod p and evaluating Test(x. Sf) for values 0 :s i :s t < p. 
Exercise 13.6: Our analysis of Bucket sort in Section 5.2.2 assumed that 11 elements 
were chosen independently and uniformly at random from the range [0.2'" ). Suppose 
instead that n elements are chosen uniformly from the range [0.2'" ) in such a way that 
they are only pairwise independent. Show that, under these conditions. Bucket sort 
still requires linear expected time. 
Exercise 13.7: (a) We have shown that the maximum load when 11 items are hashed 
into n bins using a hash function chosen from a 2-universal family of hash functions 
is at most v'2i1 with probability at least 1/2. Generalize this argument to k-universal 
333 

PAIRWISE INDEPENDENCE AND UNIVERSAL HASH FUNCTIONS 
hash functions. That is, find a value such that the probability that the maximum load is 
larger than that value is at most 1/2. 
(b) In Lemma 5.l we showed that, under the standard balls-and-bins model, the 
maximum load when n balls are thrown independently and uniformly at random into 
n bins is at most 31nnllnlnn with probability 1 - lin. Find the smallest value of k 
such that the maximum load is at most 31n n lIn In n with probability at least 1/2 when 
choosing a hash function from a k-universal family. 
Exercise 13.8: We can generalize the problem of finding a large cut to finding a large 
k-cut. A k-cut is a partition of the vertices into k disjoint sets, and the value of a cut is 
the weight of all edges crossing from one of the k sets to another. In Section 13.1.2 we 
considered 2-cuts when all edges had the same weight 1, and we showed how to deran-
domize the standard randomized algorithm using collections of n pairwise independent 
bits. Explain how this derandomization could be generalized to obtain a polynomial 
time algorithm for 3-cuts, and give the running time for your algorithm. (Hint: You 
may want to use a hash function of the type found in Section 13.3.2.) 
Exercise 13.9: Suppose we are given m vectors V1, V2, ... , VIII E {O, l}t such that any k 
of the m vectors are linearly independent modulo 2. Let Vi = (Vi,1, Vi.2 • ... , Vd)' Let 
U be chosen uniformly at random from {O, l}t, and let Xi = L~=1 Vi.jUj mod 2. Show 
that the Xi are uniform, k-wise independent bits. 
Exercise 13.10: We examine a specific way in which 2-universal hash functions dif-
fer from completely random hash functions. Let 5 = {O, 1, 2, ... , k}, and consider a 
hash function h with range {O, 1,2, ... , p -
I} for some prime p much larger than k. 
Consider the values h(O), h(l), ... , h(k). If h is a completely random hash function. 
then the probability that h(O) is smaller than any of the other values is roughly 1 I(k + 1). 
(There may be a tie for the smallest value, so the probability that any hU) is the unique 
smallest value is slightly less than l/(k + 1).) Now consider a hash function h chosen 
uniformly from the family 
H = {h a .b 10.::::: a,b :s p -
I} 
of Section 13.3.2. Estimate the probability that h(O) is smaller than h(1), ... , h(k) by 
randomly choosing 10,000 hash functions from h and computing hex) for all XES. 
Run this experiment for k = 32 and k = 128. using primes p = 5,023.309 and p = 
10,570,849. Is your estimate close to l/(k + 1)7 
Exercise 13.11: In a multi-set, each element can appear multiple times. Suppose that 
we have two multi-sets, 51 and 52, consisting of positive integers. We want to test if 
the two sets are the "same" - that is, if each item appears the same number of times in 
each set. One way of doing this is to sort both sets and then compare the sets in sorted 
order. This takes D( 11 log n) time if each multi-set contains 11 elements. 
(a) Consider the following algorithm. Hash each element of 51 into a hash table with 
en counters; the counters are initially 0, and the ith counter is incremented each 
334 

13.5 EXERCISES 
time the hash value of an element is i. Using another tahle of the same size and 
using the same hash function, do the same for 52. If the ith counter in the tirst 
table matches the ith counter in the second table for all i. report that the sets are 
the same. and otherwise report that the sets are different. 
Analyze the running time and error probability of this algorithm. assuming that 
the hash function is chosen from a 2-universal family. Explain how this algorithm 
can be extended to a Monte Carlo algorithm. and analyze the trade-off hetween its 
running time and its error probability. 
(b) We can also design a Las Vegas algorithm for this problem. Now each entry in the 
hash table corresponds to a linked list of counters. Each entry holds a list of the 
number of occurrences of each element that hashes to that location; this list can be 
kept in sorted order. Again. we create a hash table for 51 and a hash table for 52. 
and we test after hashing if the resulting tables are equal. 
Argue that this algorithm requires only linear expected time using only linear 
space. 
Exercise 13.12: In Section 1.3.3.1 we showed that the family 
H = {hu.h 11 ~ ([ ~ 17 -
1. () ~ h ~ 17) 
is 2-universal when p 2: 11. where 
hu.h(X) = ((ax -r-/J)mOdplmodll. 
Consider now the hash functions 
huCr) = (ax mod 17) mod 11 
and the family 
H' = {hu 11 ~ {[ ~ p -
I}. 
Give an example to show that H' is not 2-universal. Then prove that H' is almost 
2-universal in the following sense: for any x. y E {G, 1.2 ..... p - I}, if h is chosen uni-
formly at random from H I then 
2 
Pr ( h ( x) = h ( \')) < -. 
. 
-
n 
Exercise 13.13: In describing count-min filters. we assumed that the data stream con-
sisted of pairs of the form (il. ('I). where il was an item and ('I > G an integer count 
increment. Suppose that one were also allowed to decrement the count for an item. so 
that the stream could include pairs of the form (il. ('I) with ('I < O. We could require 
that the total count for an item i, 
Count(i. T) = 
l:i[=i.1:'::1~7 
always be positive. 
Explain how you could modify or otherwise use count-min filters to find heavy hit-
ters in this situation. 
335 

CHAPTER FOURTEEN* 
Balanced Allocations 
In this chapter we examine a simple yet powerful variant of the classic balls-and-bins 
paradigm, with applications to hashing and dynamic resource allocation. 
14.1. The Power of Two Choices 
Suppose that we sequentially place n balls into n bins by putting each ball into a bin 
chosen independently and uniformly at random. We studied this classic balls-and-bins 
problem in Chapter 5. There we showed that, at the end of the process, the most balls 
in any bin - the maximum load - is GOn n/ln In n) with high probability. 
In a variant of the process, each ball comes with d possible destination bins, each 
chosen independently and uniformly at random, and is placed in the least full bin 
among the d possible locations at the time of the placement. The original balls-and-
bins process corresponds to the case where d = 1. Surprisingly, even when d = 2, the 
behavior is completely different: when the process terminates, the maximum load is 
In In n / In 2 + O( 1) with high probability. Thus, an apparently minor change in the ran-
dom allocation process results in an exponential decrease in the maximum load. We 
may then ask what happens if each ball has three choices; perhaps the resulting load is 
then 0 On In In n). We shall consider the general case of d choices per ball and show 
that, when d 2: 2, with high probability the maximum load is In In n / In d + 0) 0). 
Although having more than two choices does reduce the maximum load, the reduction 
changes it by only a constant factor, so it remains 0) (In In n). 
14.1.1. The Upper Bound 
Theorem 14.1: Suppose that n balls are sequentially placed into n bills in thefollmv-
ing manner. For each ball, d 2: 2 bins are chosen independently and uniformly at 
random (with replacement). Each ball is placed in the least full of the d bins at the 
time (~f the placement, with ties broken randomly. After all the balls are placed, the 
maximum load of an}' bin is at most Inlnn/lnd + 00) with probability 1 - oO/n). 
336 

14.1 THE POWER OF TWO CHOICES 
The proof is rather technical, so before beginning we informally sketch the main points. 
In order to bound the maximum load, we need to approximately bound the number of 
bins with i balls for all values of i. In fact, for any given i, instead of trying to bound 
the number of bins with load exactly i, it will be easier to bound the number of bins 
with load at least i. The argument proceeds via what is, for the most part. a straight-
forward induction. We wish to find a sequence of values f3i such that the number of 
bins with load at least i is bounded above by f3i with high probability. 
Suppose that we knew that, over the entire course of the process, the number of bins 
with load at least i was bounded above by f3i. Let us consider how we would deter-
mine an appropriate inductive bound for f3i+! that holds with high probability. Define 
the height of a ball to be one more than the number of balls already in the bin in which 
the ball is placed. That is, if we think of balls as being stacked in the bin by order of 
arrival, the height of a ball is its position in the stack. The number of balls of height at 
least i + I gives an upper bound for the number of bins with at least i + 1 balls. 
A ball will have height at least i + I only if each of its d choices for a bin has load 
at least i. If there are indeed at most f3i bins with load at least i at all times, then the 
probability that each choice yields a bin with load at least i is at most f3i/n. Therefore. 
the probability that a ball has height at least i + 1 is at most (f3i/n )£1. We can lIse a 
Chernoff bound to conclude that, with high probability. the number of balls of height 
at least i + I will be at most 211 (f3i/n )£1. That is. if e\'t~rything works as sketched. then 
~ 
~ 2(f3i )". 
n 
n 
We examine this recursion carefully in the analysis and show that f3j becomes O(ln n) 
when j = In In n/ln d + 0(1). At this point. we must be a bit more careful in our analy-
sis because Chernoff bounds will no longer be sufficiently useful, but the result is easy 
to finish from there. 
The proof is technically challenging primarily because one must handle the condi-
tioning appropriately. In bounding f3i+!' we assumed that we had a bound on f3i' This 
assumption must be treated as a conditioning in the formal argument, which requires 
some care. 
We shall use the following notation: the state at time t refers to the state of the sys-
tem immediately after the tth ball is placed. The variable h (t) denotes the height of 
the tth ball, and lJi(t) and l1i(t) refer (respectively) to the number of bins with load at 
least i and the number of balls with height at least i at time t. We use lJi and l1i for 
lJi(n) and l1i(n) when the meaning is clear. An obvious but important fact, of which 
we make frequent use in the proof, is that lJi(t) ~ l1i(t), since every bin with load at 
least i must contain at least one ball with height at least i. 
Before beginning, we make note of two simple lemmas. First, we utilize a specific 
Chernoff bound for binomial random variables, easily derived from Eqn. (4.2) by let-
ting (; = 1. 
Lemma 14.2: 
Pre B(n, p) 2: 2np) ~ e-np/3. 
04.1) 
337 

BALANCED ALLOCATIONS 
The following lemma will help us cope with dependent random variables in the main 
proof. 
Lemma 14.3: Let XI, X 2 , ... , Xn be a sequence of random variables in an arbitrary 
domain, and let YI , Y2, ... , Yn be a sequence of binary random variables 'rvith the prop-
erty that }j = }j( XI, ... , Xi). If 
then 
pr( t Y, > k) ::: Pr(H(Il, p) > k). 
1=1 
Proof' If we consider the }j. one at a time, then each }j is less likely to take on the 
value I than an independent Bernoulli trial with success probability p, regardless of the 
values of the Xi. The result then follows by a simple induction. 
• 
We now begin the main proof. 
Proof of Theorem 14.1: Following the earlier sketch, we shall construct values f3i 
such that, with high probability, lJi(n) .::: f3i for all i. Let f34 = n/4, and let f3i+1 = 
2f3fl/n d - 1 for 4 .::: i < i*, where i* is to be determined. We let [i be the event that 
lJi(n) .::: f3i. Note that [4 holds with probability 1: there cannot be more than n/4 bins 
with at least 4 balls when there are only n balls. We now show that, with high proba-
bility, if [i holds then [i + I holds for 4 .::: i < i *. 
Fix a value of i in the given range. Let Yr be a binary random variable such that 
Yr = I if and only if h(t) :::: i + 1 and lJi(t - 1).::: f3i. 
That is, Yr is I if the height of the tth ball is at least i + 1 and if, at time t - 1, there 
are at most f3i bins with load at least i. The requirement that Yr be 1 only if there are at 
most f3i bins with load at least i may seem a bit odd: however, it makes handling the 
conditioning much easier. 
Specifically, let Wj represent the bins selected by the jth ball. Then 
f3d 
Pr(Yr = I I WI,"" Wr-I) .::: ~. 
n( 
That is, given the choices made by the first t - 1 balls, the probability that Yr is 1 is 
bounded by (f3dn )d. This is because, in order for Yr to be 1, there must be at most f3i 
bins with load at least i: and when this condition holds, the d choices of bins for the 
tth ball all have load at least i with probability (f3dn )d. If we did not force Yr to be 0 
if there are more than f3i bins with load at least i, then we would not be able to bound 
this conditional probability in this way. 
Let Pi = f3fl/n d . Then, from Lemma 14.3, we can conclude that 
pr( t Y, > k) ::: Pr(H(n, p;) > k). 
r=1 
338 

14.1 THE POWER OF TWO CHOICES 
This holds independently of any of the events [;i. owing to our careful definition of }~. 
(Had we not included the condition that Y/ = 1 only if lJi(t -
1) ~ f3i. the inequality 
would not necessarily hold.) 
Conditioned on [;i. we have L.:~I=1 Y/ = /li-I. Since lJi+1 ~ fli+l. we have 
= P1( t Y, > k I [i) 
/=1 
Pr(L.:;I=1 Y/ > k) 
<------
Pr([;i) 
Pr ( B ( II. Pi) > k) 
<-------
We bound the tail of the binomial distribution by using the Chernoff bound of 
Lemma 14.2. Letting k = f3i+1 = 2npi in the pre\"ious equations yields 
which gives 
whenever Pin 2: 61n n. 
We now remove the conditioning by using the fact that 
Pr ( -, [; i + I) = Pr ( -, [; i + 1 I [; i ) Pr ( [; i ) + Pr ( -, [; i + 1 I -, [; i ) Pr ( -, [; i ) 
~ Pr ( -, [; i + 1 I [; i ) Pr ([; i ) + Pr ( -, [; i ) : 
(14.3) 
then. by Eqns. (14.2) and (14.3), 
(14.4) 
as long as Pin 2: 61nn. 
Hence. whenever Pin 2: 61nn and[;i holds with high probability, then so does [;i+l. 
To conclude we need two more steps. First, we need to show that Pi n < 61n n when 
i is approximately In In n / In d. since this is our desired bound on the maximum load. 
Second, we must carefully handle the case where Pi n < 61n n separately, since the 
Chernoff bound is no longer strong enough to give appropriate bounds once Pi is this 
small. 
Let i* be the smallest value of i such that Pi = f3ft/n d < 61nn/n. We show that i* 
is Inlnn/lnd + 0(1). To do this, we prove inductively the bound 
This holds true when i = 0, and the induction argument follo\\'s: 
339 

BALANCED ALLOCATIONS 
2f3:~4 
f3(i+1l+4 = n d - I 
2C'd,_/~:::,d' r 
nd - I 
n 
The first line is the definition of f3i; the second follows from the induction hypothesis. 
It follows that f3i+4 ~ n/2d
l and hence that i* is Inlnl1/lnd + DO). By inductively 
applying Eqn. 04.4). we find that 
i* 
Pr ( -, E» 
~ -::;-. 
11-
We now handle the case where Pin < 61n n. We have 
Pr(vi*+1 > 181nl1 I [i*) ~ Pr(fli*+1 > 181n11 I [it) 
Pr ( B ( n , 6 Inn / n) 2: 18 Inn) 
< --------------------
Pr([i* ) 
< 
, 
' 
-
n- Pr([i*) 
where the last inequality again follows from the Chernoff bound. Removing the con-
ditioning as before then yields 
04.5 ) 
To wrap up. we note that 
and bound the latter quantity as follows: 
Pr(B(n, (l8Inn/n)d) > 2) 
(~)08Inn/n)2d 
Pr(fli x +2 2: 2 IVi'+1 ~ 181nl1) ~ 
-
< -------------
Pr ( Vi * + 1 ~ 18 In 11 ) 
-
Pr ( Vi * + 1 ::s 18 In 11 ) 
Here the last inequality comes from applying the crude union bound; there are (~) ways 
of choosing two balls, and for each pair the probability that both balls have height at 
least i* + 2 is (l8Inn/n)2d. 
Removing the conditioning as before and then using Eqn. (14.5) yields 
Pr ( v i * + J 2: I) ~ Pr ( fl i * + 2 2: 2) 
~ Pr ( fl i * + 2 2: 2 I Vi * + 1 ~ 18 Inn) Pr ( Vi * + 1 ~ 18 Inn) 
+ Pr(vi*+! > 181n n) 
081nnfd 
i*+1 
'd-' 
+ ---,-, 
n~ -
n~ 
< 
340 

14.2 TWO CHOICES: THE LOWER BOUND 
showing that Pr( Vi--+-:l 2: 1) is o(1/n) for d 2: 2 and hence that the probability the max-
imum bin load is more than i* + 3 = In Inn/lnd + 0(1) is o(1/n). 
• 
Breaking ties randomly is convenient for the prooL but in practice any natural tie-
breaking scheme will suffice. For example. in Exercise 14.1 we show that if the bins 
are numbered from 1 to n then breaking ties in fa\or of the smaller-numbered bin is 
sufficient. 
As an interesting variation, suppose that we split the n bins into two groups of equal 
size. Think of half of the bins as being on the left and the other half on the right. 
Each ball now chooses one bin independently and uniformly at random from each half. 
Again, each ball is placed in the least loaded of the two bins - but now, if there is a 
tie, the ball is placed in the bin on the left half. Surprisingly. hy splitting the bins and 
breaking ties in this fashion, we can obtain a slightly better hound on the maximum 
load: In In n / 21n ((1 + JS)/ 2) + 0(1). One can generalize this approach by splitting 
the bins into d ordered equal-sized groups: in case of a tie for the least-loaded bin, the 
bin in the lowest-ranked group obtains the ball. This \ariation is the suhject of Exer-
cise 14.13. 
14.2. Two Choices: The Lower Bound 
In this section we demonstrate that the result of Theorem 1'+.1 is essentially tight hy 
proving a cOiTesponding lower bound. 
Theorem 14.4: Suppose that 11 balls are sequentially placed into n bins in thefollo}('-
ing manner. For each ball. d 2: 2 bins are chosen independently and un(formly at 
random (~\'ith replacement). Each ball is placed in the leastfidl qf the d bins at the 
time qf the placement, ~\'ith ties broken randomly. A.fter all the balls are placed. the 
maximum load o(allY bin is at least In In n / In d -
O( I) with probahility 1 - o(1/n). 
The proof is similar in spirit to the upper bound, but there are some key differences. As 
with the upper bound. we wish to find a sequence of values Yi such that the numher of 
bins with load at least i is hounded below by Yi with high probability. In deriving the 
upper bound, we used the number of balls with height at least i as an upper bound on 
the number of bins with height at least i. We cannot do this in proving a lower bound. 
however. Instead, we find a lower hound on the number of balls with height exactly i 
and then use this as a lower hound on the number of bins with height at least i. 
In a similar vein. for the proof of the upper bound we used that the numher of hins 
with at least i balls at time n was at least Vi(t) for any time t ~ n. This is not helpful 
now that we are proving a lower bound: We need a lower bound on \'t( [). not an upper 
bound. to determine the probability that the tth ball has height i + I. To cope with this, 
we determine a lower bound Yi on the number of bins with load at least i that exist at 
time nO - 1/2i) and then bound the number of balls of height i + 1 that arise over the 
interval (11(1 - 1/2i),I1(l -
1/2
1+ I)J. This guarantees that appropriate Imver hounds 
hold when we need them in the induction. as we shall clarify in the proof. 
341 

BALANCED ALLOCATIONS 
We state the lemmas that we need, which are similar to those for the upper bound. 
Lemma 14.5: 
Pr(B(n,p) ~ np/2) ~ e-np/ 8 . 
(14.6) 
Lemma 14.6: Let XI, X 2, ... , Xn be a sequence of random variables in an arbitrar:v 
domain, and let YI , Y2 , " ., Yn be a sequence of binary random variables with the prop-
erty that ~ = ~(XI,"" Xi). If 
Pr(Yz· = 1 I XI,"" Xi-d 2: p, 
then 
pr(:t Y, > k) C> Pr(B(n, p) > k). 
z=1 
Proof of Theorem 14.4: Let:Ji be the event that vi(n(l -
1/2i» > Yi, where Yi IS 
given by: 
Yo = 11; 
( 
)
d 
n 
Yi 
Yi+1 = 2i +3 
-;; 
. 
Clearly Fo holds with probability 1. We now show inductively that successive :1;. hold 
with sufficiently high probability to obtain the desired lower bound. 
We want to compute 
Pr(-,Fi + 1 I Fi). 
With this in mind, for t in the range R = (11(1-l/2 i ), nO-l/2i+I)], define the binary 
random variable by 
Zt = 1 if and only if hU) = i + lor Vi+l{t -1) 2: Yi+I. 
Hence Zt is always 1 if Vi+1 (t -
1) 2: Yi+l· 
The probability that the tth ball has height exactly i + 1 is 
(",(tll- I))" -("'+1(:' - I) r 
The first term is the probability that all the d bins chosen by the tth ball have load at 
least i. This is necessary for the height of the tth ball to have height exactly i + 1. 
However, we must subtract out the probability that all d choices have at least i + 1 
balls, because in this case the height of the ball will be larger than i + 1. 
Again letting Wj represent the bins selected by the jth ball, we conclude that 
Yi 
Yi+1 
( )d ( 
)d 
Pr(Zt = 1 I WI,"" Wt-I, F i ) 2: 
-;; 
-
-11-
. 
This is because Zt is automatically 1 if Vi +1 (t -
1) 2: Yi +1; hence we can consider the 
probability in the case where Vi+l(t -
1) ~ Yi+l. Also, conditioned on F i, we have 
Vi{t -
1) 2: Yi· 
342 

1-'.2 TWO CHOICES: THE LOWER BOl:\D 
From the definition of the Yi we can further conclude that 
( 
'/1 )" _ (~)(f ~ ~ ('/! )':. 
n 
n 
2 
n 
Let PI = ~ (yz/n r/. 
Applying Lemma 14.6 yields 
Now our choice of Yi nicely satisfies 
1 n 
Yi+1 = "2 21+1 PI· 
By the Chernoff bound, 
pr(B (_n_ p.) < y. ) < e-IIPi/(S.2i+l) 
21+1' 
I 
1+1 
_ 
, 
which is oO/n2) provided that Pi n / 21+1 2: 171n n. Let i* be a lower bound on the 
largest integer for which this holds. We subsequently show that i* can be chosen to be 
In In n /In d - 00); for now let us assume that this is the case. Then, for i .::: i*, we 
have shown that 
Further, by definition we have that LtER Zt < Yi+1 implies .....,.0+1. Hence, for i .::: i*, 
Therefore, for sufficiently large n, 
Pr(.J;*) 2: Pr(.1> I .1>-1) . Pr(.0*-1 I .1>-2) ... Pr(FI I Fo) . Pr(Fo) 
~ (1 -
1/11 2)i' 
=1-o(1/n). 
All that remains is to demonstrate that In In n / In d -
0(1) is indeed an appropriate 
choice for i*. It suffices to show that YI 2: 171nn when i is Inlnn/lnd -
0(1). From 
the recursions YI+I = y/I/(2i+Jl1d-I), we find by a simple induction that 
A very rough bound gives 
n 
11 
Y·>--
I 
-
210di-l· 
We therefore look for the maximum i such that 
n 
--.- > 171nn. 
210d,-l -
343 

BALANCED ALLOCATIONS 
For n sufficiently large, we find that we can take i as large as In In n / In d -
0(1) by 
using the following chain of inequalities: 
11 
--.- > 171n11 
210d,-1 -
, 
10d,-1 
n 
2 
<--, 
-
17 In n 
10d i -
1 ~ log2 n -log2(17Inl1), 
. 
1 
1 
d l -
< -Inn 
-
20 
' 
In In Il 
i < -- -
0(1). 
-
Ind 
14.3. Applications of the Power of Two Choices 
• 
The balanced allocation paradigm has a number of interesting applications to comput-
ing problems. We elaborate here on two simple applications. When considering these 
applications, keep in mind that the In In n / In d + O( 1) bound we obtain for the balanced 
allocation paradigm generally corresponds to a maximum load of at most 5 in practice. 
14.3.1. Hashing 
When we considered hashing in Chapter 5, we related it to the balls-and-bins paradigm 
by assuming that the hash function maps the items being hashed to random entries in 
the hash table. Subject to this assumption, we proved that (a) when 0(11) items are 
hashed to a table with 11 entries, the expected number of items hashed to each individ-
ual entry in the table is 0(1), and (b) with high probability, the maximum number of 
items hashed to any entry in the table is (:")(lnn/lnlnn). 
These results are satisfactory for most applications, but for some they are not, since 
the expected value of the worst-case lookup time over all items is E-)(1n n /In In n). For 
example, when storing a routing table in a router, the worst-case time for a lookup in 
a hash table can be an important performance criterion, and the E-) (In n / In In 11) result 
is too large. Another potential problem is wasted memory. For example, suppose that 
we design a hash table where each bin should fit in a single fixed-size cache line of 
memory. Because the maximum load is so much larger than the average, we will have 
to use a large number of cache lines and many of them will be completely empty. For 
some applications, such as routers, this waste of memory is undesirable. 
Applying the balanced allocation paradigm, we obtain a hashing scheme with 0(1) 
expected and O( In In 11) maximum access time. The 2-H'ay chaininf? technique uses 
two random hash functions. The two hash functions define two possible entries in the 
table for each item. The item is inserted to the location that is least full at the time of 
insertion. Items in each entry of the table are stored in a linked list. If n items are se-
quentially inserted into a table of size 11, the expected insertion and lookup time is still 
0(1). (See Exercise 14.3.) Theorem 14.1 implies that with high probability the maxi-
mum time to find an item is O(1n In n), versus the (~) (In n / In In n) time when a single 
344 

14--' EXERCISES 
random hash function is used. This improvement does not come without cost. Since a 
search for an item now involves a search in two bins instead of one. the improvement in 
the expected maximum search time comes at the cost of roughly doubling the average 
search time. This cost can be mitigated if the two bins can be searched in parallel. 
14.3.2. Dynamic Resource Allocation 
Suppose a user or a process has to choose on-line between a number of identical re-
sources (choosing a server to use among servers in a network; choosing a disk to store a 
directory; choosing a printer; etc.). To find the least loaded resource, users may check 
the load on all resources before placing their requests. This process is expensive, since 
it requires sending an interrupt to each of the resources. A second approach is to send 
the task to a random resource. This approach has minimal overhead, but if all users 
follow it then loads will vary significantly among servers. The balanced allocation 
paradigm suggests a more efficient solution. If each user samples the load of two re-
sources and sends his request to the least loaded one, then the total overhead remains 
small while the load on the n resources varies much less. 
14.4. Exercises 
Exercise 14.1: (a) For Theorems 14.1 and 14.4. the statement of the proof is for the 
case that ties are broken randomly. Argue informally that. if the bins are numbered 
from 1 to nand if ties are broken in favor of the lower-numbered bin. then the theorems 
still hold. 
(b) Argue informally that the theorems apply to any tie-breaking mechanism that 
has no knowledge of the bin choices made by balls that have not yet been placed. 
Exercise 14.2: Consider the following variant of the balanced allocation paradigm: 
11 balls are placed sequentially in II bins. with the bins labeled from 0 to n -
1. Each 
ball chooses a bin i uniformly at random. and the ball is placed in the least loaded of 
bins i, i + I mod II. i + 2 mod II . .... i + d - I mod 11. Argue that, when d is a constant, 
the maximum load grows as H ( In II / In In 11). That is. the balanced allocation paradigm 
does not yield an O( In In II) result in this case. 
Exercise 14.3: Explain why. with 2-way chaining, the expected time to insert an item 
and to search for an item in a hash table of size 11 with n items is 0(1). Consider two 
cases: the search is for an item that is in the table; and the search is for an item that is 
not in the table. 
Exercise 14.4: Consider the following variant of the balanced allocation paradigm: 
11 balls are placed sequentially in 11 bins. Each ball comes with d choices. chosen 
independently and uniformly at random from the n bins. When a ball is placed. we 
are also allowed to move balls among these d bins to equalize their load as much as 
possible. Show that the maximum load is still at least In In II / In d -
O( I) with proba-
bility 1 - 0(1 /n) in this case. 
345 

BALANCED ALLOCATIONS 
Exercise 14.5: Suppose that in the balanced allocation setup there are n bins, but the 
bins are not chosen uniformly at random. Instead, the bins have two types: 1/3 of the 
bins are type A and 2 I 3 of the bins are type B. When a bin is chosen at random, each 
of the type-A bins is chosen with probability 21n and each of the type-B bins is chosen 
with probability 1/2n. Prove that the maximum load of any bin when each ball has d 
bin choices is still In In n I In d + 00). 
Exercise 14.6: Consider a parallel version of the balanced allocation paradigm in 
which we have n I k rounds, where k new balls arrive in each round. Each ball is placed 
in the least loaded of its d choices, where in this setting the load of each bin is the load 
at the end of the previous round. Ties are broken randomly. Note that the k new balls 
cannot affect each other's placement. Give an upper bound on the maximum load as a 
function of n, d, and k. 
Exercise 14.7: We have shown that sequentially throwing n balls into n bins randomly, 
using two bin choices for each ball, yields a maximum load of In In n I In 2 + O( 1) with 
high probability. Suppose that, instead of placing the balls sequentially, we had ac-
cess to all of the 2n choices for the n balls, and suppose we wanted to place each ball 
into one of its choices while minimizing the maximum load. In this setting, with high 
probability, we can obtain a maximum load that is constant. 
Write a program to explore this scenario. Your program should take as input a param-
eter k and implement the following greedy algorithm. At each step, some subset of the 
balls are active; initially, all balls are active. Repeatedly find a bin that has at least one 
but no more than k active balls that have chosen it, assign these active balls to that bin, 
and then remove these balls from the set of active balls. The process stops either when 
there are no active balls remaining or when there is no suitable bin. If the algorithm 
stops with no active balls remaining, then every bin is assigned no more than k balls. 
Try running your program with 10,000 balls and 10,000 bins. What is the smallest 
value of k for which the program terminates with no active balls remaining at least four 
out of five times? If your program is fast enough, try experimenting with more trials. 
Also, if your program is fast enough, try answering the same question for 100,000 balls 
and 100,000 bins. 
Exercise 14.8: The following problem models a simple distributed system where 
agents contend for resources and back off in the face of contention. As in Exercise 5.1 L 
balls represent agents and bins represent resources. 
The system evolves over rounds. In the first part of every round, balls are thrown 
independently and uniformly at random into n bins. In the second part of each round. 
each bin in which at least one ball has landed in that round serves exactly one ball from 
that round. The remaining balls are thrown again in the next round. We begin with 11 
balls in the first round, and we finish when every ball is served. 
(a) Show that, with probability 1 - oOln), this approach takes at most log210g2 n + 
0(1) rounds. (Hint: Let bk be the number of balls left after k rounds; show that 
bk+1 :::: c(bd 2111, for a suitable constant c with high probability, as long as bk+1 is 
sufficiently large.) 
346 

I-'A EXERCISES 
(b) Suppose that we modify the system so that a hin accepts a ball in a round if and 
only if that ball was the ollly ball to request that bin in that round. Show that. again 
with probahility I - o( 1/11). this approach takes at most log2 log2 11 + O( I ) rounds. 
Exercise 14.9: The natural way to simulate experiments with balls and bins is to cre-
ate an array that stores the load at each bin. To simulate 1,000,000 balls being placed 
into 1.000.000 bins would require an array of 1,000,000 counters. An alternative ap-
proach is to keep an array that records in the jth cell the number of bins with load j. 
Explain how this could be used to simulate placing 1,000.000 balls into 1,000,000 bins 
using the standard balls-and-bins paradigm and the halanced allocation paradigm with 
much less space. 
Exercise 14.10: Write a program to compare the performance of the standard balls-
and-bins paradigm and the balanced allocation paradigm. Run simulations placing n 
balls into 11 bins, with each ball having d = I, 2, 3, and 4 random choices. You should 
try 11 = 10,000,11 = 100.000, and 11 = LOOO.OOO. Repeat each experiment at least 100 
times and compute the expectation and variance of the maximum load for each value 
of d based on your trials. You may wish to use the idea of Exercise 14.9. 
Exercise 14.11: Write a simulation showing how the balanced allocation paradigm 
can improve performance for distributed queueing systems. Consider a hank of 11 FIFO 
queues with a Poisson arrival stream of customers to the entire bank of rate All per 
second, where A < 1. Upon entry a customer chooses a queue for service. and the 
service time for each customer is exponentially distrihuted with mean I second. You 
should compare two settings: (i) where each customer chooses a queue independently 
and uniformly at random from the 11 queues for service; and (ii) where each customer 
chooses two queues independently and uniformly at random from the II queues and 
waits at the queue with fewer customers, breaking ties randomly. Notice that the first 
setting is equivalent to having a bank of 11 M / M / I FIFO queues. each with Poisson ar-
rivals of rate ).. < I per second. You may find the discussion in Exercise 8.26 helpful 
in constructing your simulation. 
Your simulation should run for t seconds. and it should return the average (over all 
customers that have completed service) of the time spent in the system as well as the av-
erage (over all customers that have arrived) of the number of customers found waiting 
in the queue they selected for service. You should present results for your simulations 
for 11 = 100 and for t = 10.000 seconds, with Ie = 0.5, 0.8, 0.9. and 0.99. 
Exercise 14.12: Write a program to compare the performance of the following \'ari-
ation of the standard balls-and-bins paradigm and the balanced allocation paradigm. 
Initially 11 points are placed uniformly at random on the boundary of a circle of circum-
ference 1. These 11 points divide the circle into 11 arcs, which correspond to hins. \Yc 
now place 11 balls into the bins as follows: each ball chooses d points on the houndar: 
of the circle, uniformly at random. These d points correspond to the arcs (or. equi\ Ll-
lently. bins) that they lie on. The ball is placed in the least loaded of the d hins. hreaking 
ties in favor of the smallest arc. 
347 

BALANCED ALLOCATIONS 
Run simulations placing n balls into n bins for the cases d = 1 and d = 2. You 
should try n = 1,000, n = 10,000, and n = 100,000. Repeat each experiment at least 
100 times; for each run, the n initial points should be re-chosen. Give a chart showing 
the number of times the maximum load was k, based on your trials for each value of d. 
You may note that some arcs are much larger than others, and therefore when d = 
1 the maximum load can be rather high. Also, to find which bin each ball is placed in 
may require implementing a binary search or some other additional data structure to 
quickly map points on the circle boundary to the appropriate bin. 
Exercise 14.13: There is a small but interesting improvement that can be made to the 
balanced allocation scheme we have described. Again we will place n balls into n bins. 
We assume here than n is even. Suppose that we divide the n bins into two groups of 
size n/2. We call the two groups the left group and the right group. For each ball, we 
independently choose one bin uniformly at random from the left and one bin uniformly 
at random from the right. We put the ball in the least loaded bin. but if there is a tie we 
always put the ball in the bin from the left group. With this scheme, the maximum load 
is reduced to lnlnn/2ln1> + 0(1), where 1> = (1 + J"S)/2 is the golden ratio. This 
improves the result of Theorem 14.1 by a constant factor. (Note the two changes to our 
original scheme: the bins are split into two groups, and ties are broken in a consistent 
way; both changes are necessary to obtain the improvement we describe.) 
(a) Write a program to compare the performance of the original balanced allocation 
paradigm with this variation. Run simulations placing n balls into n bins, with 
each ball having d = 2 choices. You should try n = 10,000, n = 100,000, and 
n = 1,000,000. Repeat each experiment at least 100 times and compute the expec-
tation and variance of the maximum load based on your trials. Describe the extent 
of the improvement of the new variation. 
(b) Adapt Theorem 14.1 to prove this result. The key idea in how the theorem's proof 
must change is that we now require two sequences, f3i and Yi. Similar to Theo-
rem 14.1, f3i represents a desired upper bound on the number of bins on the left 
with load at least i, and Yi is a desired upper bound on the number of bins on the 
right with load at least i. Argue that choosing 
and 
for some constants c, and C2 is suitable (as long as f3i and Yi are large enough that 
Chernoff bounds may apply). 
Now let Fk be the kth Fibonacci number. Apply induction to show that, for suf-
ficiently large i, f3i .:s nC3c:2i and Yi .:s nC3c:'2i+1 for some constants C3 and C4· 
Following Theorem 14.1, use this to prove the In In n/2ln 1> + 0(1) upper bound. 
(c) This variation can easily be extended to the case of d > 2 choices by splitting the 
n bins into d ordered groups, choosing one bin uniformly at random from each 
group, and breaking ties in favor of the group that comes first in the ordering. Sug-
gest what would be the appropriate upper bound on the maximum load for this 
case, and give an argument backing your suggestion. (You need not give a com-
plete formal proof.) 
348 

Further Reading 
N. Alon and J. Spencer, The Probabilistic Method, 2nd ed. Wiley, New York, 2000. 
B. Bollobas, Random Graphs, 2nd ed. Academic Press, Orlando, FL, 1999. 
T. H. Corman, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, 2nd ed. 
MIT Press / McGraw-Hill, Cambridge / New York, 200l. 
T. M. Cover and 1. A. Thomas, Elements of Information Theory, Wiley, New York, 1991. 
W. Feller, An Introduction to the Prohahilit.v Theory, Vol. I, 3rd ed. Wiley, New York, 1968. 
W. Feller, An Introduction to the Probability Theory, Vol. 2. Wiley, New York. 1966. 
S. Karlin and H. M. Taylor. A First Course in Stochastic Processes, 2nd ed. Academic Press, 
New York. 1975. 
S. Karlin and H. M. Taylor, A Second Course in Stochastic Processes. Academic Press, New 
York. 1981. 
F. Leighton, Parallel Algorithms and Architectures. Morgan Kauffmann. San Mateo. CA, 1992. 
R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press. 1995. 
J. H. Spencer. Tell Lectures on the Probabilistic Method, 2nd ed. SIAM, Philadelphia, 1994. 
S. Ross. Stochastic Processes. Wiley, New York. 1996. 
S. Ross. A First Course ill Probability, 6th ed., Prentice-Hall. Englewood Cliffs, NJ, 2002. 
S. Ross, Prohahility Models for Computer Science. Academic Press, Orlando, FL, 2002. 
R. W. Wolff. Stochastic Modeling and the Theory of Queues. Prentice-HaiL Englewood Cliffs, 
NJ, 1989. 
349 

;:P.255 
arithmetic coding, 250 
Azuma-Hoeffding inequality. 303 
balanced allocation. 336-45 
ballot theorem. 299 
balls and bins model. 92-104 
halls and hins with feedback. 199-201 
Bayes' law. 10 
Bernoulli random variable. 25 
expectation. 25 
variance. 4X 
Bernoulli trials. 63 
binary erasure channel. 251 
hinary symmetric channel. 238 
binomial coefficients. bounds. 228 -9 
binomial di~trihution. 25.161 
binomial identity. 26 
binomial random variable. 25. 29 
expectation. 26 
variance. -I-X 
binomial theorem. -1-8. 22X 
birthday paradox. 90-2 
bit-tixing routing algorithm. 7-1-. 79. 80. XX 
Bloom tilter. 109-11. 329 
branching process. 30 
Buhblesort. 41.58.94 
Bucket sort. 93-4 
Buffon's needle. 267 
burst errors. 171 
butterfly. wrapped. 78 
routing algorithm. XO 
chain hashing. 106-7 
channel. 238 
binary symmetric. 238 
Chebyshev's inequality, 4X-9. 56. 134 
pairwise independent variables. 31X-19 
Chernoff bound. 63, 61-X3 
geometric random variables. 85 
Index 
Poisson random variable. 97 
Poisson trials. 63-7. 85 
sum of {-I, + I} random variables. 69-71 
weighted sum of Poisson trials. X6 
compressi on. 234-7 
compression function. 235 
conditional density function. 193 
conditional entropy. 246 
conditional expectation. 26. 26-30.131-3. 181 
random variable. 2X 
conditional expectation inequality. 136-8 
conditional probability. 6.27, 140. 159. 192 
contidence interval. 68. 319 
connectivity algorithm. 176 
convex function. 24 
count-min filter. 329-32 
conservative update. 332 
coupling. 274. 271-X9 
coupon collector's problem. 32-4. 50. 104-6. 117 
expectation. 33 
variance. 52 
covariance. 46. 135. 31X 
cUh.316-17 
data stream model. 329 
decoding function. 238 
delay .~equence. 81 
den;,ity function. 189 
conditional. 193 
dependency graph. 139 
derandomization. 126 
conditional expectations. 131-3 
pairwise independence. 316-17 
discrete prohability space. see probability space. 
discrete 
discrete random variable. see random variable. discrete 
distribution function. 189 
joint distribution, 191 
DNF formula. 255 
350 
approximating the numher of satisfying assignment. 
255-9 

II'iDEX 
Dooh martingale. 296 
dynamic resource alll)cation .. 
~-I-5 
edge-disjoint path~, 1-1-1 
edge exposure martingale. 296 
embedded Markm chain. 210 
encoding function. 238 
entropy, 225. 225--1-5 
entropy function. hinary, 226 
(E, c))-appro.\imation. 253, 25-1-
f'-uniform sample. 259 
event. 3 
set notation. 3 
simple. 3 
expectation. 2 L 20-6. 128-31 
Bernoulli random variable, 25 
binomial random variable, 26 
conditional. see conditional expectation 
continuous random variable, 190 
coupon collector's problem, 33 
exponential random variable, 197 
geometric random variahle, 31 
linearity of. see linearity of expectations 
Poisson random variable, 96 
product of random variables, 46 
uniform random variable, 194 
exponential distribution. 196, 205 
density, 197 
distribution function, 196 
memory less property, 197 
minimum, 198 
exponential random variable, 196 
expectation, 197 
variance, 197 
extracting random hits, 230-4 
extraction function, 230, 248 
factorial bounds. 93,102.151 
fully polynomial almost uniform sampler (FPAUS), 
259, 259-h3 
fully polynomial randomized approximation scheme 
(FPRAS 1.254.257-63,267 
gamhler\ ruin. Ihh-7. 298 
geometric distribution. 30. 62 
geometric random variahle. 30. 62 
expectation, 31 
memory less property. 3() 
variance. SO 
girth, 134 
Hamiltonian cycle. 113 
algorithm. 115 
harmonic number. 33 
hashing. 106-12,3..1--1--5 
fingerprint. 108 
perfect hashing. 32h-8 
univer.sal hash functions. 321-8 
heavy hitters. 332 
algorithm. 328 
hypercube. 73 
routing algorithm, 75 
inclusion-exclusion principle. -1-. If> 
independence, 6, 21.191 
k-wi.sc, 315 
mutual. 6. 21. 138,31..1-
pairwise, 315 
independent set. 133-4 
approximate counting, 259 
sampling, 277-8, 286-9 
indicator random variable, 25, 26 
Jensen's inequality. 23-4 
Kraft inequality. 249 
k-SAT. 1..1-2-6, 156 
large cuts. 129-30 
Las Vegas algorithm, 57, 128. 130 
law of total probability, 9 
leader election. 112 
linear insertion sort. 41,94 
linearity of expectations, 22, 26 
Little's resul t. 216 
Lovasz local lemma. 138-48 
explicit constructions. 142 
general case, 146. 148 
symmetric version, 139-41 
marginal distribution function, 191 
Markov chain, 153, 153-210 
aperiodic. 165 
communicating states, 164 
coupling, 274, 271-89 
directed graph representation, 155 
embedded,210 
equilibrium distribution, 167 
ergodic, 166 
irreducible. 164 
Markov property, 154 
memory less property, 154 
mixing time, 274 
null recurrent. 165 
path coupling. 286-9 
periodic. 165 
positive recurrent. 165 
rapidly mixing, 274 
recurrent. 164 
skeleton, 210 
state. 153 
stationary distrihution, 167, 167-72 
time-reversihle, 172. 183.26-1-.266 
transient. 164 
transition matrix. 154 
transition probahility. 154: II-step. 154 
l\Jarkov chain Montc Carlo methl)d. 2h_~-:' 
l\ larkov processe~. 210 
\Jarkov\ inequality. -1--1--5 
martingale. 295. 295-~m; 
.-'\zuma-Hocffding inequ,tiit: .. 
~II~ 
edgc C\p()~ure. 29h 
~tupping thcurem. 29:-; 
\erte\ e\pll~ure. 2l)-
ma\imum (ut. L~I-.' 
ma\imum ~ati~tiabIlIt:. 1.'11-1 
351 

median algorithm, 52-7 
memoryless property, 30, 197 
Metropolis scheme, 265 
min-cut algorithm, 12-14 
minimum cut-set. 12 
moment generating function, 61. 61-3 
Poisson distribution, 97 
moments, 45 
Monte Carlo algorithm, 57, 128, :n5 
Monte Carlo method, 252-66 
negative binomial random variable, 40 
order statistics, 207 
uniform random variables, 208 
packet routing, 72-83 
hypercube, 75 
wrapped butterfly, 80 
pairwise independence, 315 
constructions, 315-18 
sampling,319-21 
Parrondo's paradox, 177 
PASTA principle, 215 
perfect hashing, 326-8 
permutation, 41 
Poisson approximation, 99-105, III 
Poisson distribution, 95, 202 
moment generating function, 97 
Poisson process, 202, 201-9 
combining, 205-7 
interarrival distribution, 204 
splitting, 205-7 
Poisson random variable, 63, 95 
Chernoff bound, 97 
expectation, 96 
sums, 96 
Poisson trial. 63 
prefix code, 234, 249 
principle of deferred decisions, 9, 116 
priority queue, 224 
probabilistic method, 126-48,242 
counting argument. 126-8 
expectation argument. 128-31 
Lovasz local lemma, 138-48 
sample and modify, 133-4 
second moment method, 134-6 
probability function, 3 
probability space, 3 
discrete, 3 
queues, 173-4,212-18 
M / M / I. 213-16 
M/M/I/K,216 
M/MIXJ,216-18 
notation, 212 
Quicksort. 34-8 
pivot. 34 
random graphs, 112-13,296 
threshold function, 135, 135-8 
random variable, 20, 20-6 
continuous, 189, 188-93 
discrete, 20 
INDEX 
random walk, 174,174-7 
cover time, 176 
stationary distribution, 175 
randomized routing, 72-83 
hypercube, 75 
wrapped butterfly, 80 
reservoir sampling, 40 
sample space, 3 
sampling, 5 
pairwise independence, 319-21 
with replacement. 5 
without replacement. 5 
satisfiability, 130, 142-6 
second moment method, 134-6 
set balancing, 71 
set membership problem, 108 
Shannon's theorem, 237-45 
skeleton Markov chain, 210 
standard deviation, 45 
stationary distribution, 179,210 
continuous time Markov processes, 210-12 
s-t connectivity algorithm, 176 
Stirling's formula, 162,246 
stochastic counting process, 201 
stochastic process, 153 
stopping time, 297, 297-300 
symmetry breaking, 111-12 
2-SAT,156 
algorithm, 156-9 
3-SAT, 159 
algorithm, 161 
uniform distribution, 193, 208 
conditional probability, 194 
density function, 194 
distribution function, 194 
uniform random variable, 193 
expectation, 194 
variance, 194 
union bound, 4, 52, 127 
universal hash functions, 321. 321-8 
variance, 45 
Bernoulli random variable, 48 
binomial random variable, 48 
continuous random variable, 190 
exponential random variable, 197 
geometric random variable, 50 
sum, 46 
sum of pairwise independent variables, 318 
uniform random variable, 194 
variation distance, 272, 278 
verifying 
matrix multiplication, 8-12 
polynomial identities, 1-8 
vertex coloring, sampling, 282-6 
vertex exposure martingale, 297 
Wald's equation, 300-2 
weak law of large numbers, 60 
352 

"Probability is part of the conceptual core of modern computer science. 
Probabilistic analysis of algorithms. randolllued algorithms. and probabili.!;.tic 
comllimltori:!I amstruaions have become fundamcntal tools rorromj)\llet scienl:C 
and applied t11athcmatia;. This book provides a thorough grounding in discrete 
probability and its applications in computing, at a level acce~~iblc t(J advanced 
undergraduates in the computational, mathematieill, and engineering sciences." 
- Richard M. i(;;rp, Unil!rnity Prqjf:Mr, Unil~J'sity I1fCnlifvmia at Brrktley 
""n exciting new book 011 t'Jooomized algorithms. II nicely OO\'t'rs all ,he basic;. 
and also has SOIllC intere;tlng modem applications for the more adl-anced 
5tudellt ... 
- Alan ~riC7c, 1l1'l!fC5lCr aj'Matfif111ntin, CnlllCBit-Mdlvn Univmity 
R.;:lndomization and probabilistic techniques play an irnponam role 
in nloOem computer .. dence, with applicatioo~ ranging from combinatorial opti-
Illiz.uion and machine learning to commUilicarion nCl'WOria; and se<:ure protocols. 
This textbook is designed to acoolllp:my a ooc- or two-st'meslef" course for 
am-allce<! UlldergradualCS or beginning gradlLll.1e srudenbi in rompl.terscieuce and 
applied mathematics. It gives an c)'cdlCllt introductiOn to the probabilistic tech-
niques and par.tdigm~ l\~ed ill the developmellt of probabilistic algorithm,; and 
iUWyses. [t a~~Ull1e~ only an elementary background in diM:rete mathematics and 
gives a rigorous yet accessible treatment oftbe: material, with Ilumerous exal11ple~ 
and ilpplications. 
T1le first half of the book 00\'!.'11> core material, illdudillg randOnJ sanJpling, 
expectatiom.. Markov'S incqu3Iit}~ Cheb,'lihe\"s incqualit}, CiJemofflxmnds, ball&-
and-bins modl.'ls, th!.' probabilistic method, and MarkO\' chains. In me second half, 
the authors delve into more am':!nced tllpics SttCh:lS cOntinuou~ probability, appli-
catloru; of limited independence, entropy, Markov chain Monte Carlo mtthod.s, 
coupling, martillgale~, and balanced a1locatlons. With its comprehensive selection 
of topics, along with many examples and excrci~es, thi~ book is an indispem;ablc 
teaching tooL 
Midlad Mitzerunacher isJohn L Loeb Associare Prole6SOr in Computer Sciencc at 
Harvard Unil'f.lSit}~ 
EU Upful is l'rofffi>Qr alld Chair of Computer Science at Brown Unil'ersity. 
""'--to, ....... _In,H 11 .. 1""""'7-... _ 
......... c.d.~ 
...... _0'1! .... _ 
10 ,'''''-
.... ~.,.,.I._f.t>il_ 
C 
~~AM=D~RI~DGE 
_ 
PRESS 
, 

