Against Chain of Thought: Toward Causally Faithful Oversight
via Chain of Memory
Abstract
Chain of Thought (CoT) prompting has emerged as a popular technique for enhancing the rea-
soning capabilities of large language models, yet its epistemic limitations—post hoc rationalizations,
confabulations, and lack of causal grounding—undermine its reliability for safety-critical applica-
tions. This essay critiques CoT's deficiencies and proposes the Chain of Memory (CoM) paradigm as
a memory-first, latent reasoning framework that prioritizes causal faithfulness and interpretability.
By modeling reasoning as structured transformations in a latent memory space, CoM offers a robust
alternative that addresses CoT's shortcomings, enabling faithful causal tracing and generalizable
cognition. We formalize CoM's architecture, discuss its empirical and practical implications, and
draw parallels with cognitive science to argue for its adoption as a foundational shift in AI reasoning
research.
1
Introduction
Chain of Thought (CoT) prompting has gained prominence for improving the reasoning performance of
large language models by encouraging explicit, step-by-step verbalization of thought processes. Despite
its success in tasks requiring arithmetic, commonsense reasoning, and problem-solving, CoT's epistemic
foundations are increasingly questioned. Studies reveal that CoT traces often serve as post hoc ratio-
nalizations, lacking causal necessity and prone to confabulation, where models generate plausible but
incorrect explanations. These limitations pose significant challenges for safety-critical applications and
mechanistic interpretability, where understanding the true reasoning process is paramount.
This essay argues that CoT is insuﬀicient for achieving faithful interpretability in AI systems. In-
stead, we propose the Chain of Memory (CoM) paradigm, a memory-first approach that represents
reasoning as structured transformations in a latent state space. Unlike CoT, which relies on token-level
linguistic outputs, CoM prioritizes latent memory trajectories, with language serving as an optional,
human-readable projection. By grounding reasoning in causally traceable memory dynamics, CoM of-
fers a path toward robust, interpretable, and generalizable AI cognition. The essay proceeds as follows:
Section 2 critiques CoT's epistemic flaws; Section 3 introduces CoM's principles; Section 4 formalizes its
architecture; Section 5 explores empirical implications; Section 6 draws cognitive science parallels; and
Section 7 concludes with future directions.
2
The Epistemic Problem with Chain of Thought
Chain of Thought prompting involves instructing a language model to articulate intermediate reasoning
steps before producing a final answer. For example, in solving a mathematical problem, a model might
generate a sequence of equations or logical deductions as text tokens. While effective in improving task
performance, CoT suffers from several critical limitations.
First, CoT traces are linguistic performances rather than causal mechanisms.
Empirical studies
demonstrate that perturbations to CoT outputs—such as altering or omitting intermediate steps—often
do not affect the final answer, suggesting that the verbalized trace is not causally upstream of the model's
decision. This undermines CoT's reliability as a window into the model's reasoning process. Second,
CoT is susceptible to confabulation, where models generate plausible but incorrect explanations to justify
their outputs. For instance, a model might produce a coherent but factually inaccurate reasoning chain
due to biases in its training data or optimization pressures favoring fluency over truth. Finally, CoT's
reliance on token-level outputs makes it vulnerable to adversarial obfuscation, where subtle prompt
manipulations lead to misleading traces.
1

These epistemic flaws have profound consequences for safety and interpretability research. In safety-
critical domains, such as medical diagnosis or autonomous systems, relying on CoT risks deploying
models whose reasoning cannot be trusted or verified. Interpretability efforts, which aim to reverse-
engineer model behavior, are similarly hindered by CoT's lack of causal grounding, necessitating a new
paradigm for reasoning oversight.
3
Memory-Centric Cognition: The Chain of Memory Paradigm
The Chain of Memory (CoM) paradigm reimagines AI reasoning as a sequence of structured transforma-
tions in a latent memory space, rather than a stream of linguistic tokens. CoM is defined by three core
principles: (1) reasoning is encoded in latent memory states rather than token sequences; (2) reasoning
proceeds via state-space transformations and trajectory encoding; and (3) language outputs are optional
narrations, generated only when interpretability is required.
In contrast to CoT, which treats reasoning as a linear sequence of text, CoM models reasoning as a
dynamic evolution of memory states Mi ∈Rd, where each state encapsulates task-relevant information
and contextual embeddings. These states are updated through learned transformations, forming a causal
trajectory that can be queried or decoded as needed. The following table summarizes the key distinctions:
Feature
Chain of Thought
Chain of Memory
Representation
Token sequence
Latent memory trajectory
Primary Medium
Language
Vector space evolution
Interpretability
Narrative plausibility
Causal traceability
Causal Grounding
Weak
Strong
Narration
Always required
Optional
CoM draws theoretical inspiration from mechanistic interpretability, which seeks to map model com-
putations to human-understandable processes, and from cognitive science, particularly theories of mem-
ory and consciousness (e.g., Gazzaniga's interpreter module, Kahneman's System 1/System 2, Dehaene's
global workspace). Architectures like CDMem, MemoryBank, GUI Odyssey-CoM, Reflect-RL, and the
RSVP framework exemplify CoM principles by prioritizing structured memory over linguistic outputs,
enabling robust reasoning in tasks ranging from planning to interactive environments.
4
Formalizing CoM: Architecture and Mechanisms
A CoM agent consists of three core components: (1) memory encoding layers, (2) retrieval mechanisms,
and (3) latent reasoning cores. Memory encoding layers maintain a differentiable stack of states S =
{Mi}, where each Mi includes short-term, long-term, and graph-indexed memory components. The
update rule is:
Mi+1 = ϕ(Mi, ui, ci),
where ϕ is a transition function, ui is a utility signal, and ci is a contextual embedding.
Retrieval
mechanisms operate on a context graph G = (V, E), selecting relevant memory states via:
M∗= arg max
v∈V ⟨Mv, τ⟩.
The reasoning core processes these states to produce outputs, with optional language decoding for inter-
pretability:
Ti = GenCoT(Mi; θ).
CoM ensures causal faithfulness by tying outputs to latent trajectories, enabling gradient-based trac-
ing (I(Mi →y) =
∂y
∂Mi ). This contrasts with CoT, where token-level traces lack differentiability. In
RSVP-based CoM, memory states are points in a field (Φi(x), vi(x), Si(x)), with dynamics governed by
a variational action, ensuring thermodynamic and structural coherence. CoM's architecture supports
robustness to adversarial perturbations, as memory states are causally upstream of outputs, and enables
transferable cognition across tasks by leveraging shared memory trajectories.
2

5
Empirical and Practical Implications
CoM addresses CoT's safety challenges by providing causally faithful reasoning traces, critical for applica-
tions like autonomous decision-making or scientific discovery. Experimental setups can test CoM's faith-
fulness by measuring the impact of memory perturbations on outputs, using metrics like KL-divergence
between trajectories. Applications include GUI agents, where CoM enables robust navigation of inter-
active environments, and multi-task learning, where shared memory structures facilitate generalization.
Challenges remain, including scaling CoM to large language models and integrating it with existing
architectures, but initial results from frameworks like Reflect-RL suggest feasibility.
6
Philosophical and Cognitive Science Perspectives
CoM aligns with human cognition, where memory serves as the substrate for reasoning, and verbalization
is a secondary process. Cognitive science supports this view: Gazzaniga's work on the brain's inter-
preter module suggests that humans rationalize decisions post hoc, akin to CoT, while Dehaene's global
workspace theory emphasizes structured memory integration, mirroring CoM. By prioritizing memory
over language, CoM offers epistemic virtues—causal transparency, robustness, and generalizability—that
make it a natural evolution of AI cognition, moving beyond the linguistic biases of current models.
7
Conclusion and Future Directions
This essay has argued that CoT's reliance on linguistic traces leads to epistemic flaws that undermine its
utility for safe and interpretable AI. The Chain of Memory paradigm, by contrast, offers a memory-first,
causally faithful framework that redefines reasoning as latent state transformations. CoM's potential to
enable robust, generalizable, and interpretable AI cognition warrants a shift in research focus toward
memory-structured models. Future work should explore scalable CoM architectures, empirical validation
of causal faithfulness, and integration with existing LLMs. By building AI systems that think before
they speak, we can move closer to trustworthy and transparent artificial intelligence.
Mathematical Appendix: Formalizing Chain of Memory Archi-
tectures
A.1 From Token Traces to Latent Reasoning Trajectories
In Chain of Thought (CoT), reasoning traces are token sequences T = (t1, t2, . . . , tn) sampled from an
autoregressive decoder with probability:
P(T|x) =
n
∏
i=1
P(ti | x, t<i)
where x is the input prompt or query. The reasoning is implicit in the token distribution — it is not
explicitly represented in model memory.
In contrast, Chain of Memory (CoM) represents reasoning as a structured evolution of latent memory
states:
M0
f1
−→M1
f2
−→· · ·
fk
−→Mk,
where each Mi ∈Rd is a memory state and fi are learned transformations conditioned on task context
or environmental feedback. This sequence constitutes the causal trajectory of reasoning.
A.2 Memory Stack Dynamics
We model the memory state as a differentiable stack:
S = {Mi}k
i=0,
Mi ∈Rd,
with update rule:
Mi+1 = ϕ(Mi, ui, ci),
where
3

• ϕ is a latent transition function (e.g., an MLP or attention-based operator),
• ui is a utility or entropy-reduction signal,
• ci is a contextual embedding encoding task, environment, or retrieval signals.
This latent stack can be queried for downstream planning or reasoning, and optionally decoded into
natural language via a projection:
Ti = Decode(Mi),
only when interpretability is required.
A.3 Causal Faithfulness and Traceability
A CoM system is causally faithful if for each output y, there exists a latent trajectory {Mi} such that:
y = ψ(Mk),
with
Mk = fk ◦· · · ◦f1(M0),
and any perturbation to Mi induces a measurable effect on y:
∥ψ(Mk) −ψ(M′
k)∥> ϵ =⇒Mi was causally necessary,
where M′
k denotes the final memory after perturbing Mi.
This allows us to define the causal influence:
I(Mi →y) :=
∂y
∂Mi
,
enabling gradient-based interpretability and oversight — in contrast to token-level CoT traces, which
lack differentiability w.r.t. latent state.
A.4 Memory Retrieval and Graph Indexing
CoM agents retrieve prior trajectories based on task embeddings τ and a context graph G = (V, E),
where
• Each node v ∈V corresponds to a stored memory state Mv,
• Edges E encode semantic or topological similarity between reasoning paths.
The retrieval mechanism is defined as:
M∗= Retrieve(G, τ) := arg max
v∈V ⟨Mv, τ⟩,
where ⟨·, ·⟩denotes an appropriate similarity or attention kernel.
This generalizes CDMem-style memory retrieval to trajectory-aware, semantically indexed latent
states.
A.5 RSVP Field Encoding of Memory States
In RSVP-based CoM agents, each memory state Mi is interpreted as a point in a derived field:
Mi =
(
Φi(x), vi(x), Si(x)
)
,
where
• Φi(x) is a scalar entropy field,
• vi(x) is the baryonic vector flow field,
• Si(x) is the semantic structure tensor.
The update dynamics follow a variational principle minimizing a field-theoretic action:
δ
∫
L(Φ, v, S) d4x = 0,
where L encodes entropy descent, alignment constraints, and field coherence.
This ensures that memory evolution respects thermodynamic and structural priors — a core feature
of RSVP-CoM reasoning.
4

A.6 Optional Natural Language Virtualization
If interpretability is required, a virtual Chain of Thought trace is synthesized by a decoder:
Ti = GenCoT(Mi; θ),
where GenCoT is trained to translate latent memory states into human-readable explanations.
These traces are not causally upstream of y, but informationally entailed by the memory evolution.
A.7 Comparison with Chain of Thought
Feature
Chain of Thought (CoT)
Chain of Memory (CoM)
Reasoning Representation
Token sequence
Latent memory trajectory
Primary Medium
Language
Vector field evolution
Interpretability
Narrative plausibility
Causal traceability
Causal Grounding
Weak
Strong (via Mi dynamics)
Perturbation Robustness
Low
High (causal intervention monitorable)
Narration Necessity
Always required
Optional, on-demand
5

